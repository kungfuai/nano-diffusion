[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Nano Diffusion: Diffusion and Flow Matching from Scratch",
    "section": "",
    "text": "Welcome to Nano Diffusion, a hands-on tutorial series that teaches you how to build diffusion models and flow matching from the ground up. These models are what powers the latest generative AI applications in creating high quality images, videos, audio, and more modalities.\nThis course provides a minimal yet complete implementation of diffusion and flow matching models, making the complex topic accessible while maintaining practical utility."
  },
  {
    "objectID": "index.html#what-youll-learn",
    "href": "index.html#what-youll-learn",
    "title": "Nano Diffusion: Diffusion and Flow Matching from Scratch",
    "section": "What You’ll Learn",
    "text": "What You’ll Learn\n\nImplementation of training recipes for diffusion and flow matching models from scratch\nWorking with popular model architectures like UNet and DiT\nTraining on various datasets including 2D point clouds, animal face images, minecraft gameplay videos, etc.\nExperiment tracking and model evaluation."
  },
  {
    "objectID": "index.html#prerequisites",
    "href": "index.html#prerequisites",
    "title": "Nano Diffusion: Diffusion and Flow Matching from Scratch",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nAccess to a GPU (12GB VRAM or above)\nFamiliarity with PyTorch and deep learning basics\nDocker and NVIDIA container toolkit installed"
  },
  {
    "objectID": "index.html#tutorials",
    "href": "index.html#tutorials",
    "title": "Nano Diffusion: Diffusion and Flow Matching from Scratch",
    "section": "Tutorials",
    "text": "Tutorials\nPlease use the sidebar to navigate through the tutorials."
  },
  {
    "objectID": "index.html#references",
    "href": "index.html#references",
    "title": "Nano Diffusion: Diffusion and Flow Matching from Scratch",
    "section": "References",
    "text": "References\nThere are many great resources for the theoretical foundation, large scale training and applications of diffusion and flow matching models:\n\nDDPM paper\nLillian Weng’s blog\nCVPR’22 tutorial\nDiT paper\nFlow matching paper\nFAIR’s flow matching guide"
  },
  {
    "objectID": "inception_embedding_and_tsne.html",
    "href": "inception_embedding_and_tsne.html",
    "title": "Library code for trajectory visualization",
    "section": "",
    "text": "from cleanfid.inception_pytorch import fid_inception_v3\n\ninception_v3 = fid_inception_v3()\n_ = inception_v3.eval()\n\n/home/ubuntu/.local/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/home/ubuntu/.local/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n  warnings.warn(msg)\nfrom typing import Tuple\n\n\nclass TrajectorySet:\n    def __init__(self, embeddings):\n        \"\"\"\n        Managing a set of trajectories, each of which is a sequence of embeddings.\n\n        Parameters\n        ----------\n        embeddings: (n_timesteps, n_samples, *embedding_dims). This assumes\n            the first dimension is time. And it is ordered from t=0 to t=n_timesteps-1.\n            With t=0 representing the clean data and t=n_timesteps-1 representing the noise.\n\n        \"\"\"\n        self.embeddings = embeddings\n        self.embeddings_2d = None\n    \n    def run_tsne(self, n_components: int = 2, seed: int = 0, **kwargs):\n        \"\"\"Run t-SNE on the embeddings.\n        \"\"\"\n        print(f\"Running t-SNE on {self.embeddings.shape} embeddings...\")\n        from sklearn.manifold import TSNE\n        tsne = TSNE(n_components=n_components, random_state=seed, **kwargs)\n        flattened_embeddings = self.embeddings.reshape(-1, self.embeddings.shape[-1])\n        flattened_embeddings_2d = tsne.fit_transform(flattened_embeddings)\n        self.embeddings_2d = flattened_embeddings_2d.reshape(self.embeddings.shape[0], self.embeddings.shape[1], -1)\n        print(f\"t-SNE done. Shape of 2D embeddings: {self.embeddings_2d.shape}\")\n        return self.embeddings_2d\n    \n    def plot_trajectories(\n            self,\n            n: int = 10,\n            show_figure: bool = False,\n            noise_color: Tuple[float, float, float] = (0, 0, 1),  # blue\n            data_color: Tuple[float, float, float] = (1, 0, 0),  # red\n            figsize: tuple = (6, 6),\n            with_ticks: bool = False,\n            tsne_seed: int = 0,\n            **kwargs):\n        \"\"\"Plot trajectories of some selected samples.\n\n        This assumes the first dimension is time. And it is ordered from t=0 to t=n_timesteps-1.\n        With t=0 representing the clean data and t=n_timesteps-1 representing the noise.\n\n        Parameters\n        ----------\n        n: int\n            number of samples to plot\n        figsize: tuple\n            figure size\n        kwargs:\n            other keyword arguments for matplotlib.pyplot.scatter\n        \"\"\"\n        import numpy as np\n        import matplotlib.pyplot as plt\n\n        colors = []\n        for t in range(self.embeddings.shape[0]):\n            # interpolate between noise_color and data_color\n            factor = t / (self.embeddings.shape[0] - 1)\n            colors.append(np.array(noise_color) * factor + np.array(data_color) * (1 - factor))\n        colors = np.array(colors)\n        \n        if self.embeddings_2d is None:\n            if self.embeddings.shape[2] == 2:\n                self.embeddings_2d = self.embeddings\n            else:\n                self.embeddings_2d = self.run_tsne(seed=tsne_seed)\n\n        traj = self.embeddings_2d[:, :n, :]\n        plt.figure(figsize=figsize)\n        plt.scatter(traj[0, :n, 0], traj[0, :n, 1], s=10, alpha=0.8, c=\"red\")  # real\n        plt.scatter(traj[-1, :n, 0], traj[-1, :n, 1], s=4, alpha=1, c=\"blue\")  # noise\n        plt.scatter(traj[:, :n, 0], traj[:, :n, 1], s=0.5, alpha=0.7, c=colors.repeat(n, axis=0))  # \"olive\"\n        plt.plot(traj[:, :n, 0], traj[:, :n, 1], c=\"olive\", alpha=0.3)\n        plt.legend([\"Data\", \"Noise\", \"Intermediate Samples (color coded)\", \"Flow trajectory\"])\n        if not with_ticks:\n            plt.xticks([])\n            plt.yticks([])\n        if show_figure:\n            plt.show()\n        else:\n            plt.close()\n        \n        # return the figure\n        return plt.gcf()"
  },
  {
    "objectID": "inception_embedding_and_tsne.html#library-code-for-dataset-loading",
    "href": "inception_embedding_and_tsne.html#library-code-for-dataset-loading",
    "title": "Library code for trajectory visualization",
    "section": "Library code for dataset loading",
    "text": "Library code for dataset loading\n\nfrom torch.utils.data import Dataset\nimport torchvision.transforms as transforms\nfrom datasets import load_dataset\n\n\nclass HuggingFaceDataset(Dataset):\n    def __init__(self, dataset_path: str, transform=None):\n        self.dataset = load_dataset(dataset_path, split=\"train\")\n        self.transform = transform or self.default_transform\n        self.image_key = self.find_image_key()\n    \n    @property\n    def default_transform(self):\n        # ToTensor()\n        return transforms.ToTensor()\n\n    def find_image_key(self) -&gt; str:\n        # Check if the dataset has the \"image\" key\n        # NOTE: Can exapnd this to other common keys if needed\n        if \"image\" in self.dataset[0].keys():\n            return \"image\"\n        raise KeyError(\"Dataset does not have an 'image' key\")\n\n    def __len__(self):\n        return len(self.dataset)\n\n    def __getitem__(self, idx):\n        image = self.dataset[idx][self.image_key]\n        image = image.convert(\"RGB\")  # Convert to RGB to ensure 3 channels\n        # By default, set label to 0 to conform to current expected batch format\n        label = 0\n        if self.transform:\n            image = self.transform(image)\n        return image, label\n\n\nfrom torch.utils.data import DataLoader\nimport torch\nimport numpy as np\n\nbatch_size = 32\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ntransforms_list = [\n    transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n]\ntransform = transforms.Compose(transforms_list)\ninception_v3 = inception_v3.to(device)\nafhq_dataset = HuggingFaceDataset(\"zzsi/afhq64_16k\", transform=transform)\ndataloader = DataLoader(\n    afhq_dataset, batch_size=batch_size, shuffle=False, num_workers=2\n)\n\nreal_embeddings = []\nimages = next(iter(dataloader))[0]\nreal_images = images.clone()\nimages = images.to(device)\nresize = transforms.Resize((224, 224))\nwith torch.no_grad():\n    images = resize(images)\n    features = inception_v3(images)\nreal_embeddings.append(features.cpu().numpy())\n\nreal_embeddings = np.concatenate(real_embeddings, axis=0)\n\n\nnp.random.seed(0)\nnoise_images = torch.randn_like(images)\nnoise_images = noise_images.to(device)\nwith torch.no_grad():\n    noise_images = resize(noise_images)\n    features = inception_v3(noise_images)\nnoise_embeddings = features.cpu().numpy()\n\n\nreal_embeddings.shape, noise_embeddings.shape\n\n((32, 1008), (32, 1008))\n\n\n\n# t-sne\nfrom sklearn.manifold import TSNE\n\ntsne = TSNE(n_components=2, random_state=0)\nreal_and_noise_embeddings = np.concatenate([real_embeddings, noise_embeddings], axis=0)\nembeddings_2d = tsne.fit_transform(real_and_noise_embeddings)\n\n\n\n# visualize\nimport matplotlib.pyplot as plt\n\nplt.scatter(embeddings_2d[:real_embeddings.shape[0], 0], embeddings_2d[:real_embeddings.shape[0], 1], c=\"red\", label=\"real\")\nplt.scatter(embeddings_2d[real_embeddings.shape[0]:, 0], embeddings_2d[real_embeddings.shape[0]:, 1], c=\"blue\", label=\"noise\")\nplt.legend(loc=\"upper right\")\nplt.xlim(-7, 5)\nplt.ylim(-5, 5)\nplt.title(\"t-SNE of Real and Noise Embeddings\")\nplt.show()"
  },
  {
    "objectID": "inception_embedding_and_tsne.html#forward-diffusion",
    "href": "inception_embedding_and_tsne.html#forward-diffusion",
    "title": "Library code for trajectory visualization",
    "section": "Forward diffusion",
    "text": "Forward diffusion\n\ndef forward_diffusion(x_0, t, noise_schedule, noise=None):\n    _ts = t.view(-1, 1, 1, 1)\n    if noise is None:\n        noise = torch.randn_like(x_0)\n    assert _ts.max() &lt; len(noise_schedule[\"alphas_cumprod\"]), f\"t={_ts.max()} is larger than the length of noise_schedule: {len(noise_schedule['alphas_cumprod'])}\"\n    alpha_prod_t = noise_schedule[\"alphas_cumprod\"][_ts]\n    x_t = (alpha_prod_t ** 0.5) * x_0 + ((1 - alpha_prod_t) ** 0.5) * noise\n    return x_t, noise\n\n\nfrom typing import Dict\nbeta_min, beta_max = 1e-4, 0.02\n# beta_min, beta_max = 1e-4, 1\n# beta_min, beta_max = 0, 0.02\n\ndef create_noise_schedule(n_T: int, device: torch.device) -&gt; Dict[str, torch.Tensor]:\n    betas = torch.linspace(beta_min, beta_max, n_T).to(device)\n    alphas = 1. - betas\n    alphas_cumprod = torch.cumprod(alphas, axis=0).to(device)\n    alphas_cumprod_prev = torch.cat([torch.ones(1).to(device), alphas_cumprod[:-1].to(device)])\n    sqrt_recip_alphas = torch.sqrt(1.0 / alphas).to(device)\n    sqrt_alphas_cumprod = torch.sqrt(alphas_cumprod).to(device)\n    sqrt_one_minus_alphas_cumprod = torch.sqrt(1. - alphas_cumprod)\n    posterior_variance = betas * (1. - alphas_cumprod_prev) / (1. - alphas_cumprod)\n\n    return {\n        \"betas\": betas,\n        \"alphas\": alphas,\n        \"alphas_cumprod\": alphas_cumprod,\n        \"sqrt_recip_alphas\": sqrt_recip_alphas,\n        \"sqrt_alphas_cumprod\": sqrt_alphas_cumprod,\n        \"sqrt_one_minus_alphas_cumprod\": sqrt_one_minus_alphas_cumprod,\n        \"posterior_variance\": posterior_variance,\n    }\n\nnoise_schedule = create_noise_schedule(1000, \"cpu\")\n\n\nnoised_images = {}\ncolors = {}\n# for t in [0, 50, 100, 200, 300, 500, 800]:\ntorch.manual_seed(0)\ncommon_noise = torch.randn_like(real_images)\nfor t in range(0, 1000, 10):\n    noised_images[t] = forward_diffusion(real_images.cpu(), torch.tensor([t]), noise_schedule, common_noise)[0]\n    alpha_accum = noise_schedule[\"alphas_cumprod\"][t]\n    # interpolate between red (1,0,0) and blue (0,0,1)\n    #   small alpha_accum is blue, large is red\n    colors[t] = (alpha_accum, 0, 1 - alpha_accum)\n\nnoised_images[0][0].shape\n\ntorch.Size([3, 64, 64])\n\n\n\nnoised_embeddings = {}\nfor t in noised_images:\n    with torch.no_grad():\n        noised_images[t] = resize(noised_images[t])\n        features = inception_v3(noised_images[t].to(device))\n    noised_embeddings[t] = features.cpu().numpy()\n\n\nall_embeddings = np.concatenate([real_embeddings, *noised_embeddings.values()], axis=0)\nprint(f\"all_embeddings.shape: {all_embeddings.shape}\")\nembeddings_2d = tsne.fit_transform(all_embeddings)\nreal_embeddings_2d = embeddings_2d[:real_embeddings.shape[0]]\n# put them back to the original order\nnoised_embeddings_2d = {}\noffset = real_embeddings.shape[0]\nfor t in noised_embeddings:\n    noised_embeddings_2d[t] = embeddings_2d[offset:offset+len(noised_embeddings[t])]\n    offset += len(noised_embeddings[t])\n\n# plot the real embeddings\n# plt.scatter(real_embeddings_2d[:, 0], real_embeddings_2d[:, 1], c=\"red\", label=\"real\")\n# show color gradient from blue to red\ntrajs = []\nfor t in noised_embeddings_2d:\n    noised_embeddings_this_t_2d = noised_embeddings_2d[t]\n    trajs.append(noised_embeddings_this_t_2d)\ntrajs = np.array(trajs)\ncolor_for_trajs = []\nfor t in noised_embeddings_2d:\n    color_for_trajs.append(colors[t])\ncolor_for_trajs = np.array(color_for_trajs)\nprint(trajs.shape)\n\nall_embeddings.shape: (3232, 1008)\n(100, 32, 2)\n\n\n\n_ = TrajectorySet(trajs).plot_trajectories(n=8, show_figure=True)\n\n\n\n\n\n\n\n\n&lt;Figure size 640x480 with 0 Axes&gt;"
  },
  {
    "objectID": "inception_embedding_and_tsne.html#what-about-flow-matching",
    "href": "inception_embedding_and_tsne.html#what-about-flow-matching",
    "title": "Library code for trajectory visualization",
    "section": "What about flow matching?",
    "text": "What about flow matching?\n\nFlow matching utils\n\nfrom typing import Union\n\n\ndef pad_t_like_x(t, x):\n    \"\"\"Function to reshape the time vector t by the number of dimensions of x.\n\n    Parameters\n    ----------\n    x : Tensor, shape (bs, *dim)\n        represents the source minibatch\n    t : FloatTensor, shape (bs)\n\n    Returns\n    -------\n    t : Tensor, shape (bs, number of x dimensions)\n\n    Example\n    -------\n    x: Tensor (bs, C, W, H)\n    t: Vector (bs)\n    pad_t_like_x(t, x): Tensor (bs, 1, 1, 1)\n    \"\"\"\n    if isinstance(t, (float, int)):\n        return t\n    return t.reshape(-1, *([1] * (x.dim() - 1)))\n\nclass ConditionalFlowMatcher:\n    \"\"\"Base class for conditional flow matching methods. This class implements the independent\n    conditional flow matching methods from [1] and serves as a parent class for all other flow\n    matching methods.\n\n    It implements:\n    - Drawing data from gaussian probability path N(t * x1 + (1 - t) * x0, sigma) function\n    - conditional flow matching ut(x1|x0) = x1 - x0\n    - score function $\\nabla log p_t(x|x0, x1)$\n    \"\"\"\n\n    def __init__(self, sigma: Union[float, int] = 0.0):\n        r\"\"\"Initialize the ConditionalFlowMatcher class. It requires the hyper-parameter $\\sigma$.\n\n        Parameters\n        ----------\n        sigma : Union[float, int]\n        \"\"\"\n        self.sigma = sigma\n\n    def compute_mu_t(self, x0, x1, t):\n        \"\"\"\n        Compute the mean of the probability path N(t * x1 + (1 - t) * x0, sigma), see (Eq.14) [1].\n\n        Parameters\n        ----------\n        x0 : Tensor, shape (bs, *dim)\n            represents the source minibatch\n        x1 : Tensor, shape (bs, *dim)\n            represents the target minibatch\n        t : FloatTensor, shape (bs)\n\n        Returns\n        -------\n        mean mu_t: t * x1 + (1 - t) * x0\n\n        References\n        ----------\n        [1] Improving and Generalizing Flow-Based Generative Models with minibatch optimal transport, Preprint, Tong et al.\n        \"\"\"\n        t = pad_t_like_x(t, x0)\n        return t * x1 + (1 - t) * x0\n\n    def compute_sigma_t(self, t):\n        \"\"\"\n        Compute the standard deviation of the probability path N(t * x1 + (1 - t) * x0, sigma), see (Eq.14) [1].\n\n        Parameters\n        ----------\n        t : FloatTensor, shape (bs)\n\n        Returns\n        -------\n        standard deviation sigma\n\n        References\n        ----------\n        [1] Improving and Generalizing Flow-Based Generative Models with minibatch optimal transport, Preprint, Tong et al.\n        \"\"\"\n        del t\n        return self.sigma\n\n    def sample_xt(self, x0, x1, t, epsilon):\n        \"\"\"\n        Draw a sample from the probability path N(t * x1 + (1 - t) * x0, sigma), see (Eq.14) [1].\n\n        Parameters\n        ----------\n        x0 : Tensor, shape (bs, *dim)\n            represents the source minibatch\n        x1 : Tensor, shape (bs, *dim)\n            represents the target minibatch\n        t : FloatTensor, shape (bs)\n        epsilon : Tensor, shape (bs, *dim)\n            noise sample from N(0, 1)\n\n        Returns\n        -------\n        xt : Tensor, shape (bs, *dim)\n\n        References\n        ----------\n        [1] Improving and Generalizing Flow-Based Generative Models with minibatch optimal transport, Preprint, Tong et al.\n        \"\"\"\n        mu_t = self.compute_mu_t(x0, x1, t)\n        sigma_t = self.compute_sigma_t(t)\n        sigma_t = pad_t_like_x(sigma_t, x0)\n        return mu_t + sigma_t * epsilon\n\n    def compute_conditional_flow(self, x0, x1, t, xt):\n        \"\"\"\n        Compute the conditional vector field ut(x1|x0) = x1 - x0, see Eq.(15) [1].\n\n        Parameters\n        ----------\n        x0 : Tensor, shape (bs, *dim)\n            represents the source minibatch\n        x1 : Tensor, shape (bs, *dim)\n            represents the target minibatch\n        t : FloatTensor, shape (bs)\n        xt : Tensor, shape (bs, *dim)\n            represents the samples drawn from probability path pt\n\n        Returns\n        -------\n        ut : conditional vector field ut(x1|x0) = x1 - x0\n\n        References\n        ----------\n        [1] Improving and Generalizing Flow-Based Generative Models with minibatch optimal transport, Preprint, Tong et al.\n        \"\"\"\n        del t, xt\n        return x1 - x0\n\n    def sample_noise_like(self, x):\n        return torch.randn_like(x)\n\n    def sample_location_and_conditional_flow(self, x0, x1, t=None, return_noise=False):\n        \"\"\"\n        Compute the sample xt (drawn from N(t * x1 + (1 - t) * x0, sigma))\n        and the conditional vector field ut(x1|x0) = x1 - x0, see Eq.(15) [1].\n\n        Parameters\n        ----------\n        x0 : Tensor, shape (bs, *dim)\n            represents the source minibatch\n        x1 : Tensor, shape (bs, *dim)\n            represents the target minibatch\n        (optionally) t : Tensor, shape (bs)\n            represents the time levels\n            if None, drawn from uniform [0,1]\n        return_noise : bool\n            return the noise sample epsilon\n\n\n        Returns\n        -------\n        t : FloatTensor, shape (bs)\n        xt : Tensor, shape (bs, *dim)\n            represents the samples drawn from probability path pt\n        ut : conditional vector field ut(x1|x0) = x1 - x0\n        (optionally) eps: Tensor, shape (bs, *dim) such that xt = mu_t + sigma_t * epsilon\n\n        References\n        ----------\n        [1] Improving and Generalizing Flow-Based Generative Models with minibatch optimal transport, Preprint, Tong et al.\n        \"\"\"\n        if t is None:\n            t = torch.rand(x0.shape[0]).type_as(x0)\n        assert len(t) == x0.shape[0], f\"t has to have batch size dimension, got {len(t)}\"\n\n        eps = self.sample_noise_like(x0)\n        xt = self.sample_xt(x0, x1, t, eps)\n        ut = self.compute_conditional_flow(x0, x1, t, xt)\n        if return_noise:\n            return t, xt, ut, eps\n        else:\n            return t, xt, ut\n\n    def compute_lambda(self, t):\n        \"\"\"Compute the lambda function, see Eq.(23) [3].\n\n        Parameters\n        ----------\n        t : FloatTensor, shape (bs)\n\n        Returns\n        -------\n        lambda : score weighting function\n\n        References\n        ----------\n        [4] Simulation-free Schrodinger bridges via score and flow matching, Preprint, Tong et al.\n        \"\"\"\n        sigma_t = self.compute_sigma_t(t)\n        return 2 * sigma_t / (self.sigma**2 + 1e-8)\n\n\nimport numpy as np\nimport ot as pot\nimport warnings\nfrom functools import partial\n\nclass OTPlanSampler:\n    \"\"\"OTPlanSampler implements sampling coordinates according to an OT plan (wrt squared Euclidean\n    cost) with different implementations of the plan calculation.\"\"\"\n\n    def __init__(\n        self,\n        method: str,\n        reg: float = 0.05,\n        reg_m: float = 1.0,\n        normalize_cost: bool = False,\n        num_threads: Union[int, str] = 1,\n        warn: bool = True,\n    ) -&gt; None:\n        \"\"\"Initialize the OTPlanSampler class.\n\n        Parameters\n        ----------\n        method: str\n            choose which optimal transport solver you would like to use.\n            Currently supported are [\"exact\", \"sinkhorn\", \"unbalanced\",\n            \"partial\"] OT solvers.\n        reg: float, optional\n            regularization parameter to use for Sinkhorn-based iterative solvers.\n        reg_m: float, optional\n            regularization weight for unbalanced Sinkhorn-knopp solver.\n        normalize_cost: bool, optional\n            normalizes the cost matrix so that the maximum cost is 1. Helps\n            stabilize Sinkhorn-based solvers. Should not be used in the vast\n            majority of cases.\n        num_threads: int or str, optional\n            number of threads to use for the \"exact\" OT solver. If \"max\", uses\n            the maximum number of threads.\n        warn: bool, optional\n            if True, raises a warning if the algorithm does not converge\n        \"\"\"\n        # ot_fn should take (a, b, M) as arguments where a, b are marginals and\n        # M is a cost matrix\n        if method == \"exact\":\n            self.ot_fn = partial(pot.emd, numThreads=num_threads)\n        elif method == \"sinkhorn\":\n            self.ot_fn = partial(pot.sinkhorn, reg=reg)\n        elif method == \"unbalanced\":\n            self.ot_fn = partial(pot.unbalanced.sinkhorn_knopp_unbalanced, reg=reg, reg_m=reg_m)\n        elif method == \"partial\":\n            self.ot_fn = partial(pot.partial.entropic_partial_wasserstein, reg=reg)\n        else:\n            raise ValueError(f\"Unknown method: {method}\")\n        self.reg = reg\n        self.reg_m = reg_m\n        self.normalize_cost = normalize_cost\n        self.warn = warn\n\n    def get_map(self, x0, x1):\n        \"\"\"Compute the OT plan (wrt squared Euclidean cost) between a source and a target\n        minibatch.\n\n        Parameters\n        ----------\n        x0 : Tensor, shape (bs, *dim)\n            represents the source minibatch\n        x1 : Tensor, shape (bs, *dim)\n            represents the source minibatch\n\n        Returns\n        -------\n        p : numpy array, shape (bs, bs)\n            represents the OT plan between minibatches\n        \"\"\"\n        a, b = pot.unif(x0.shape[0]), pot.unif(x1.shape[0])\n        if x0.dim() &gt; 2:\n            x0 = x0.reshape(x0.shape[0], -1)\n        if x1.dim() &gt; 2:\n            x1 = x1.reshape(x1.shape[0], -1)\n        x1 = x1.reshape(x1.shape[0], -1)\n        M = torch.cdist(x0, x1) ** 2\n        if self.normalize_cost:\n            M = M / M.max()  # should not be normalized when using minibatches\n        p = self.ot_fn(a, b, M.detach().cpu().numpy())\n        if not np.all(np.isfinite(p)):\n            print(\"ERROR: p is not finite\")\n            print(p)\n            print(\"Cost mean, max\", M.mean(), M.max())\n            print(x0, x1)\n        if np.abs(p.sum()) &lt; 1e-8:\n            if self.warn:\n                warnings.warn(\"Numerical errors in OT plan, reverting to uniform plan.\")\n            p = np.ones_like(p) / p.size\n        return p\n\n    def sample_map(self, pi, batch_size, replace=True):\n        r\"\"\"Draw source and target samples from pi  $(x,z) \\sim \\pi$\n\n        Parameters\n        ----------\n        pi : numpy array, shape (bs, bs)\n            represents the source minibatch\n        batch_size : int\n            represents the OT plan between minibatches\n        replace : bool\n            represents sampling or without replacement from the OT plan\n\n        Returns\n        -------\n        (i_s, i_j) : tuple of numpy arrays, shape (bs, bs)\n            represents the indices of source and target data samples from $\\pi$\n        \"\"\"\n        p = pi.flatten()\n        p = p / p.sum()\n        choices = np.random.choice(\n            pi.shape[0] * pi.shape[1], p=p, size=batch_size, replace=replace\n        )\n        return np.divmod(choices, pi.shape[1])\n\n    def sample_plan(self, x0, x1, replace=True):\n        r\"\"\"Compute the OT plan $\\pi$ (wrt squared Euclidean cost) between a source and a target\n        minibatch and draw source and target samples from pi $(x,z) \\sim \\pi$\n\n        Parameters\n        ----------\n        x0 : Tensor, shape (bs, *dim)\n            represents the source minibatch\n        x1 : Tensor, shape (bs, *dim)\n            represents the source minibatch\n        replace : bool\n            represents sampling or without replacement from the OT plan\n\n        Returns\n        -------\n        x0[i] : Tensor, shape (bs, *dim)\n            represents the source minibatch drawn from $\\pi$\n        x1[j] : Tensor, shape (bs, *dim)\n            represents the source minibatch drawn from $\\pi$\n        \"\"\"\n        pi = self.get_map(x0, x1)\n        i, j = self.sample_map(pi, x0.shape[0], replace=replace)\n        return x0[i], x1[j]\n\n    def sample_plan_with_labels(self, x0, x1, y0=None, y1=None, replace=True):\n        r\"\"\"Compute the OT plan $\\pi$ (wrt squared Euclidean cost) between a source and a target\n        minibatch and draw source and target labeled samples from pi $(x,z) \\sim \\pi$\n\n        Parameters\n        ----------\n        x0 : Tensor, shape (bs, *dim)\n            represents the source minibatch\n        x1 : Tensor, shape (bs, *dim)\n            represents the target minibatch\n        y0 : Tensor, shape (bs)\n            represents the source label minibatch\n        y1 : Tensor, shape (bs)\n            represents the target label minibatch\n        replace : bool\n            represents sampling or without replacement from the OT plan\n\n        Returns\n        -------\n        x0[i] : Tensor, shape (bs, *dim)\n            represents the source minibatch drawn from $\\pi$\n        x1[j] : Tensor, shape (bs, *dim)\n            represents the target minibatch drawn from $\\pi$\n        y0[i] : Tensor, shape (bs, *dim)\n            represents the source label minibatch drawn from $\\pi$\n        y1[j] : Tensor, shape (bs, *dim)\n            represents the target label minibatch drawn from $\\pi$\n        \"\"\"\n        pi = self.get_map(x0, x1)\n        i, j = self.sample_map(pi, x0.shape[0], replace=replace)\n        return (\n            x0[i],\n            x1[j],\n            y0[i] if y0 is not None else None,\n            y1[j] if y1 is not None else None,\n        )\n\n    def sample_trajectory(self, X):\n        \"\"\"Compute the OT trajectories between different sample populations moving from the source\n        to the target distribution.\n\n        Parameters\n        ----------\n        X : Tensor, (bs, times, *dim)\n            different populations of samples moving from the source to the target distribution.\n\n        Returns\n        -------\n        to_return : Tensor, (bs, times, *dim)\n            represents the OT sampled trajectories over time.\n        \"\"\"\n        times = X.shape[1]\n        pis = []\n        for t in range(times - 1):\n            pis.append(self.get_map(X[:, t], X[:, t + 1]))\n\n        indices = [np.arange(X.shape[0])]\n        for pi in pis:\n            j = []\n            for i in indices[-1]:\n                j.append(np.random.choice(pi.shape[1], p=pi[i] / pi[i].sum()))\n            indices.append(np.array(j))\n\n        to_return = []\n        for t in range(times):\n            to_return.append(X[:, t][indices[t]])\n        to_return = np.stack(to_return, axis=1)\n        return to_return\n\n\nclass ExactOptimalTransportConditionalFlowMatcher(ConditionalFlowMatcher):\n    \"\"\"Child class for optimal transport conditional flow matching method. This class implements\n    the OT-CFM methods from [1] and inherits the ConditionalFlowMatcher parent class.\n\n    It overrides the sample_location_and_conditional_flow.\n    \"\"\"\n\n    def __init__(self, sigma: Union[float, int] = 0.0):\n        r\"\"\"Initialize the ConditionalFlowMatcher class. It requires the hyper-parameter $\\sigma$.\n\n        Parameters\n        ----------\n        sigma : Union[float, int]\n        ot_sampler: exact OT method to draw couplings (x0, x1) (see Eq.(17) [1]).\n        \"\"\"\n        super().__init__(sigma)\n        self.ot_sampler = OTPlanSampler(method=\"exact\")\n\n    def sample_location_and_conditional_flow(self, x0, x1, t=None, return_noise=False):\n        r\"\"\"\n        Compute the sample xt (drawn from N(t * x1 + (1 - t) * x0, sigma))\n        and the conditional vector field ut(x1|x0) = x1 - x0, see Eq.(15) [1]\n        with respect to the minibatch OT plan $\\Pi$.\n\n        Parameters\n        ----------\n        x0 : Tensor, shape (bs, *dim)\n            represents the source minibatch\n        x1 : Tensor, shape (bs, *dim)\n            represents the target minibatch\n        (optionally) t : Tensor, shape (bs)\n            represents the time levels\n            if None, drawn from uniform [0,1]\n        return_noise : bool\n            return the noise sample epsilon\n\n        Returns\n        -------\n        t : FloatTensor, shape (bs)\n        xt : Tensor, shape (bs, *dim)\n            represents the samples drawn from probability path pt\n        ut : conditional vector field ut(x1|x0) = x1 - x0\n        (optionally) epsilon : Tensor, shape (bs, *dim) such that xt = mu_t + sigma_t * epsilon\n\n        References\n        ----------\n        [1] Improving and Generalizing Flow-Based Generative Models with minibatch optimal transport, Preprint, Tong et al.\n        \"\"\"\n        x0, x1 = self.ot_sampler.sample_plan(x0, x1)\n        return super().sample_location_and_conditional_flow(x0, x1, t, return_noise)\n\n    def guided_sample_location_and_conditional_flow(\n        self, x0, x1, y0=None, y1=None, t=None, return_noise=False\n    ):\n        r\"\"\"\n        Compute the sample xt (drawn from N(t * x1 + (1 - t) * x0, sigma))\n        and the conditional vector field ut(x1|x0) = x1 - x0, see Eq.(15) [1]\n        with respect to the minibatch OT plan $\\Pi$.\n\n        Parameters\n        ----------\n        x0 : Tensor, shape (bs, *dim)\n            represents the source minibatch\n        x1 : Tensor, shape (bs, *dim)\n            represents the target minibatch\n        y0 : Tensor, shape (bs) (default: None)\n            represents the source label minibatch\n        y1 : Tensor, shape (bs) (default: None)\n            represents the target label minibatch\n        (optionally) t : Tensor, shape (bs)\n            represents the time levels\n            if None, drawn from uniform [0,1]\n        return_noise : bool\n            return the noise sample epsilon\n\n        Returns\n        -------\n        t : FloatTensor, shape (bs)\n        xt : Tensor, shape (bs, *dim)\n            represents the samples drawn from probability path pt\n        ut : conditional vector field ut(x1|x0) = x1 - x0\n        (optionally) epsilon : Tensor, shape (bs, *dim) such that xt = mu_t + sigma_t * epsilon\n\n        References\n        ----------\n        [1] Improving and Generalizing Flow-Based Generative Models with minibatch optimal transport, Preprint, Tong et al.\n        \"\"\"\n        x0, x1, y0, y1 = self.ot_sampler.sample_plan_with_labels(x0, x1, y0, y1)\n        if return_noise:\n            t, xt, ut, eps = super().sample_location_and_conditional_flow(x0, x1, t, return_noise)\n            return t, xt, ut, y0, y1, eps\n        else:\n            t, xt, ut = super().sample_location_and_conditional_flow(x0, x1, t, return_noise)\n            return t, xt, ut, y0, y1\n\n2024-12-02 23:00:43.531809: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n2024-12-02 23:00:43.543894: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-12-02 23:00:43.556582: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-12-02 23:00:43.560354: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2024-12-02 23:00:43.569758: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2024-12-02 23:00:44.333698: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n\n\n\n\nVisualize flow matching plan\n\nFM = ConditionalFlowMatcher(sigma=0)\n\n\nfrom tqdm import tqdm\n\n\ndef generate_trajectories(FM, num_timesteps=10, n=32, seed=0):\n    x_clean = real_images[:n]\n\n    # noisy data\n    torch.manual_seed(seed)\n    x_noisy = torch.randn_like(x_clean)\n    # Sample multiple timesteps to visualize flow progression\n    t_grid = torch.linspace(0, 1, num_timesteps)\n    x_ts = []\n    # u_ts = []\n\n    for i, t in enumerate(t_grid):\n        t_batch = t.repeat(x_clean.shape[0])\n        np.random.seed(seed)\n        t, x_t, u_t = FM.sample_location_and_conditional_flow(x0=x_clean, x1=x_noisy, t=t_batch)\n        x_ts.append(x_t.detach().cpu().numpy())\n        if i == 0:\n            print(\"x_t images mean (t=0):\", x_t.mean(), \"x_clean images mean:\", x_clean.mean())\n        if i == len(t_grid) - 1:\n            print(\"x_t images mean (t=1):\", x_t.mean(), \"x_noisy images mean:\", x_noisy.mean())\n        # u_ts.append(u_t.detach().cpu().numpy())\n    \n    # First, extract the embeddings\n    print(\"Extracting embeddings...\")\n    x_ts_embeddings = []\n    resize = transforms.Resize((224, 224))\n\n    def to_tensor(x):\n        return torch.from_numpy(x)\n    \n    inception_v3.eval()\n    with torch.no_grad():\n        for x_t in tqdm(x_ts):\n            x_ts_embeddings.append(inception_v3(resize(to_tensor(x_t).to(device))).cpu().numpy())\n\n    all_x_embeddings = np.array(x_ts_embeddings)\n    traj_set = TrajectorySet(all_x_embeddings)\n    return traj_set\n\n\ntraj_set = generate_trajectories(FM, num_timesteps=100, n=16)\nprint(\"clean data mean:\", traj_set.embeddings[0].mean())\n\nx_t images mean (t=0): tensor(-0.2593) x_clean images mean: tensor(-0.2593)\nx_t images mean (t=1): tensor(-0.0035) x_noisy images mean: tensor(-0.0035)\nExtracting embeddings...\n\n\n100%|██████████| 100/100 [00:01&lt;00:00, 94.71it/s]\n\n\nclean data mean: -0.035028137\n\n\n\n\n\n\n_ = traj_set.plot_trajectories(n=16, show_figure=True,  with_ticks=True)\n\n\n\n\n\n\n\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\nFM_ot = ExactOptimalTransportConditionalFlowMatcher(sigma=0)\not_traj_set = generate_trajectories(FM_ot, num_timesteps=100, n=16)\nprint(\"clean data mean:\", ot_traj_set.embeddings[0].mean())\n\nExtracting embeddings...\n\n\n100%|██████████| 100/100 [00:01&lt;00:00, 94.74it/s]\n\n\nclean data mean: -0.03625052\n\n\n\n\n\n\n_ = ot_traj_set.plot_trajectories(n=16, show_figure=True, with_ticks=True)\n\nRunning t-SNE on (100, 16, 1008) embeddings...\nt-SNE done. Shape of 2D embeddings: (100, 16, 2)\n\n\n\n\n\n\n\n\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\ntraj_set = generate_trajectories(FM, num_timesteps=100, n=16)\nprint(\"clean embedding mean:\", traj_set.embeddings[0].mean())\nprint(\"noise embedding mean:\", traj_set.embeddings[-1].mean())\not_traj_set = generate_trajectories(FM_ot, num_timesteps=100, n=16)\nprint(\"OT clean embedding mean:\", ot_traj_set.embeddings[0].mean())\nprint(\"OT noise embedding mean:\", ot_traj_set.embeddings[-1].mean())\n\nx_t images mean (t=0): tensor(-0.2593) x_clean images mean: tensor(-0.2593)\nx_t images mean (t=1): tensor(-0.0035) x_noisy images mean: tensor(-0.0035)\nExtracting embeddings...\n\n\n100%|██████████| 100/100 [00:01&lt;00:00, 95.40it/s]\n\n\nclean embedding mean: -0.035028137\nnoise embedding mean: -0.023954444\nx_t images mean (t=0): tensor(-0.1974) x_clean images mean: tensor(-0.2593)\nx_t images mean (t=1): tensor(-0.0002) x_noisy images mean: tensor(-0.0035)\nExtracting embeddings...\n\n\n100%|██████████| 100/100 [00:01&lt;00:00, 95.34it/s]\n\n\nOT clean embedding mean: -0.03625052\nOT noise embedding mean: -0.024131157"
  },
  {
    "objectID": "inception_embedding_and_tsne.html#visualize-the-images-in-x_t-using-ot-plan",
    "href": "inception_embedding_and_tsne.html#visualize-the-images-in-x_t-using-ot-plan",
    "title": "Library code for trajectory visualization",
    "section": "Visualize the images in x_t using OT plan",
    "text": "Visualize the images in x_t using OT plan\n\nreal_images = real_images[:8]\nx_clean = real_images\nprint(\"x_clean.shape:\", x_clean.shape)\ntorch.manual_seed(0)\nx_noisy = torch.randn_like(x_clean)\nFM_ot = ExactOptimalTransportConditionalFlowMatcher(sigma=0)\nt, x_t, u_t = FM_ot.sample_location_and_conditional_flow(x0=x_clean, x1=x_noisy, t=torch.tensor(0).repeat(x_clean.shape[0]))\n\nx_clean.shape: torch.Size([8, 3, 64, 64])\n\n\n\n# display x_clean\n# Do a subplot with 2 by 4 layout\nfig, axs = plt.subplots(2, 4, figsize=(10, 5))\nfor i in range(2):\n    for j in range(4):  \n        axs[i, j].imshow((x_clean[i * 4 + j].permute(1, 2, 0) + 1))\n        axs[i, j].set_title(\"mean: \" + str(x_clean[i * 4 + j].mean()))\nplt.show()\n\n# display x_t\nfig, axs = plt.subplots(2, 4, figsize=(10, 5))\nfor i in range(2):\n    for j in range(4):\n        axs[i, j].imshow((x_t[i * 4 + j].permute(1, 2, 0) + 1))\n        axs[i, j].set_title(\"mean: \" + str(x_t[i * 4 + j].mean()))\nplt.show()\n\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n\n\n\n\n\n\n\n\n\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers)."
  },
  {
    "objectID": "compare_denoising_step_fns.html",
    "href": "compare_denoising_step_fns.html",
    "title": "Nano Diffusion",
    "section": "",
    "text": "import sys\n\nsys.path.append(\"..\")\n\nfrom src.train import denoising_step_0, denoising_step_1, create_noise_schedule\n\n\n# compare the two denoising step functions\nimport torch\nimport numpy as np\n\nbatch_size = 8\nx_t = torch.randn(batch_size, 2)\n\nclass DenoisingModel(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(2, 2)\n\n    def forward(self, t, x):\n        return self.linear(x)\n\ndenoising_model = DenoisingModel()\ndenoising_model.eval()\nnoise_schedule = create_noise_schedule(n_T=1000, device=\"cpu\")\n\nt = 10\n# seed the noise\ntorch.manual_seed(0)\nx_0_0 = denoising_step_0(denoising_model, x_t=x_t, t=t, noise_schedule=noise_schedule, clip_sample=False)\n\ntorch.manual_seed(0)\nx_0_1 = denoising_step_1(denoising_model, x_t=x_t, t=t, noise_schedule=noise_schedule, clip_sample=False)\n\n# compare the two, mean squared error\nprint(\"MSE difference:\", np.mean((x_0_0.cpu().numpy() - x_0_1.cpu().numpy()) ** 2))\n# print the first 20 numbers\nprint(\"x_0_0:\", x_0_0.cpu().numpy().flatten()[:10])\nprint(\"x_0_1:\", x_0_1.cpu().numpy().flatten()[:10])\n\nMSE difference: 2.4966038e-10\nx_0_0: [-1.3758283  -1.7300689   0.56866246  0.7879243   0.6137402  -1.5527974\n -0.3410042   1.8221784   0.75911075 -0.6098999 ]\nx_0_1: [-1.3758079  -1.7300419   0.5686541   0.7879141   0.61373085 -1.552776\n -0.34100035  1.822156    0.7590997  -0.60988957]"
  },
  {
    "objectID": "2_1_nano_diffusion_afhq.html",
    "href": "2_1_nano_diffusion_afhq.html",
    "title": "Training a Diffusion Model for Animal Face Images",
    "section": "",
    "text": "(under construction)\n# !pip install datasets==3.0.2\nimport torch\nimport torch.optim as optim\nfrom torchvision import transforms\nfrom torch.utils.data import DataLoader, random_split\nfrom torch.nn import MSELoss\nfrom torchvision.utils import make_grid\nfrom dataclasses import dataclass\nfrom typing import Dict, Tuple\nfrom torch.utils.data import DataLoader",
    "crumbs": [
      "2.1 Diffusion for Animal Face Images"
    ]
  },
  {
    "objectID": "2_1_nano_diffusion_afhq.html#training-configuration",
    "href": "2_1_nano_diffusion_afhq.html#training-configuration",
    "title": "Training a Diffusion Model for Animal Face Images",
    "section": "Training Configuration",
    "text": "Training Configuration\n\n@dataclass\nclass TrainingConfig:\n    dataset: str = \"zzsi/afhq64_16k\"\n    # Model architecture\n    resolution: int = 64 # resolution of the image\n    num_denoising_steps: int = 1000 # number of timesteps\n\n    # Training loop and optimizer\n    total_steps: int = 100000  # total number of training steps\n    batch_size: int = 32 # batch size\n    learning_rate: float = 5e-4 # initial learning rate\n    weight_decay: float = 1e-6 # weight decay\n\n    # Data augmentation\n    random_flip: bool = False # randomly flip images horizontally\n\n\nconfig = TrainingConfig(resolution=32)",
    "crumbs": [
      "2.1 Diffusion for Animal Face Images"
    ]
  },
  {
    "objectID": "2_1_nano_diffusion_afhq.html#load-data",
    "href": "2_1_nano_diffusion_afhq.html#load-data",
    "title": "Training a Diffusion Model for Animal Face Images",
    "section": "Load data",
    "text": "Load data\n\nfrom torch.utils.data import Dataset\nfrom datasets import load_dataset\n\n\nclass HuggingFaceDataset(Dataset):\n    def __init__(self, dataset_path: str, transform=None):\n        self.dataset = load_dataset(dataset_path, split=\"train\")\n        self.transform = transform\n        self.image_key = self.find_image_key()\n\n    def find_image_key(self) -&gt; str:\n        # Check if the dataset has the \"image\" key\n        # NOTE: Can exapnd this to other common keys if needed\n        if \"image\" in self.dataset[0].keys():\n            return \"image\"\n        raise KeyError(\"Dataset does not have an 'image' key\")\n\n    def __len__(self):\n        return len(self.dataset)\n\n    def __getitem__(self, idx):\n        image = self.dataset[idx][self.image_key]\n        image = image.convert(\"RGB\")  # Convert to RGB to ensure 3 channels\n        # By default, set label to 0 to conform to current expected batch format\n        label = 0\n        if self.transform:\n            image = self.transform(image)\n        return image, label\n\n\ndef load_data(config: TrainingConfig) -&gt; Tuple[DataLoader, DataLoader]:\n    resolution = config.resolution\n    transforms_list = [\n        transforms.Resize((resolution, resolution)),\n        transforms.ToTensor(),\n        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n    ]\n    if config.random_flip:\n        transforms_list.insert(0, transforms.RandomHorizontalFlip())\n\n    transform = transforms.Compose(transforms_list)\n    full_dataset = HuggingFaceDataset(config.dataset, transform=transform)\n\n    train_size = int(0.9 * len(full_dataset))\n    val_size = len(full_dataset) - train_size\n    train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n    train_dataloader = DataLoader(\n        train_dataset, batch_size=config.batch_size, shuffle=True, num_workers=2\n    )\n    val_dataloader = DataLoader(\n        val_dataset, batch_size=config.batch_size, shuffle=False, num_workers=2\n    )\n\n    return train_dataloader, val_dataloader\n\n\n\ntrain_dataloader, val_dataloader = load_data(config)\n\n\nx = next(iter(train_dataloader))\n\nfrom matplotlib import pyplot as plt\ngrid_img = make_grid(x[0]).permute(1, 2, 0)\ngrid_img = (grid_img - grid_img.min()) / (grid_img.max() - grid_img.min())\nplt.imshow(grid_img)",
    "crumbs": [
      "2.1 Diffusion for Animal Face Images"
    ]
  },
  {
    "objectID": "2_1_nano_diffusion_afhq.html#create-model-components-for-diffusion",
    "href": "2_1_nano_diffusion_afhq.html#create-model-components-for-diffusion",
    "title": "Training a Diffusion Model for Animal Face Images",
    "section": "Create model components for diffusion",
    "text": "Create model components for diffusion\n\nLibrary code for model architecture\n\n\"\"\"\nFrom: https://github.com/VSehwag/minimal-diffusion/blob/main/unets.py\n\"\"\"\nfrom abc import abstractmethod\n\nimport math\nimport numpy as np\nimport torch as th\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass GroupNorm32(nn.GroupNorm):\n    def forward(self, x):\n        return super().forward(x.float()).type(x.dtype)\n\n\ndef conv_nd(dims, *args, **kwargs):\n    \"\"\"\n    Create a 1D, 2D, or 3D convolution module.\n    \"\"\"\n    if dims == 1:\n        return nn.Conv1d(*args, **kwargs)\n    elif dims == 2:\n        return nn.Conv2d(*args, **kwargs)\n    elif dims == 3:\n        return nn.Conv3d(*args, **kwargs)\n    raise ValueError(f\"unsupported dimensions: {dims}\")\n\n\ndef linear(*args, **kwargs):\n    \"\"\"\n    Create a linear module.\n    \"\"\"\n    return nn.Linear(*args, **kwargs)\n\n\ndef avg_pool_nd(dims, *args, **kwargs):\n    \"\"\"\n    Create a 1D, 2D, or 3D average pooling module.\n    \"\"\"\n    if dims == 1:\n        return nn.AvgPool1d(*args, **kwargs)\n    elif dims == 2:\n        return nn.AvgPool2d(*args, **kwargs)\n    elif dims == 3:\n        return nn.AvgPool3d(*args, **kwargs)\n    raise ValueError(f\"unsupported dimensions: {dims}\")\n\n\ndef update_ema(target_params, source_params, rate=0.99):\n    \"\"\"\n    Update target parameters to be closer to those of source parameters using\n    an exponential moving average.\n\n    :param target_params: the target parameter sequence.\n    :param source_params: the source parameter sequence.\n    :param rate: the EMA rate (closer to 1 means slower).\n    \"\"\"\n    for targ, src in zip(target_params, source_params):\n        targ.detach().mul_(rate).add_(src, alpha=1 - rate)\n\n\ndef zero_module(module):\n    \"\"\"\n    Zero out the parameters of a module and return it.\n    \"\"\"\n    for p in module.parameters():\n        p.detach().zero_()\n    return module\n\n\ndef normalization(channels):\n    \"\"\"\n    Make a standard normalization layer.\n\n    :param channels: number of input channels.\n    :return: an nn.Module for normalization.\n    \"\"\"\n    return GroupNorm32(32, channels)\n\n\ndef timestep_embedding(timesteps, dim, max_period=10000):\n    \"\"\"\n    Create sinusoidal timestep embeddings.\n\n    :param timesteps: a 1-D Tensor of N indices, one per batch element.\n                      These may be fractional.\n    :param dim: the dimension of the output.\n    :param max_period: controls the minimum frequency of the embeddings.\n    :return: an [N x dim] Tensor of positional embeddings.\n    \"\"\"\n    half = dim // 2\n    freqs = th.exp(\n        -math.log(max_period) * th.arange(start=0, end=half, dtype=th.float32) / half\n    ).to(device=timesteps.device)\n    if timesteps.ndim == 1:\n        args = timesteps[:, None].float() * freqs[None]\n    else:\n        args = timesteps.float() * freqs[None]\n    embedding = th.cat([th.cos(args), th.sin(args)], dim=-1)\n    if dim % 2:\n        embedding = th.cat([embedding, th.zeros_like(embedding[:, :1])], dim=-1)\n    return embedding\n\n\ndef checkpoint(func, inputs, params, flag):\n    \"\"\"\n    Evaluate a function without caching intermediate activations, allowing for\n    reduced memory at the expense of extra compute in the backward pass.\n\n    :param func: the function to evaluate.\n    :param inputs: the argument sequence to pass to `func`.\n    :param params: a sequence of parameters `func` depends on but does not\n                   explicitly take as arguments.\n    :param flag: if False, disable gradient checkpointing.\n    \"\"\"\n    if flag:\n        args = tuple(inputs) + tuple(params)\n        return CheckpointFunction.apply(func, len(inputs), *args)\n    else:\n        return func(*inputs)\n\n\nclass CheckpointFunction(th.autograd.Function):\n    @staticmethod\n    def forward(ctx, run_function, length, *args):\n        ctx.run_function = run_function\n        ctx.input_tensors = list(args[:length])\n        ctx.input_params = list(args[length:])\n        with th.no_grad():\n            output_tensors = ctx.run_function(*ctx.input_tensors)\n        return output_tensors\n\n    @staticmethod\n    def backward(ctx, *output_grads):\n        ctx.input_tensors = [x.detach().requires_grad_(True) for x in ctx.input_tensors]\n        with th.enable_grad():\n            # Fixes a bug where the first op in run_function modifies the\n            # Tensor storage in place, which is not allowed for detach()'d\n            # Tensors.\n            shallow_copies = [x.view_as(x) for x in ctx.input_tensors]\n            output_tensors = ctx.run_function(*shallow_copies)\n        input_grads = th.autograd.grad(\n            output_tensors,\n            ctx.input_tensors + ctx.input_params,\n            output_grads,\n            allow_unused=True,\n        )\n        del ctx.input_tensors\n        del ctx.input_params\n        del output_tensors\n        return (None, None) + input_grads\n\n\nclass AttentionPool2d(nn.Module):\n    \"\"\"\n    Adapted from CLIP: https://github.com/openai/CLIP/blob/main/clip/model.py\n    \"\"\"\n\n    def __init__(\n        self,\n        spacial_dim: int,\n        embed_dim: int,\n        num_heads_channels: int,\n        output_dim: int = None,\n    ):\n        super().__init__()\n        self.positional_embedding = nn.Parameter(\n            th.randn(embed_dim, spacial_dim ** 2 + 1) / embed_dim ** 0.5\n        )\n        self.qkv_proj = conv_nd(1, embed_dim, 3 * embed_dim, 1)\n        self.c_proj = conv_nd(1, embed_dim, output_dim or embed_dim, 1)\n        self.num_heads = embed_dim // num_heads_channels\n        self.attention = QKVAttention(self.num_heads)\n\n    def forward(self, x):\n        b, c, *_spatial = x.shape\n        x = x.reshape(b, c, -1)  # NC(HW)\n        x = th.cat([x.mean(dim=-1, keepdim=True), x], dim=-1)  # NC(HW+1)\n        x = x + self.positional_embedding[None, :, :].to(x.dtype)  # NC(HW+1)\n        x = self.qkv_proj(x)\n        x = self.attention(x)\n        x = self.c_proj(x)\n        return x[:, :, 0]\n\n\nclass TimestepBlock(nn.Module):\n    \"\"\"\n    Any module where forward() takes timestep embeddings as a second argument.\n    \"\"\"\n\n    @abstractmethod\n    def forward(self, x, emb):\n        \"\"\"\n        Apply the module to `x` given `emb` timestep embeddings.\n        \"\"\"\n\n\nclass TimestepEmbedSequential(nn.Sequential, TimestepBlock):\n    \"\"\"\n    A sequential module that passes timestep embeddings to the children that\n    support it as an extra input.\n    \"\"\"\n\n    def forward(self, x, emb):\n        for layer in self:\n            if isinstance(layer, TimestepBlock):\n                x = layer(x, emb)\n            else:\n                x = layer(x)\n        return x\n\n\nclass Upsample(nn.Module):\n    \"\"\"\n    An upsampling layer with an optional convolution.\n    :param channels: channels in the inputs and outputs.\n    :param use_conv: a bool determining if a convolution is applied.\n    :param dims: determines if the signal is 1D, 2D, or 3D. If 3D, then\n                 upsampling occurs in the inner-two dimensions.\n    \"\"\"\n\n    def __init__(self, channels, use_conv, dims=2, out_channels=None):\n        super().__init__()\n        self.channels = channels\n        self.out_channels = out_channels or channels\n        self.use_conv = use_conv\n        self.dims = dims\n        if use_conv:\n            self.conv = conv_nd(dims, self.channels, self.out_channels, 3, padding=1)\n\n    def forward(self, x):\n        assert x.shape[1] == self.channels\n        if self.dims == 3:\n            out = F.interpolate(\n                x, (x.shape[2], x.shape[3] * 2, x.shape[4] * 2), mode=\"nearest\"\n            )\n        else:\n            out = F.interpolate(x, scale_factor=2, mode=\"nearest\")\n        if x.shape[-1] == x.shape[-2] == 3:\n            # upsampling layer transform [3x3] to [6x6]. Manually paddding it to make [7x7]\n            out = F.pad(out, (1, 0, 1, 0))\n        if self.use_conv:\n            out = self.conv(out)\n        return out\n\n\nclass Downsample(nn.Module):\n    \"\"\"\n    A downsampling layer with an optional convolution.\n    :param channels: channels in the inputs and outputs.\n    :param use_conv: a bool determining if a convolution is applied.\n    :param dims: determines if the signal is 1D, 2D, or 3D. If 3D, then\n                 downsampling occurs in the inner-two dimensions.\n    \"\"\"\n\n    def __init__(self, channels, use_conv, dims=2, out_channels=None):\n        super().__init__()\n        self.channels = channels\n        self.out_channels = out_channels or channels\n        self.use_conv = use_conv\n        self.dims = dims\n        stride = 2 if dims != 3 else (1, 2, 2)\n        if use_conv:\n            self.op = conv_nd(\n                dims, self.channels, self.out_channels, 3, stride=stride, padding=1\n            )\n        else:\n            assert self.channels == self.out_channels\n            self.op = avg_pool_nd(dims, kernel_size=stride, stride=stride)\n\n    def forward(self, x):\n        assert x.shape[1] == self.channels\n        return self.op(x)\n\n\nclass ResBlock(TimestepBlock):\n    \"\"\"\n    A residual block that can optionally change the number of channels.\n    :param channels: the number of input channels.\n    :param emb_channels: the number of timestep embedding channels.\n    :param dropout: the rate of dropout.\n    :param out_channels: if specified, the number of out channels.\n    :param use_conv: if True and out_channels is specified, use a spatial\n        convolution instead of a smaller 1x1 convolution to change the\n        channels in the skip connection.\n    :param dims: determines if the signal is 1D, 2D, or 3D.\n    :param use_checkpoint: if True, use gradient checkpointing on this module.\n    :param up: if True, use this block for upsampling.\n    :param down: if True, use this block for downsampling.\n    \"\"\"\n\n    def __init__(\n        self,\n        channels,\n        emb_channels,\n        dropout,\n        out_channels=None,\n        use_conv=False,\n        use_scale_shift_norm=False,\n        dims=2,\n        use_checkpoint=False,\n        up=False,\n        down=False,\n    ):\n        super().__init__()\n        self.channels = channels\n        self.emb_channels = emb_channels\n        self.dropout = dropout\n        self.out_channels = out_channels or channels\n        self.use_conv = use_conv\n        self.use_checkpoint = use_checkpoint\n        self.use_scale_shift_norm = use_scale_shift_norm\n\n        self.in_layers = nn.Sequential(\n            normalization(channels),\n            nn.SiLU(),\n            conv_nd(dims, channels, self.out_channels, 3, padding=1),\n        )\n\n        self.updown = up or down\n\n        if up:\n            self.h_upd = Upsample(channels, False, dims)\n            self.x_upd = Upsample(channels, False, dims)\n        elif down:\n            self.h_upd = Downsample(channels, False, dims)\n            self.x_upd = Downsample(channels, False, dims)\n        else:\n            self.h_upd = self.x_upd = nn.Identity()\n\n        self.emb_layers = nn.Sequential(\n            nn.SiLU(),\n            linear(\n                emb_channels,\n                2 * self.out_channels if use_scale_shift_norm else self.out_channels,\n            ),\n        )\n        self.out_layers = nn.Sequential(\n            normalization(self.out_channels),\n            nn.SiLU(),\n            nn.Dropout(p=dropout),\n            zero_module(\n                conv_nd(dims, self.out_channels, self.out_channels, 3, padding=1)\n            ),\n        )\n\n        if self.out_channels == channels:\n            self.skip_connection = nn.Identity()\n        elif use_conv:\n            self.skip_connection = conv_nd(\n                dims, channels, self.out_channels, 3, padding=1\n            )\n        else:\n            self.skip_connection = conv_nd(dims, channels, self.out_channels, 1)\n\n    def forward(self, x, emb):\n        \"\"\"\n        Apply the block to a Tensor, conditioned on a timestep embedding.\n        :param x: an [N x C x ...] Tensor of features.\n        :param emb: an [N x emb_channels] Tensor of timestep embeddings.\n        :return: an [N x C x ...] Tensor of outputs.\n        \"\"\"\n        return checkpoint(\n            self._forward, (x, emb), self.parameters(), self.use_checkpoint\n        )\n\n    def _forward(self, x, emb):\n        if self.updown:\n            in_rest, in_conv = self.in_layers[:-1], self.in_layers[-1]\n            h = in_rest(x)\n            h = self.h_upd(h)\n            x = self.x_upd(x)\n            h = in_conv(h)\n        else:\n            h = self.in_layers(x)\n        emb_out = self.emb_layers(emb).type(h.dtype)\n        while len(emb_out.shape) &lt; len(h.shape):\n            emb_out = emb_out[..., None]\n        if self.use_scale_shift_norm:\n            out_norm, out_rest = self.out_layers[0], self.out_layers[1:]\n            scale, shift = th.chunk(emb_out, 2, dim=1)\n            h = out_norm(h) * (1 + scale) + shift\n            h = out_rest(h)\n        else:\n            h = h + emb_out\n            h = self.out_layers(h)\n        return self.skip_connection(x) + h\n\n\nclass AttentionBlock(nn.Module):\n    \"\"\"\n    An attention block that allows spatial positions to attend to each other.\n    Originally ported from here, but adapted to the N-d case.\n    https://github.com/hojonathanho/diffusion/blob/1e0dceb3b3495bbe19116a5e1b3596cd0706c543/diffusion_tf/models/unet.py#L66.\n    \"\"\"\n\n    def __init__(\n        self,\n        channels,\n        num_heads=1,\n        num_head_channels=-1,\n        use_checkpoint=False,\n        use_new_attention_order=False,\n    ):\n        super().__init__()\n        self.channels = channels\n        if num_head_channels == -1:\n            self.num_heads = num_heads\n        else:\n            assert (\n                channels % num_head_channels == 0\n            ), f\"q,k,v channels {channels} is not divisible by num_head_channels {num_head_channels}\"\n            self.num_heads = channels // num_head_channels\n        self.use_checkpoint = use_checkpoint\n        self.norm = normalization(channels)\n        self.qkv = conv_nd(1, channels, channels * 3, 1)\n        if use_new_attention_order:\n            # split qkv before split heads\n            self.attention = QKVAttention(self.num_heads)\n        else:\n            # split heads before split qkv\n            self.attention = QKVAttentionLegacy(self.num_heads)\n\n        self.proj_out = zero_module(conv_nd(1, channels, channels, 1))\n\n    def forward(self, x):\n        return checkpoint(self._forward, (x,), self.parameters(), True)\n\n    def _forward(self, x):\n        b, c, *spatial = x.shape\n        x = x.reshape(b, c, -1)\n        qkv = self.qkv(self.norm(x))\n        h = self.attention(qkv)\n        h = self.proj_out(h)\n        return (x + h).reshape(b, c, *spatial)\n\n\ndef count_flops_attn(model, _x, y):\n    \"\"\"\n    A counter for the `thop` package to count the operations in an\n    attention operation.\n    Meant to be used like:\n        macs, params = thop.profile(\n            model,\n            inputs=(inputs, timestamps),\n            custom_ops={QKVAttention: QKVAttention.count_flops},\n        )\n    \"\"\"\n    b, c, *spatial = y[0].shape\n    num_spatial = int(np.prod(spatial))\n    # We perform two matmuls with the same number of ops.\n    # The first computes the weight matrix, the second computes\n    # the combination of the value vectors.\n    matmul_ops = 2 * b * (num_spatial ** 2) * c\n    model.total_ops += th.DoubleTensor([matmul_ops])\n\n\nclass QKVAttentionLegacy(nn.Module):\n    \"\"\"\n    A module which performs QKV attention. Matches legacy QKVAttention + input/ouput heads shaping\n    \"\"\"\n\n    def __init__(self, n_heads):\n        super().__init__()\n        self.n_heads = n_heads\n\n    def forward(self, qkv):\n        \"\"\"\n        Apply QKV attention.\n        :param qkv: an [N x (H * 3 * C) x T] tensor of Qs, Ks, and Vs.\n        :return: an [N x (H * C) x T] tensor after attention.\n        \"\"\"\n        bs, width, length = qkv.shape\n        assert width % (3 * self.n_heads) == 0\n        ch = width // (3 * self.n_heads)\n        q, k, v = qkv.reshape(bs * self.n_heads, ch * 3, length).split(ch, dim=1)\n        scale = 1 / math.sqrt(math.sqrt(ch))\n        weight = th.einsum(\n            \"bct,bcs-&gt;bts\", q * scale, k * scale\n        )  # More stable with f16 than dividing afterwards\n        weight = th.softmax(weight.float(), dim=-1).type(weight.dtype)\n        a = th.einsum(\"bts,bcs-&gt;bct\", weight, v)\n        return a.reshape(bs, -1, length)\n\n    @staticmethod\n    def count_flops(model, _x, y):\n        return count_flops_attn(model, _x, y)\n\n\nclass QKVAttention(nn.Module):\n    \"\"\"\n    A module which performs QKV attention and splits in a different order.\n    \"\"\"\n\n    def __init__(self, n_heads):\n        super().__init__()\n        self.n_heads = n_heads\n\n    def forward(self, qkv):\n        \"\"\"\n        Apply QKV attention.\n        :param qkv: an [N x (3 * H * C) x T] tensor of Qs, Ks, and Vs.\n        :return: an [N x (H * C) x T] tensor after attention.\n        \"\"\"\n        bs, width, length = qkv.shape\n        assert width % (3 * self.n_heads) == 0\n        ch = width // (3 * self.n_heads)\n        q, k, v = qkv.chunk(3, dim=1)\n        scale = 1 / math.sqrt(math.sqrt(ch))\n        weight = th.einsum(\n            \"bct,bcs-&gt;bts\",\n            (q * scale).view(bs * self.n_heads, ch, length),\n            (k * scale).view(bs * self.n_heads, ch, length),\n        )  # More stable with f16 than dividing afterwards\n        weight = th.softmax(weight.float(), dim=-1).type(weight.dtype)\n        a = th.einsum(\"bts,bcs-&gt;bct\", weight, v.reshape(bs * self.n_heads, ch, length))\n        return a.reshape(bs, -1, length)\n\n    @staticmethod\n    def count_flops(model, _x, y):\n        return count_flops_attn(model, _x, y)\n\n\nclass UNetModel(nn.Module):\n    \"\"\"\n    The full UNet model with attention and timestep embedding.\n    :param in_channels: channels in the input Tensor.\n    :param emb_dim: base dimension of timestep embedding.\n    :param model_channels: base channel count for the model.\n    :param out_channels: channels in the output Tensor.\n    :param num_res_blocks: number of residual blocks per downsample.\n    :param attention_resolutions: a collection of downsample rates at which\n        attention will take place. May be a set, list, or tuple.\n        For example, if this contains 4, then at 4x downsampling, attention\n        will be used.\n    :param dropout: the dropout probability.\n    :param channel_mult: channel multiplier for each level of the UNet.\n    :param conv_resample: if True, use learned convolutions for upsampling and\n        downsampling.\n    :param dims: determines if the signal is 1D, 2D, or 3D.\n    :param num_classes: if specified (as an int), then this model will be\n        class-conditional with `num_classes` classes.\n    :param use_checkpoint: use gradient checkpointing to reduce memory usage.\n    :param num_heads: the number of attention heads in each attention layer.\n    :param num_heads_channels: if specified, ignore num_heads and instead use\n                               a fixed channel width per attention head.\n    :param num_heads_upsample: works with num_heads to set a different number\n                               of heads for upsampling. Deprecated.\n    :param use_scale_shift_norm: use a FiLM-like conditioning mechanism.\n    :param resblock_updown: use residual blocks for up/downsampling.\n    :param use_new_attention_order: use a different attention pattern for potentially\n                                    increased efficiency.\n    \"\"\"\n\n    def __init__(\n        self,\n        image_size,\n        in_channels,\n        model_channels,\n        out_channels,\n        num_res_blocks,\n        attention_resolutions,\n        time_emb_factor=4,\n        dropout=0,\n        channel_mult=(1, 2, 4, 8),\n        conv_resample=True,\n        dims=2,\n        num_classes=None,\n        use_checkpoint=False,\n        use_fp16=False,\n        num_heads=1,\n        num_head_channels=-1,\n        num_heads_upsample=-1,\n        use_scale_shift_norm=False,\n        resblock_updown=False,\n        use_new_attention_order=False,\n    ):\n        super().__init__()\n\n        if num_heads_upsample == -1:\n            num_heads_upsample = num_heads\n\n        self.image_size = image_size\n        self.in_channels = in_channels\n        self.model_channels = model_channels\n        self.out_channels = out_channels\n        self.num_res_blocks = num_res_blocks\n        self.attention_resolutions = attention_resolutions\n        self.dropout = dropout\n        self.channel_mult = channel_mult\n        self.conv_resample = conv_resample\n        self.num_classes = num_classes\n        self.use_checkpoint = use_checkpoint\n        self.dtype = th.float16 if use_fp16 else th.float32\n        self.num_heads = num_heads\n        self.num_head_channels = num_head_channels\n        self.num_heads_upsample = num_heads_upsample\n\n        time_embed_dim = model_channels * time_emb_factor\n        self.time_embed = nn.Sequential(\n            linear(model_channels, time_embed_dim),\n            nn.SiLU(),\n            linear(time_embed_dim, time_embed_dim),\n        )\n\n        if self.num_classes is not None:\n            self.label_emb = nn.Embedding(num_classes, time_embed_dim)\n\n        ch = input_ch = int(channel_mult[0] * model_channels)\n        self.input_blocks = nn.ModuleList(\n            [TimestepEmbedSequential(conv_nd(dims, in_channels, ch, 3, padding=1))]\n        )\n        self._feature_size = ch\n        input_block_chans = [ch]\n        ds = 1\n        for level, mult in enumerate(channel_mult):\n            for _ in range(num_res_blocks):\n                layers = [\n                    ResBlock(\n                        ch,\n                        time_embed_dim,\n                        dropout,\n                        out_channels=int(mult * model_channels),\n                        dims=dims,\n                        use_checkpoint=use_checkpoint,\n                        use_scale_shift_norm=use_scale_shift_norm,\n                    )\n                ]\n                ch = int(mult * model_channels)\n                if ds in attention_resolutions:\n                    layers.append(\n                        AttentionBlock(\n                            ch,\n                            use_checkpoint=use_checkpoint,\n                            num_heads=num_heads,\n                            num_head_channels=num_head_channels,\n                            use_new_attention_order=use_new_attention_order,\n                        )\n                    )\n                self.input_blocks.append(TimestepEmbedSequential(*layers))\n                self._feature_size += ch\n                input_block_chans.append(ch)\n            if level != len(channel_mult) - 1:\n                out_ch = ch\n                self.input_blocks.append(\n                    TimestepEmbedSequential(\n                        ResBlock(\n                            ch,\n                            time_embed_dim,\n                            dropout,\n                            out_channels=out_ch,\n                            dims=dims,\n                            use_checkpoint=use_checkpoint,\n                            use_scale_shift_norm=use_scale_shift_norm,\n                            down=True,\n                        )\n                        if resblock_updown\n                        else Downsample(\n                            ch, conv_resample, dims=dims, out_channels=out_ch\n                        )\n                    )\n                )\n                ch = out_ch\n                input_block_chans.append(ch)\n                ds *= 2\n                self._feature_size += ch\n\n        self.middle_block = TimestepEmbedSequential(\n            ResBlock(\n                ch,\n                time_embed_dim,\n                dropout,\n                dims=dims,\n                use_checkpoint=use_checkpoint,\n                use_scale_shift_norm=use_scale_shift_norm,\n            ),\n            AttentionBlock(\n                ch,\n                use_checkpoint=use_checkpoint,\n                num_heads=num_heads,\n                num_head_channels=num_head_channels,\n                use_new_attention_order=use_new_attention_order,\n            ),\n            ResBlock(\n                ch,\n                time_embed_dim,\n                dropout,\n                dims=dims,\n                use_checkpoint=use_checkpoint,\n                use_scale_shift_norm=use_scale_shift_norm,\n            ),\n        )\n        self._feature_size += ch\n\n        self.output_blocks = nn.ModuleList([])\n        for level, mult in list(enumerate(channel_mult))[::-1]:\n            for i in range(num_res_blocks + 1):\n                ich = input_block_chans.pop()\n                layers = [\n                    ResBlock(\n                        ch + ich,\n                        time_embed_dim,\n                        dropout,\n                        out_channels=int(model_channels * mult),\n                        dims=dims,\n                        use_checkpoint=use_checkpoint,\n                        use_scale_shift_norm=use_scale_shift_norm,\n                    )\n                ]\n                ch = int(model_channels * mult)\n                if ds in attention_resolutions:\n                    layers.append(\n                        AttentionBlock(\n                            ch,\n                            use_checkpoint=use_checkpoint,\n                            num_heads=num_heads_upsample,\n                            num_head_channels=num_head_channels,\n                            use_new_attention_order=use_new_attention_order,\n                        )\n                    )\n                if level and i == num_res_blocks:\n                    out_ch = ch\n                    layers.append(\n                        ResBlock(\n                            ch,\n                            time_embed_dim,\n                            dropout,\n                            out_channels=out_ch,\n                            dims=dims,\n                            use_checkpoint=use_checkpoint,\n                            use_scale_shift_norm=use_scale_shift_norm,\n                            up=True,\n                        )\n                        if resblock_updown\n                        else Upsample(ch, conv_resample, dims=dims, out_channels=out_ch)\n                    )\n                    ds //= 2\n                self.output_blocks.append(TimestepEmbedSequential(*layers))\n                self._feature_size += ch\n\n        self.out = nn.Sequential(\n            normalization(ch),\n            nn.SiLU(),\n            zero_module(conv_nd(dims, input_ch, out_channels, 3, padding=1)),\n        )\n\n    def forward(self, t, x, y=None, *args, **kwargs):\n        \"\"\"\n        Apply the model to an input batch.\n        :param t: a 1-D batch of timesteps.\n        :param x: an [N x C x ...] Tensor of inputs.\n        :param y: an [N] Tensor of labels, if class-conditional.\n        :return: an [N x C x ...] Tensor of outputs.\n        \"\"\"\n\n        assert (y is not None) == (\n            self.num_classes is not None\n        ), \"must specify y if and only if the model is class-conditional\"\n\n        hs = []\n        emb = self.time_embed(timestep_embedding(t, self.model_channels))\n        if self.num_classes is not None:\n            assert y.shape == (x.shape[0],)\n            emb = emb + self.label_emb(y)\n        h = x.type(self.dtype)\n        for module in self.input_blocks:\n            h = module(h, emb)\n            hs.append(h)\n        h = self.middle_block(h, emb)\n        for module in self.output_blocks:\n            h = th.cat([h, hs.pop()], dim=1)\n            h = module(h, emb)\n        h = h.type(x.dtype)\n        return self.out(h)\n\n\ndef UNetBig(\n    image_size,\n    in_channels=3,\n    out_channels=3,\n    base_width=192,\n    num_classes=None,\n):\n    if image_size == 128:\n        channel_mult = (1, 1, 2, 3, 4)\n    elif image_size == 64:\n        channel_mult = (1, 2, 3, 4)\n    elif image_size == 32:\n        channel_mult = (1, 2, 2, 2)\n    elif image_size == 28:\n        channel_mult = (1, 2, 2, 2)\n    else:\n        raise ValueError(f\"unsupported image size: {image_size}\")\n\n    attention_ds = []\n    if image_size == 28:\n        attention_resolutions = \"28,14,7\"\n    else:\n        attention_resolutions = \"32,16,8\"\n    for res in attention_resolutions.split(\",\"):\n        attention_ds.append(image_size // int(res))\n\n    return UNetModel(\n        image_size=image_size,\n        in_channels=in_channels,\n        model_channels=base_width,\n        out_channels=out_channels,\n        num_res_blocks=3,\n        attention_resolutions=tuple(attention_ds),\n        dropout=0.1,\n        channel_mult=channel_mult,\n        num_classes=num_classes,\n        use_checkpoint=False,\n        use_fp16=False,\n        num_heads=4,\n        num_head_channels=64,\n        num_heads_upsample=-1,\n        use_scale_shift_norm=True,\n        resblock_updown=True,\n        use_new_attention_order=True,\n    )\n\n\ndef UNet(\n    image_size,\n    in_channels=3,\n    out_channels=3,\n    base_width=64,\n    num_classes=None,\n):\n    if image_size == 128:\n        channel_mult = (1, 1, 2, 3, 4)\n    elif image_size == 64:\n        channel_mult = (1, 2, 3, 4)\n    elif image_size == 32:\n        channel_mult = (1, 2, 2, 2)\n    elif image_size == 28:\n        channel_mult = (1, 2, 2, 2)\n    else:\n        raise ValueError(f\"unsupported image size: {image_size}\")\n\n    attention_ds = []\n    if image_size == 28:\n        attention_resolutions = \"28,14,7\"\n    else:\n        attention_resolutions = \"32,16,8\"\n    for res in attention_resolutions.split(\",\"):\n        attention_ds.append(image_size // int(res))\n\n    return UNetModel(\n        image_size=image_size,\n        in_channels=in_channels,\n        model_channels=base_width,\n        out_channels=out_channels,\n        num_res_blocks=3,\n        attention_resolutions=tuple(attention_ds),\n        dropout=0.1,\n        channel_mult=channel_mult,\n        num_classes=num_classes,\n        use_checkpoint=False,\n        use_fp16=False,\n        num_heads=4,\n        num_head_channels=64,\n        num_heads_upsample=-1,\n        use_scale_shift_norm=True,\n        resblock_updown=True,\n        use_new_attention_order=True,\n    )\n\n\ndef UNetSmall(\n    image_size,\n    in_channels=3,\n    out_channels=3,\n    base_width=32,\n    num_classes=None,\n):\n    if image_size == 128:\n        channel_mult = (1, 1, 2, 3, 4)\n    elif image_size == 64:\n        channel_mult = (1, 2, 3, 4)\n    elif image_size == 32:\n        channel_mult = (1, 2, 2, 2)\n    elif image_size == 28:\n        channel_mult = (1, 2, 2, 2)\n    else:\n        raise ValueError(f\"unsupported image size: {image_size}\")\n\n    attention_ds = []\n    if image_size == 28:\n        attention_resolutions = \"28,14,7\"\n    else:\n        attention_resolutions = \"32,16,8\"\n    for res in attention_resolutions.split(\",\"):\n        attention_ds.append(image_size // int(res))\n\n    return UNetModel(\n        image_size=image_size,\n        in_channels=in_channels,\n        model_channels=base_width,\n        out_channels=out_channels,\n        num_res_blocks=2,\n        attention_resolutions=tuple(attention_ds),\n        time_emb_factor=2,\n        dropout=0.1,\n        channel_mult=channel_mult,\n        num_classes=num_classes,\n        use_checkpoint=False,\n        use_fp16=False,\n        num_heads=4,\n        num_head_channels=32,\n        num_heads_upsample=-1,\n        use_scale_shift_norm=True,\n        resblock_updown=True,\n        use_new_attention_order=True,\n    )\n\n\n\nCreate model (the user logic)\n\ndevice = torch.device(\"cuda\")\ndenoising_model = UNet(\n    image_size=config.resolution,\n).to(device)\n\nprint(f\"model params: {sum(p.numel() for p in denoising_model.parameters()) / 1e6:.2f} M\")\n\nmodel params: 14.42 M\n\n\n\n\nOptimizer\n\noptimizer = optim.AdamW(denoising_model.parameters(), lr=config.learning_rate, weight_decay=config.weight_decay)\n\n\n\nDiffusion noise schedule\n\nbeta_min, beta_max = 1e-4, 0.02\n# beta_min, beta_max = 1e-4, 1\n# beta_min, beta_max = 0, 0.02\n\ndef create_noise_schedule(n_T: int, device: torch.device) -&gt; Dict[str, torch.Tensor]:\n    betas = torch.linspace(beta_min, beta_max, n_T).to(device)\n    alphas = 1. - betas\n    alphas_cumprod = torch.cumprod(alphas, axis=0).to(device)\n    alphas_cumprod_prev = torch.cat([torch.ones(1).to(device), alphas_cumprod[:-1].to(device)])\n    sqrt_recip_alphas = torch.sqrt(1.0 / alphas).to(device)\n    sqrt_alphas_cumprod = torch.sqrt(alphas_cumprod).to(device)\n    sqrt_one_minus_alphas_cumprod = torch.sqrt(1. - alphas_cumprod)\n    posterior_variance = betas * (1. - alphas_cumprod_prev) / (1. - alphas_cumprod)\n\n    return {\n        \"betas\": betas,\n        \"alphas\": alphas,\n        \"alphas_cumprod\": alphas_cumprod,\n        \"sqrt_recip_alphas\": sqrt_recip_alphas,\n        \"sqrt_alphas_cumprod\": sqrt_alphas_cumprod,\n        \"sqrt_one_minus_alphas_cumprod\": sqrt_one_minus_alphas_cumprod,\n        \"posterior_variance\": posterior_variance,\n    }\n\n\nnoise_schedule = create_noise_schedule(config.num_denoising_steps, device)\n\n# plot the schedule\nplt.figure(figsize=(10, 10))\nplt.subplot(2, 2, 1); plt.plot(range(1000), noise_schedule[\"betas\"].cpu().numpy())\nplt.title(\"beta_t\")\n\nplt.subplot(2, 2, 2); plt.plot(range(1000), noise_schedule[\"alphas_cumprod\"].cpu().numpy())\n_ = plt.title(\"alphas_cumprod_t\")\n\nplt.subplot(2, 2, 3); plt.plot(range(1000), noise_schedule[\"sqrt_alphas_cumprod\"].cpu().numpy())\nplt.title(\"sqrt_alphas_cumprod_t\")\n\nplt.subplot(2, 2, 4); plt.plot(range(1000), noise_schedule[\"sqrt_one_minus_alphas_cumprod\"].cpu().numpy())\n_ = plt.title(\"sqrt_one_minus_alphas_cumprod_t\")",
    "crumbs": [
      "2.1 Diffusion for Animal Face Images"
    ]
  },
  {
    "objectID": "2_1_nano_diffusion_afhq.html#train",
    "href": "2_1_nano_diffusion_afhq.html#train",
    "title": "Training a Diffusion Model for Animal Face Images",
    "section": "Train",
    "text": "Train\n\nForward diffusion\nForward diffusion can be considered as “data augmentation” in the training step. It adds noise to the data to challenge the model to be able to tell apart signal from noise. Diffusion is a particular way of adding noise, with much mathematical rigor.\n\n# Model components\ndef forward_diffusion(x_0, t, noise_schedule, noise=None):\n    _ts = t.view(-1, 1, 1, 1)\n    if noise is None:\n        noise = torch.randn_like(x_0)\n    assert _ts.max() &lt; len(noise_schedule[\"alphas_cumprod\"]), f\"t={_ts.max()} is larger than the length of noise_schedule: {len(noise_schedule['alphas_cumprod'])}\"\n    alpha_prod_t = noise_schedule[\"alphas_cumprod\"][_ts]\n    x_t = (alpha_prod_t ** 0.5) * x_0 + ((1 - alpha_prod_t) ** 0.5) * noise\n    return x_t, noise\n\n\n\nVisualizing forward diffusion on an image\n\n# Let's see what forward diffusion does.\nx_0, _ = next(iter(val_dataloader))\nx_0 = x_0.to(device)\nx_t_list = []\ncommon_noise = torch.randn_like(x_0)\n# print(f\"x_0 std:\", x_0[0].std().item())\n\nnoise_levels = [0, 10, 50, 100, 500]\nfor t in noise_levels:\n  t = torch.full((x_0.shape[0],), t, device=device)\n  x_t_list.append(forward_diffusion(x_0, t, noise_schedule, noise=common_noise)[0])\n\n# visualize\nplt.figure(figsize=(20, 5))\nfor i, (t, x_t) in enumerate(zip(noise_levels, x_t_list)):\n  # print(x_t[0].min().item(), x_t[0].max().item())\n  plt.subplot(1, 10, i+1)\n  plt.title(f\"t={t}, std={x_t[0].std():.2f}\")\n  img = x_t[0].cpu().numpy().transpose(1, 2, 0)\n  img = (img - img.min()) / (img.max() - img.min())\n  plt.imshow(img)\n  if i &gt;= 10:\n    break\n\n\n\n\n\n\n\n\n\n\nTraining loop\n\nfrom tqdm.auto import tqdm\nimport itertools\n\ncriterion = MSELoss()\ndenoising_model.train()\n\ndef train(denoising_model, steps=100):\n  print(\"Training on device:\", device)\n  max_train_steps = steps\n\n  train_progress_bar = tqdm(enumerate(itertools.cycle(train_dataloader)))\n\n  num_examples = 0\n  for step, (x_0, _) in train_progress_bar:\n    x_0 = x_0.to(device)  # x_0 is the clean data to teach the model to generate\n    optimizer.zero_grad()\n\n    noise = torch.randn(x_0.shape).to(device)\n    t = torch.randint(0, config.num_denoising_steps, (x_0.shape[0],), device=device).long()\n    x_t, true_noise = forward_diffusion(x_0, t, noise_schedule, noise=noise)\n\n    predicted_noise = denoising_model(t=t, x=x_t)\n\n    loss = criterion(predicted_noise, true_noise)\n    loss.backward();\n    # torch.nn.utils.clip_grad_norm_(denoising_model.parameters(), 1)  # try commenting it out\n    optimizer.step()\n\n    train_progress_bar.set_postfix({\"loss\": loss.cpu().item()})\n    num_examples += len(x_0)\n\n    if step &gt;= max_train_steps:\n      print(f\"Reached the max training steps:\", max_train_steps)\n      break\n\n  print(f\"Trained on {num_examples} examples.\")\n  return loss\n\n\nloss = train(denoising_model, steps=100)\n\nTraining on device: cuda\n\n\n\n\n\nReached the max training steps: 100\nTrained on 3232 examples.",
    "crumbs": [
      "2.1 Diffusion for Animal Face Images"
    ]
  },
  {
    "objectID": "2_1_nano_diffusion_afhq.html#generate",
    "href": "2_1_nano_diffusion_afhq.html#generate",
    "title": "Training a Diffusion Model for Animal Face Images",
    "section": "Generate",
    "text": "Generate\n\nThe sampling algo\n\ndef denoising_step(denoising_model, x_t, t, noise_schedule, thresholding=False, clip_sample=True, clip_sample_range=1.0):\n    \"\"\"\n    This is the backward diffusion step, with the effect of denoising.\n    \"\"\"\n    if isinstance(t, int):\n        t_tensor = torch.full((x_t.shape[0],), t, device=x_t.device)\n    else:\n        t_tensor = t\n    with torch.no_grad():\n        model_output = denoising_model(t=t_tensor, x=x_t)\n    if hasattr(model_output, \"sample\"):\n        model_output = model_output.sample\n\n    # Extract relevant values from noise_schedule\n    alpha_prod_t = noise_schedule[\"alphas_cumprod\"][t_tensor]\n    # deal with t=0 case where t can be a tensor\n    alpha_prod_t_prev = torch.where(t_tensor &gt; 0,\n                                    noise_schedule[\"alphas_cumprod\"][t_tensor - 1],\n                                    torch.ones_like(t_tensor, device=x_t.device))\n\n    # Reshape alpha_prod_t_prev for proper broadcasting\n    alpha_prod_t = alpha_prod_t.view(-1, 1, 1, 1)\n    alpha_prod_t_prev = alpha_prod_t_prev.view(-1, 1, 1, 1)\n\n    beta_prod_t = 1 - alpha_prod_t\n    beta_prod_t_prev = 1 - alpha_prod_t_prev\n    current_alpha_t = alpha_prod_t / alpha_prod_t_prev\n    current_beta_t = 1 - current_alpha_t\n\n    # Compute the previous sample mean\n    pred_original_sample = (x_t - beta_prod_t ** 0.5 * model_output) / alpha_prod_t ** 0.5\n\n    if clip_sample:\n        pred_original_sample = torch.clamp(pred_original_sample, -clip_sample_range, clip_sample_range)\n\n    # Compute the coefficients for pred_original_sample and current sample\n    pred_original_sample_coeff = (alpha_prod_t_prev ** 0.5 * current_beta_t) / beta_prod_t\n    current_sample_coeff = current_alpha_t ** 0.5 * beta_prod_t_prev / beta_prod_t\n\n    # Compute the previous sample\n    pred_prev_sample = pred_original_sample_coeff * pred_original_sample + current_sample_coeff * x_t\n\n    # Add noise\n    variance = torch.zeros_like(x_t)\n    variance_noise = torch.randn_like(x_t)\n\n    # Handle t=0 case where t can be a tensor\n    non_zero_mask = (t_tensor != 0).float().view(-1, 1, 1, 1)\n    variance = non_zero_mask * ((1 - alpha_prod_t_prev) / (1 - alpha_prod_t) * current_beta_t)\n    variance = torch.clamp(variance, min=1e-20)\n\n    pred_prev_sample = pred_prev_sample + (variance ** 0.5) * variance_noise\n\n    return pred_prev_sample\n\n\ndef denoising_step_direct(\n    denoising_model,\n    x_t,\n    t,\n    noise_schedule,\n    clip_sample=True,\n    clip_sample_range=1.0,\n):\n    \"\"\"\n    This is the backward diffusion step, with the effect of denoising.\n    \"\"\"\n    if isinstance(t, int):\n        t_tensor = torch.full((x_t.shape[0],), t, device=x_t.device)\n    else:\n        t_tensor = t\n\n    with torch.no_grad():\n        eps_theta = denoising_model(t=t_tensor, x=x_t)\n    if hasattr(eps_theta, \"sample\"):\n        eps_theta = eps_theta.sample\n\n    # Extract alphas from noise schedule\n    alpha_t = noise_schedule[\"alphas\"][t_tensor]\n    alpha_t_cumprod = noise_schedule[\"alphas_cumprod\"][t_tensor]\n\n    # Reshape for broadcasting\n    view_shape = (-1,) + (1,) * (x_t.ndim - 1)\n    alpha_t = alpha_t.view(*view_shape)\n    alpha_t_cumprod = alpha_t_cumprod.view(*view_shape)\n\n    # Calculate epsilon factor\n    eps_factor = (1 - alpha_t) / (1 - alpha_t_cumprod).sqrt()\n\n    # Calculate mean for reverse process\n    mean = (1 / torch.sqrt(alpha_t)) * (x_t - eps_factor * eps_theta)\n\n    # Apply clipping\n    # if clip_sample:\n    #     mean = torch.clamp(mean, -clip_sample_range, clip_sample_range)\n\n    # Add noise scaled by variance for non-zero timesteps\n    noise = torch.randn_like(x_t)\n    beta_t = 1 - alpha_t\n    variance = beta_t * (1 - alpha_t_cumprod / alpha_t) / (1 - alpha_t_cumprod)\n    variance = torch.clamp(variance, min=1e-20)  # Add clamp to prevent numerical instability\n    \n    # Mask out noise for t=0 timesteps\n    non_zero_mask = (t_tensor &gt; 0).float().view(*view_shape)\n    noise_scale = torch.sqrt(variance) * non_zero_mask\n    \n    pred_prev_sample = mean + noise_scale * noise\n\n    # Apply clipping\n    if clip_sample:\n        pred_prev_sample = torch.clamp(pred_prev_sample, -clip_sample_range, clip_sample_range)\n\n    return pred_prev_sample\n\ndef generate_samples_by_denoising(denoising_model, x_T, noise_schedule, n_T, device, thresholding=False, clip_sample=True, clip_sample_range=1.0, seed=0, method=\"direct\"):\n    \"\"\"\n    This is the generation process.\n    \"\"\"\n    torch.manual_seed(seed)\n\n    x_t = x_T.to(device)\n    pbar = tqdm(range(n_T - 1, -1, -1))\n    for t in pbar:\n        if method == \"direct\":\n            x_t = denoising_step_direct(denoising_model, x_t, t, noise_schedule, clip_sample, clip_sample_range)\n        else:\n            x_t = denoising_step(denoising_model, x_t, t, noise_schedule, thresholding, clip_sample, clip_sample_range)\n        pbar.set_postfix({\"std\": x_t.std().item()})\n\n    # print(\"raw x_t range\", x_t.min(), x_t.max())\n    x_t = (x_t / 2 + 0.5).clamp(0, 1)\n    # print(\"after clamp\", x_t.min(), x_t.max())\n    return x_t\n\n\n\nVisualize sampled images\n\n# visualize the sampled images\ndef visualize_sampled_images(method=None):\n  print(\"Loss of the denoising model:\", loss.item())\n  x_T = torch.randn(16, 3, 32, 32)\n  x_sampled = generate_samples_by_denoising(\n    denoising_model, x_T,\n    noise_schedule, n_T=1000,\n    device=device,\n    clip_sample=True,\n    clip_sample_range=1.0,\n    method=method,\n  )\n  x_sampled = (x_sampled * 255)\n\n  sampled = make_grid(x_sampled).permute(1, 2, 0).cpu().numpy().astype(np.uint8)\n  _ = plt.imshow(sampled)\n\n\nvisualize_sampled_images(method=\"direct\")\n\nLoss of the denoising model: 0.061633966863155365\n\n\n\n\n\n\n\n\n\n\n\n\n\nvisualize_sampled_images()\n\nLoss of the denoising model: 0.061633966863155365\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTrain some more\n\n# Train some more\nloss = train(denoising_model, steps=1000)\nprint(\"loss:\", loss.item())\n\nTraining on device: cuda\n\n\n\n\n\nReached the max training steps: 1000\nTrained on 31998 examples.\nloss: 0.02666069194674492\n\n\n\nvisualize_sampled_images()\n\nLoss of the denoising model: 0.02666069194674492\n\n\n\n\n\n\n\n\n\n\n\n\n\nvisualize_sampled_images(method=\"direct\")\n\nLoss of the denoising model: 0.02666069194674492\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTrain even more\n\nfor g in optimizer.param_groups:\n    g['lr'] = 1e-4  # reduce learning rate\nloss = train(denoising_model, steps=5000)\nprint(\"loss:\", loss.item())\nvisualize_sampled_images()\n\nTraining on device: cuda\n\n\n\n\n\nReached the max training steps: 5000\nTrained on 159828 examples.\nloss: 0.06194191426038742\nLoss of the denoising model: 0.06194191426038742\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnd, some more\n\nfor g in optimizer.param_groups:\n    g['lr'] = 1e-4  # reduce learning rate\nloss = train(denoising_model, steps=10000)\nprint(\"loss:\", loss.item())\nvisualize_sampled_images()\n\nTraining on device: cuda\n\n\n\n\n\nReached the max training steps: 10000\nTrained on 319624 examples.\nloss: 0.028765305876731873\nLoss of the denoising model: 0.028765305876731873\n\n\n\n\n\n\n\n\n\n\n\n\n\nloss = train(denoising_model, steps=10000)\nprint(\"loss:\", loss.item())\nvisualize_sampled_images()\n\nTraining on device: cuda",
    "crumbs": [
      "2.1 Diffusion for Animal Face Images"
    ]
  },
  {
    "objectID": "1_1_b_Diffusion_2D_hyperparams.html",
    "href": "1_1_b_Diffusion_2D_hyperparams.html",
    "title": "Model Hyperparameter Comparison",
    "section": "",
    "text": "from lib_1_1.eval import chamfer_distance\nfrom lib_1_1.model import Model1, Model2, Model3\nfrom lib_1_1.training_loop  import train\nfrom lib_1_1.data import load_data\nfrom lib_1_1.config import TrainingConfig\nfrom lib_1_1.diffusion import generate_samples_by_denoising, create_noise_schedule\nfrom dataclasses import dataclass\nimport torch\nimport torch.optim as optim\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n@dataclass\nclass ModelConfig:\n    model: str\n    hidden_features: list[int]\n    train_steps: int\n\nmodels = [\"model1\", \"model2\", \"model3\"]\nlayers = [[128], [256, 256], [512, 512], [256, 256, 256, 256, 256]]\ntrain_steps = [1000, 5000, 10000]\n\nmodel_configs = [\n    ModelConfig(model=model, hidden_features=layer, train_steps=num_timestep)\n    for model in models for layer in layers for num_timestep in train_steps\n]\nprint(f\"Number of model configs: {len(model_configs)}\")\n\nNumber of model configs: 36",
    "crumbs": [
      "1.1b Experimenting with diffusion recipes"
    ]
  },
  {
    "objectID": "1_1_b_Diffusion_2D_hyperparams.html#define-the-grid-of-hyperparameters",
    "href": "1_1_b_Diffusion_2D_hyperparams.html#define-the-grid-of-hyperparameters",
    "title": "Model Hyperparameter Comparison",
    "section": "",
    "text": "from lib_1_1.eval import chamfer_distance\nfrom lib_1_1.model import Model1, Model2, Model3\nfrom lib_1_1.training_loop  import train\nfrom lib_1_1.data import load_data\nfrom lib_1_1.config import TrainingConfig\nfrom lib_1_1.diffusion import generate_samples_by_denoising, create_noise_schedule\nfrom dataclasses import dataclass\nimport torch\nimport torch.optim as optim\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n@dataclass\nclass ModelConfig:\n    model: str\n    hidden_features: list[int]\n    train_steps: int\n\nmodels = [\"model1\", \"model2\", \"model3\"]\nlayers = [[128], [256, 256], [512, 512], [256, 256, 256, 256, 256]]\ntrain_steps = [1000, 5000, 10000]\n\nmodel_configs = [\n    ModelConfig(model=model, hidden_features=layer, train_steps=num_timestep)\n    for model in models for layer in layers for num_timestep in train_steps\n]\nprint(f\"Number of model configs: {len(model_configs)}\")\n\nNumber of model configs: 36",
    "crumbs": [
      "1.1b Experimenting with diffusion recipes"
    ]
  },
  {
    "objectID": "1_1_b_Diffusion_2D_hyperparams.html#run-model-training-experiments",
    "href": "1_1_b_Diffusion_2D_hyperparams.html#run-model-training-experiments",
    "title": "Model Hyperparameter Comparison",
    "section": "Run model training experiments",
    "text": "Run model training experiments\nThis can take about 10 minutes to run.\n\ntrain_config = TrainingConfig()\ntrain_dataloader, _ = load_data(train_config)\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n\nnoise_schedule = create_noise_schedule(train_config.num_denoising_steps, device)\nlosses = []\nchamfer_distances = []\ntarget_spiral = next(iter(train_dataloader))\nfor i, m_config in enumerate(model_configs):\n    print(f\"Experiment run {i+1} of {len(model_configs)}\")\n    model = None\n    if m_config.model == \"model1\":\n        model = Model1(hidden_features=m_config.hidden_features, num_timesteps=m_config.train_steps).to(device)\n    elif m_config.model == \"model2\":\n        model = Model2(hidden_features=m_config.hidden_features, num_timesteps=m_config.train_steps).to(device)\n    elif m_config.model == \"model3\":\n        model = Model3(\n            hidden_features=m_config.hidden_features,\n            num_timesteps=m_config.train_steps,\n        ).to(device)\n    print(f\"{m_config.model}, Layers: {m_config.hidden_features}, Train Steps: {m_config.train_steps}\")\n    print(f\"model params: {sum(p.numel() for p in model.parameters())}\")\n    optimizer = optim.AdamW(model.parameters(), lr=train_config.learning_rate, weight_decay=train_config.weight_decay)\n    loss = train(model, train_dataloader=train_dataloader, optimizer=optimizer, steps=m_config.train_steps, noise_schedule=noise_schedule, device=device)\n    print(\"loss:\", loss.item())\n    generated_points = generate_samples_by_denoising(model, torch.randn(128, 2), noise_schedule, n_T=1000, device=device)\n    generated_points = generated_points.cpu().numpy()\n    if m_config.model == \"model1\":\n        # special treatment for model1\n        generated_points = np.clip(generated_points, -3.6, 3.6)\n    chamfer_dist = chamfer_distance(generated_points, target_spiral, direction='bi')\n    chamfer_distances.append(chamfer_dist)\n    print(\"Chamfer Distance:\", chamfer_dist)\n\nmodel1, Layers: [128], Train Steps: 1000\nmodel params: 770\nTraining on device: cuda\n\n\n\n\n\nReached the max training steps: 1000\nloss: 0.4509263038635254\n\n\n100%|██████████| 1000/1000 [00:00&lt;00:00, 1025.70it/s, std=50.1]\n\n\nChamfer Distance: 40.83210172000426\nmodel1, Layers: [128], Train Steps: 5000\nmodel params: 770\nTraining on device: cuda\n\n\n\n\n\nReached the max training steps: 5000\nloss: 0.46364229917526245\n\n\n100%|██████████| 1000/1000 [00:00&lt;00:00, 1068.78it/s, std=64.4]\n\n\nChamfer Distance: 47.438745148771524\nmodel1, Layers: [128], Train Steps: 10000\nmodel params: 770\nTraining on device: cuda\n\n\n\n\n\nReached the max training steps: 10000\nloss: 0.4620330333709717\n\n\n100%|██████████| 1000/1000 [00:00&lt;00:00, 1053.48it/s, std=58.7]\n\n\nChamfer Distance: 43.8795945442217\nmodel1, Layers: [256, 256], Train Steps: 1000\nmodel params: 67330\nTraining on device: cuda\n\n\n\n\n\nReached the max training steps: 1000\nloss: 0.433613121509552\n\n\n100%|██████████| 1000/1000 [00:00&lt;00:00, 1025.87it/s, std=55.3]\n\n\nChamfer Distance: 34.900078827080634\nmodel1, Layers: [256, 256], Train Steps: 5000\nmodel params: 67330\nTraining on device: cuda\n\n\n\n\n\nReached the max training steps: 5000\nloss: 0.41779962182044983\n\n\n100%|██████████| 1000/1000 [00:00&lt;00:00, 1030.43it/s, std=49.4]\n\n\nChamfer Distance: 37.8007218336046\nmodel1, Layers: [256, 256], Train Steps: 10000\nmodel params: 67330\nTraining on device: cuda\n\n\n\n\n\nReached the max training steps: 10000\nloss: 0.4159809947013855\n\n\n100%|██████████| 1000/1000 [00:00&lt;00:00, 1027.20it/s, std=56.5]\n\n\nChamfer Distance: 41.10801919781987\nmodel1, Layers: [512, 512], Train Steps: 1000\nmodel params: 265730\nTraining on device: cuda\n\n\n\n\n\nReached the max training steps: 1000\nloss: 0.4471423327922821\n\n\n100%|██████████| 1000/1000 [00:01&lt;00:00, 982.32it/s, std=64.2]\n\n\nChamfer Distance: 44.991998336324244\nmodel1, Layers: [512, 512], Train Steps: 5000\nmodel params: 265730\nTraining on device: cuda\n\n\n\n\n\nReached the max training steps: 5000\nloss: 0.3601759076118469\n\n\n100%|██████████| 1000/1000 [00:00&lt;00:00, 1003.78it/s, std=64] \n\n\nChamfer Distance: 47.84504672777196\nmodel1, Layers: [512, 512], Train Steps: 10000\nmodel params: 265730\nTraining on device: cuda\n\n\n\n\n\nReached the max training steps: 10000\nloss: 0.44915974140167236\n\n\n100%|██████████| 1000/1000 [00:01&lt;00:00, 993.84it/s, std=56.1]\n\n\nChamfer Distance: 40.11150077540098\nmodel1, Layers: [256, 256, 256, 256, 256], Train Steps: 1000\nmodel params: 264706\nTraining on device: cuda\n\n\n\n\n\nReached the max training steps: 1000\nloss: 0.42826491594314575\n\n\n100%|██████████| 1000/1000 [00:01&lt;00:00, 938.80it/s, std=64.7]\n\n\nChamfer Distance: 45.433996849178996\nmodel1, Layers: [256, 256, 256, 256, 256], Train Steps: 5000\nmodel params: 264706\nTraining on device: cuda\n\n\n\n\n\nReached the max training steps: 5000\nloss: 0.43620750308036804\n\n\n100%|██████████| 1000/1000 [00:01&lt;00:00, 956.80it/s, std=68.4]\n\n\nChamfer Distance: 47.47955309238842\nmodel1, Layers: [256, 256, 256, 256, 256], Train Steps: 10000\nmodel params: 264706\nTraining on device: cuda\n\n\n\n\n\nReached the max training steps: 10000\nloss: 0.4281949996948242\n\n\n100%|██████████| 1000/1000 [00:01&lt;00:00, 941.21it/s, std=56.2]\n\n\nChamfer Distance: 40.01103618165592\nmodel2, Layers: [128], Train Steps: 1000\nmodel params: 4738\nTraining on device: cuda\n\n\n\n\n\nReached the max training steps: 1000\nloss: 0.3815053701400757\n\n\n100%|██████████| 1000/1000 [00:00&lt;00:00, 1000.22it/s, std=1.9]\n\n\nChamfer Distance: 0.9952134982106191\nmodel2, Layers: [128], Train Steps: 5000\nmodel params: 4738\nTraining on device: cuda\n\n\n\n\n\nReached the max training steps: 5000\nloss: 0.342546284198761\n\n\n100%|██████████| 1000/1000 [00:00&lt;00:00, 1014.42it/s, std=2.02]\n\n\nChamfer Distance: 0.9870064848257294\nmodel2, Layers: [128], Train Steps: 10000\nmodel params: 4738\nTraining on device: cuda\n\n\n\n\n\nReached the max training steps: 10000\nloss: 0.40761643648147583\n\n\n100%|██████████| 1000/1000 [00:00&lt;00:00, 1019.49it/s, std=1.95]\n\n\nChamfer Distance: 0.826520714937363\nmodel2, Layers: [256, 256], Train Steps: 1000\nmodel params: 75266\nTraining on device: cuda\n\n\n\n\n\nReached the max training steps: 1000\nloss: 0.40317225456237793\n\n\n100%|██████████| 1000/1000 [00:01&lt;00:00, 990.18it/s, std=1.98]\n\n\nChamfer Distance: 0.9103175682988993\nmodel2, Layers: [256, 256], Train Steps: 5000\nmodel params: 75266\nTraining on device: cuda\n\n\n\n\n\nReached the max training steps: 5000\nloss: 0.4225660264492035\n\n\n100%|██████████| 1000/1000 [00:01&lt;00:00, 976.46it/s, std=1.98]\n\n\nChamfer Distance: 0.6969788011770026\nmodel2, Layers: [256, 256], Train Steps: 10000\nmodel params: 75266\nTraining on device: cuda\n\n\n\n\n\nReached the max training steps: 10000\nloss: 0.34160494804382324\n\n\n100%|██████████| 1000/1000 [00:01&lt;00:00, 971.04it/s, std=1.95] \n\n\nChamfer Distance: 0.5928464470174432\nmodel2, Layers: [512, 512], Train Steps: 1000\nmodel params: 281602\nTraining on device: cuda\n\n\n\n\n\nReached the max training steps: 1000\nloss: 0.3745132386684418\n\n\n100%|██████████| 1000/1000 [00:00&lt;00:00, 1001.80it/s, std=1.94]\n\n\nChamfer Distance: 0.8723187112974164\nmodel2, Layers: [512, 512], Train Steps: 5000\nmodel params: 281602\nTraining on device: cuda\n\n\n\n\n\nReached the max training steps: 5000\nloss: 0.4065595269203186\n\n\n100%|██████████| 1000/1000 [00:01&lt;00:00, 955.66it/s, std=2]   \n\n\nChamfer Distance: 0.6087286500392258\nmodel2, Layers: [512, 512], Train Steps: 10000\nmodel params: 281602\nTraining on device: cuda\n\n\n\n\n\nReached the max training steps: 10000\nloss: 0.32167112827301025\n\n\n100%|██████████| 1000/1000 [00:00&lt;00:00, 1011.62it/s, std=1.97]\n\n\nChamfer Distance: 0.46802006722301026\nmodel2, Layers: [256, 256, 256, 256, 256], Train Steps: 1000\nmodel params: 272642\nTraining on device: cuda\n\n\n\n\n\nReached the max training steps: 1000\nloss: 0.4069325625896454\n\n\n100%|██████████| 1000/1000 [00:01&lt;00:00, 934.27it/s, std=2]   \n\n\nChamfer Distance: 0.6562601955836662\nmodel2, Layers: [256, 256, 256, 256, 256], Train Steps: 5000\nmodel params: 272642\nTraining on device: cuda\n\n\n\n\n\nReached the max training steps: 5000\nloss: 0.3000778257846832\n\n\n100%|██████████| 1000/1000 [00:01&lt;00:00, 898.61it/s, std=1.94]\n\n\nChamfer Distance: 0.23615802336684183\nmodel2, Layers: [256, 256, 256, 256, 256], Train Steps: 10000\nmodel params: 272642\nTraining on device: cuda\n\n\n\n\n\nReached the max training steps: 10000\nloss: 0.3427009582519531\n\n\n100%|██████████| 1000/1000 [00:01&lt;00:00, 901.10it/s, std=1.93]\n\n\nChamfer Distance: 0.22002034624801564\nmodel3, Layers: [128], Train Steps: 1000\nmodel params: 50570\nTraining on device: cuda\n\n\n\n\n\nReached the max training steps: 1000\nloss: 0.34604138135910034\n\n\n100%|██████████| 1000/1000 [00:01&lt;00:00, 741.65it/s, std=1.93]\n\n\nChamfer Distance: 0.720878815943192\nmodel3, Layers: [128], Train Steps: 5000\nmodel params: 50570\nTraining on device: cuda\n\n\n\n\n\nReached the max training steps: 5000\nloss: 0.26689964532852173\n\n\n100%|██████████| 1000/1000 [00:01&lt;00:00, 738.06it/s, std=2.03]\n\n\nChamfer Distance: 0.281599986112402\nmodel3, Layers: [128], Train Steps: 10000\nmodel params: 50570\nTraining on device: cuda\n\n\n\n\n\nReached the max training steps: 10000\nloss: 0.3543072044849396\n\n\n100%|██████████| 1000/1000 [00:01&lt;00:00, 751.87it/s, std=1.97]\n\n\nChamfer Distance: 0.3179484760186321\nmodel3, Layers: [256, 256], Train Steps: 1000\nmodel params: 330762\nTraining on device: cuda\n\n\n\n\n\nReached the max training steps: 1000\nloss: 0.32084426283836365\n\n\n100%|██████████| 1000/1000 [00:01&lt;00:00, 619.71it/s, std=1.97]\n\n\nChamfer Distance: 0.3826757721628579\nmodel3, Layers: [256, 256], Train Steps: 5000\nmodel params: 330762\nTraining on device: cuda\n\n\n\n\n\nReached the max training steps: 5000\nloss: 0.33935433626174927\n\n\n100%|██████████| 1000/1000 [00:01&lt;00:00, 632.97it/s, std=1.93]\n\n\nChamfer Distance: 0.2178326634163048\nmodel3, Layers: [256, 256], Train Steps: 10000\nmodel params: 330762\nTraining on device: cuda\n\n\n\n\n\nReached the max training steps: 10000\nloss: 0.29220932722091675\n\n\n100%|██████████| 1000/1000 [00:01&lt;00:00, 638.39it/s, std=1.96]\n\n\nChamfer Distance: 0.20583606746893657\nmodel3, Layers: [512, 512], Train Steps: 1000\nmodel params: 1054218\nTraining on device: cuda\n\n\n\n\n\nReached the max training steps: 1000\nloss: 0.29506391286849976\n\n\n100%|██████████| 1000/1000 [00:01&lt;00:00, 641.70it/s, std=2.01]\n\n\nChamfer Distance: 0.35953832038652656\nmodel3, Layers: [512, 512], Train Steps: 5000\nmodel params: 1054218\nTraining on device: cuda\n\n\n\n\n\nReached the max training steps: 5000\nloss: 0.32820066809654236\n\n\n100%|██████████| 1000/1000 [00:01&lt;00:00, 631.76it/s, std=2.03]\n\n\nChamfer Distance: 0.2234631816484437\nmodel3, Layers: [512, 512], Train Steps: 10000\nmodel params: 1054218\nTraining on device: cuda\n\n\n\n\n\nReached the max training steps: 10000\nloss: 0.3356664776802063\n\n\n100%|██████████| 1000/1000 [00:01&lt;00:00, 634.93it/s, std=1.92]\n\n\nChamfer Distance: 0.2181650604105344\nmodel3, Layers: [256, 256, 256, 256, 256], Train Steps: 1000\nmodel params: 922890\nTraining on device: cuda\n\n\n\n\n\nReached the max training steps: 1000\nloss: 0.3741267919540405\n\n\n100%|██████████| 1000/1000 [00:02&lt;00:00, 432.72it/s, std=1.97]\n\n\nChamfer Distance: 0.25825727442566093\nmodel3, Layers: [256, 256, 256, 256, 256], Train Steps: 5000\nmodel params: 922890\nTraining on device: cuda\n\n\n\n\n\nReached the max training steps: 5000\nloss: 0.330197274684906\n\n\n100%|██████████| 1000/1000 [00:02&lt;00:00, 419.03it/s, std=2.02]\n\n\nChamfer Distance: 0.21255801422529738\nmodel3, Layers: [256, 256, 256, 256, 256], Train Steps: 10000\nmodel params: 922890\nTraining on device: cuda\n\n\n\n\n\nReached the max training steps: 10000\nloss: 0.2812938988208771\n\n\n100%|██████████| 1000/1000 [00:02&lt;00:00, 435.70it/s, std=1.94]\n\n\nChamfer Distance: 0.17926968569031748",
    "crumbs": [
      "1.1b Experimenting with diffusion recipes"
    ]
  },
  {
    "objectID": "1_1_b_Diffusion_2D_hyperparams.html#analyze-the-results",
    "href": "1_1_b_Diffusion_2D_hyperparams.html#analyze-the-results",
    "title": "Model Hyperparameter Comparison",
    "section": "Analyze the results",
    "text": "Analyze the results\n\n# Find the best model\nbest_model_idx = np.argmin(chamfer_distances)\n\nbest_model = model_configs[best_model_idx]\nprint(f\"Best model: {best_model}, Chamfer Distance: {chamfer_distances[best_model_idx]}\")\n\nBest model: ModelConfig(model='model3', hidden_features=[256, 256, 256, 256, 256], train_steps=10000), Chamfer Distance: 0.17926968569031748\n\n\n\n# Print a grid of the chamfer distances and the different configs\nimport pandas as pd\n\ndf = pd.DataFrame({\n    \"Model\": [m.model for m in model_configs],\n    \"Layers\": [m.hidden_features for m in model_configs],\n    \"Train Steps\": [m.train_steps for m in model_configs],\n    \"Chamfer Distance\": chamfer_distances,\n})\n\n# add num parameters to df\nnum_parameters = []\nfor m_config in model_configs:\n    if m_config.model == \"model1\":\n        model = Model1(hidden_features=m_config.hidden_features, num_timesteps=m_config.train_steps).to(device)\n    elif m_config.model == \"model2\":\n        model = Model2(hidden_features=m_config.hidden_features, num_timesteps=m_config.train_steps).to(device)\n    elif m_config.model == \"model3\":\n        model = Model3(\n            hidden_features=m_config.hidden_features,\n            num_timesteps=m_config.train_steps,\n        ).to(device)\n    num_parameters.append(sum(p.numel() for p in model.parameters()))\n\ndf[\"Num Parameters\"] = num_parameters\ndf['Quality (Negative Log Chamfer Distance)'] = np.log(1/df['Chamfer Distance'])\n\n\nPlot quality vs. model size\n\n# uses seaborn scatterplot to plot the scaling law\n# only uses 10,000 timesteps for the scaling law\n\nimport seaborn as sns\n\n# We fix the number of timesteps to 10,000.\nselected_df = df[df[\"Train Steps\"] == 10000]\n# model2 or 3\n# df = df[df[\"Model\"] != \"model1\"]\n\n#color based on model\n# adds layers as very small text next to point\nsns.scatterplot(data=selected_df, x=\"Num Parameters\", y=\"Quality (Negative Log Chamfer Distance)\", hue=\"Model\")\nfor i in range(len(selected_df)):\n    plt.text(\n        selected_df[\"Num Parameters\"].iloc[i],\n        selected_df[\"Quality (Negative Log Chamfer Distance)\"].iloc[i],\n        str(selected_df[\"Layers\"].iloc[i]), fontsize=5)\n\nplt.title(\"Quality vs. Model Size\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\nPlot quality vs. training steps\n\n\n# We only look at the 5 layer model\nselected_df = df[df[\"Layers\"].astype(str) == '[256, 256, 256, 256, 256]']\nsns.lineplot(data=selected_df, x=\"Train Steps\", y=\"Quality (Negative Log Chamfer Distance)\", hue=\"Model\", style=\"Model\", markers=True, dashes=False)\nplt.title(\"Quality vs. Training Steps\")\nplt.show()",
    "crumbs": [
      "1.1b Experimenting with diffusion recipes"
    ]
  },
  {
    "objectID": "1_1_b_Diffusion_2D_hyperparams.html#discussion-questions",
    "href": "1_1_b_Diffusion_2D_hyperparams.html#discussion-questions",
    "title": "Model Hyperparameter Comparison",
    "section": "Discussion questions",
    "text": "Discussion questions\n\nIs it helpful to make the model wide or deep (e.g. [512, 512] vs. [256, 256, 256, 256, 256])?\nIs it helpful to train for more steps?\nWhat is the model architecture? Does model 3 dominate the other models?",
    "crumbs": [
      "1.1b Experimenting with diffusion recipes"
    ]
  },
  {
    "objectID": "1_1_Diffusion 2D Toy.html",
    "href": "1_1_Diffusion 2D Toy.html",
    "title": "Training a Diffusion Model on 2-D Points",
    "section": "",
    "text": "Diffusion models have revolutionized generative AI, but they can be complex to understand. In this tutorial, we’ll build intuition by training a diffusion model on a simple 2D spiral dataset. This allows us to visualize the entire process and understand key concepts without the complexity of image generation.\nIn this notebook, you’ll learn how to train a diffusion model to generate spirals of 2-D points.",
    "crumbs": [
      "1.1 Diffusion for a 2D Point Cloud"
    ]
  },
  {
    "objectID": "1_1_Diffusion 2D Toy.html#what-well-cover",
    "href": "1_1_Diffusion 2D Toy.html#what-well-cover",
    "title": "Training a Diffusion Model on 2-D Points",
    "section": "What We’ll Cover",
    "text": "What We’ll Cover\n\nForward diffusion: How to gradually add noise to data\nReverse diffusion: How to learn to denoise data\nThree different model architectures and their tradeoffs\nVisualization of the diffusion process",
    "crumbs": [
      "1.1 Diffusion for a 2D Point Cloud"
    ]
  },
  {
    "objectID": "1_1_Diffusion 2D Toy.html#why-2d-points",
    "href": "1_1_Diffusion 2D Toy.html#why-2d-points",
    "title": "Training a Diffusion Model on 2-D Points",
    "section": "Why 2D Points?",
    "text": "Why 2D Points?\nWorking with 2D points offers several advantages: - Fast training on CPU - Easy visualization of the entire process - Clear understanding of how diffusion models work\nWe’ll explore the different components of diffusion models and their functions, as well as compare the quality of generated results using different model architectures.\n\nfrom dataclasses import dataclass\nimport math\nfrom typing import Dict, Tuple\n\nfrom matplotlib import pyplot as plt\nfrom sklearn.datasets import make_swiss_roll\nimport torch\nfrom torch import nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom torch.nn import MSELoss\nfrom torch.utils.data import DataLoader\nfrom tqdm.auto import tqdm",
    "crumbs": [
      "1.1 Diffusion for a 2D Point Cloud"
    ]
  },
  {
    "objectID": "1_1_Diffusion 2D Toy.html#dataloaders",
    "href": "1_1_Diffusion 2D Toy.html#dataloaders",
    "title": "Training a Diffusion Model on 2-D Points",
    "section": "Dataloaders",
    "text": "Dataloaders\nBefore training the model, we need to define some hyperparameters and create our train and validation dataloaders.\n\ndevice = torch.device(\"cpu\")\n\n@dataclass\nclass TrainingConfig:\n    batch_size: int = 256 # batch size\n    learning_rate: float = 5e-4 # initial learning rate\n    # learning_rate: float = 1e-3 # initial learning rate\n    weight_decay: float = 1e-6 # weight decay\n    num_denoising_steps: int = 1000 # number of timesteps\n\n\ndef load_data(config: TrainingConfig) -&gt; Tuple[DataLoader, DataLoader]:\n    \"\"\"\n    Load the data and return the train and validation dataloaders.\n\n    Args:\n      config: TrainingConfig object.\n    Returns:\n      train_dataloader: DataLoader for the training data.\n      val_dataloader: DataLoader for the validation data.\n    \"\"\"\n    n = int(1e+6)\n    x, _ = make_swiss_roll(n_samples=n, noise=0)\n    x = x[:, [0, 2]]\n    scaling = 2\n    x = (x - x.mean()) / x.std() * scaling\n    x_train = x[:int(n * 0.8), :]\n    x_val = x[int(n * 0.8):, :]\n\n    class SimpleDataset:\n      def __init__(self, data):\n        self.data = data\n\n      def __len__(self):\n        return len(self.data)\n\n      def __getitem__(self, i):\n        return self.data[i]\n\n    train_dataset = SimpleDataset(x_train)\n    val_dataset = SimpleDataset(x_val)\n    train_dataloader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True, num_workers=0)\n    val_dataloader = DataLoader(val_dataset, batch_size=config.batch_size, shuffle=False, num_workers=0)\n\n    return train_dataloader, val_dataloader\n\n\nconfig = TrainingConfig()\ntrain_dataloader, val_dataloader = load_data(config)\nfirst_batch = next(iter(train_dataloader))\nprint(\"batch shape:\", first_batch.shape)\n\nbatch shape: torch.Size([256, 2])",
    "crumbs": [
      "1.1 Diffusion for a 2D Point Cloud"
    ]
  },
  {
    "objectID": "1_1_Diffusion 2D Toy.html#the-models",
    "href": "1_1_Diffusion 2D Toy.html#the-models",
    "title": "Training a Diffusion Model on 2-D Points",
    "section": "The Models",
    "text": "The Models\nNow it’s time to define the models. We’ll implement three different model architectures to incorporate time information:\nModel 1: Basic Time Embedding - Simplest approach: direct concatenation of time with input. - Pros: Easy to implement. - Cons: Limited temporal understanding.\nModel 2: Sinusoidal Time Embedding - Inspired by transformer positional encodings. - Better handles periodic patterns.\nModel 3: Time-Modulated Linear Layers - The output of each linear transformation is modulated by a learned time embedding through element-wise multiplication, allowing the network to adapt its representations based on the timestep. - Time information influences every layer.\n\nModel 1: Timestep Concatination\nOur first model is a simple three layer MLP. Note that in the foward method, we concatinate the time to the input before each foward pass. This allows the model to have a sense of the where it is in the denoising process.\n\nclass Model1(nn.Module):\n    def __init__(self, hidden_features: list[int]):\n        super().__init__()\n        \n        layers = []\n        input_dim = 3  # The input dimension (t and x combined)\n        \n        for hidden_dim in hidden_features:\n            layers.append(nn.Linear(input_dim, hidden_dim))\n            layers.append(nn.ReLU())\n            input_dim = hidden_dim\n        \n        # Output layer\n        layers.append(nn.Linear(input_dim, 2))\n        layers.append(nn.Tanh())\n        \n        self.net = nn.Sequential(*layers)\n\n    def forward(self, t, x):\n        t = t / 1000.\n        t = t.reshape(-1, 1)\n        return self.net(torch.cat([t, x], 1))\n\n# Example usage\nhidden_dimensions = [512, 512]  # Example list of hidden dimensions\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel1 = Model1(hidden_dimensions).to(device)\n\nprint(f\"Model parameters: {sum(p.numel() for p in model1.parameters())}\")\n\nModel parameters: 265730\n\n\n\n\nModel 2: Sinusoidal Time Embedding Concatination\nIn this model, we utilize a 3-layer MLP. Instead of concatenating the raw time embedding directly to the image embedding, we first transform the time embedding into a sinusoidal embedding and then concatenate it with the image embedding.\nHistory\nThis model builds upon the idea of incorporating temporal information into the embedding space, inspired by the effectiveness of sinusoidal embeddings in positional encoding for transformers. The sinusoidal representation allows the model to capture periodicity and smooth transitions in the time domain, potentially improving its ability to generalize across different temporal contexts.\n\nclass MLP(nn.Sequential):\n    def __init__(\n        self,\n        in_features: int,\n        out_features: int,\n        hidden_features: list[int],\n    ):\n        layers = []\n\n        for a, b in zip(\n            (in_features, *hidden_features),\n            (*hidden_features, out_features),\n        ):\n            layers.extend([nn.Linear(a, b), nn.ELU()])\n\n        super().__init__(*layers[:-1])\n\n\nclass Model2(nn.Module):\n    def __init__(self, features: int, freqs: int = 16, **kwargs):\n        super().__init__()\n\n        self.net = MLP(2 * freqs + features, features, **kwargs)\n\n        self.register_buffer('freqs', torch.arange(1, freqs + 1) * torch.pi)\n\n    def forward(self, t, x):\n        t = t / 1000.\n        t = self.freqs * t[..., None]\n        t = torch.cat((t.cos(), t.sin()), dim=-1)\n        t = t.expand(*x.shape[:-1], -1)\n\n        return self.net(torch.cat((t, x), dim=-1))\n\n\nmodel2 = Model2(features=2, hidden_features=[512, 512]).to(device)\nprint(f\"model params: {sum(p.numel() for p in model2.parameters())}\")\n\nmodel params: 281602\n\n\n\n\nModel 3: Time-Modulated Linear Layers\nModel 3 introduces a time-conditioned architecture where time is incorporated directly into the network through TimeLinear layers. Instead of directly concatenating time embeddings with the image embeddings, the output of each linear transformation is modulated by a learned time embedding through element-wise multiplication, allowing the network to adapt its representations based on the timestep.\n\nclass TimeEmbedding(nn.Module):\n    \"\"\"\n    Time embedding module that embeds the time step into a hidden representation.\n    Args:\n        hidden_size: the size of the hidden representation.\n        frequency_embedding_size: the size of the frequency embedding\n    \"\"\"\n    def __init__(self, hidden_size, frequency_embedding_size=256):\n        super().__init__()\n        self.mlp = nn.Sequential(\n            nn.Linear(frequency_embedding_size, hidden_size, bias=True),\n            nn.SiLU(),\n            nn.Linear(hidden_size, hidden_size, bias=True),\n        )\n        self.frequency_embedding_size = frequency_embedding_size\n\n    @staticmethod\n    def timestep_embedding(t, dim, max_period=10000):\n        \"\"\"\n        Create sinusoidal timestep embeddings.\n        :param t: a 1-D Tensor of N indices, one per batch element.\n                          These may be fractional.\n        :param dim: the dimension of the output.\n        :param max_period: controls the minimum frequency of the embeddings.\n        :return: an (N, D) Tensor of positional embeddings.\n        \"\"\"\n        # https://github.com/openai/glide-text2im/blob/main/glide_text2im/nn.py\n        half = dim // 2\n        freqs = torch.exp(\n            -math.log(max_period)\n            * torch.arange(start=0, end=half, dtype=torch.float32)\n            / half\n        ).to(device=t.device)\n        args = t[:, None].float() * freqs[None]\n        embedding = torch.cat([torch.cos(args), torch.sin(args)], dim=-1)\n        if dim % 2:\n            embedding = torch.cat(\n                [embedding, torch.zeros_like(embedding[:, :1])], dim=-1\n            )\n        return embedding\n\n    def forward(self, t: torch.Tensor):\n        \"\"\"Forward pass of the module.\"\"\"\n\n        if t.ndim == 0:\n            t = t.unsqueeze(-1)\n        t_freq = self.timestep_embedding(t, self.frequency_embedding_size)\n        t_emb = self.mlp(t_freq)\n        return t_emb\n\n\nclass TimeLinear(nn.Module):\n    \"\"\"\n    Time linear layer that applies a linear transformation with time-dependent weights.\n    \"\"\"\n\n    def __init__(self, dim_in: int, dim_out: int, num_timesteps: int):\n        \"\"\"\n        Args:\n            dim_in: the number of input features.\n            dim_out: the number of output features.\n            num_timesteps: the number of timesteps.\n        \"\"\"\n        super().__init__()\n        self.dim_in = dim_in\n        self.dim_out = dim_out\n        self.num_timesteps = num_timesteps\n\n        self.time_embedding = TimeEmbedding(dim_out)\n        self.fc = nn.Linear(dim_in, dim_out)\n\n    def forward(self, x: torch.Tensor, t: torch.Tensor):\n        x = self.fc(x)\n        alpha = self.time_embedding(t).view(-1, self.dim_out)\n        return alpha * x\n\n\nclass Model3(nn.Module):\n    def __init__(\n        self, hidden_features: list[int], num_timesteps: int, dim_in:int = 2, dim_out:int = 2,\n    ):\n        super().__init__()\n        \"\"\"\n        Build a noise estimating network.\n\n        Args:\n            hidden_features: dimensions of hidden features\n            num_timesteps: number of timesteps\n            dim_in: dimension of input\n            dim_out: dimension of output\n        \"\"\"\n\n        layers = []\n        dims = [dim_in] + hidden_features\n\n        # Build MLP layers with time-dependent linear layers\n        for i in range(len(dims)-1):\n            layers.append(TimeLinear(dims[i], dims[i+1], num_timesteps))\n            layers.append(nn.ReLU())\n\n        # Final layer to output noise prediction\n        layers.append(TimeLinear(dims[-1], dim_out, num_timesteps))\n\n        self.net = nn.Sequential(*layers)\n        self.num_timesteps = num_timesteps\n\n\n    def forward(self, x: torch.Tensor, t: torch.Tensor):\n        \"\"\"\n        Args:\n            x: the noisy data after t period diffusion\n            t: the time that the forward diffusion has been running\n        \"\"\"\n        for layer in self.net:\n            if isinstance(layer, TimeLinear):\n                x = layer(x, t)\n            else:\n                x = layer(x)\n        ######################\n        return x\n\n\nmodel3 = Model3(\n    dim_in=2,\n    dim_out=2,\n    hidden_features=[512, 512],\n    num_timesteps=config.num_denoising_steps,\n).to(device)\nprint(f\"model params: {sum(p.numel() for p in model3.parameters())}\")\n\nmodel params: 1054218",
    "crumbs": [
      "1.1 Diffusion for a 2D Point Cloud"
    ]
  },
  {
    "objectID": "1_1_Diffusion 2D Toy.html#forward-diffusion",
    "href": "1_1_Diffusion 2D Toy.html#forward-diffusion",
    "title": "Training a Diffusion Model on 2-D Points",
    "section": "Forward Diffusion",
    "text": "Forward Diffusion\nWe gradually add noise to our spiral data according to a carefully designed schedule. This creates a sequence:\n\nClean data → Slightly noisy → More noisy → Pure noise\n\n\ndef forward_diffusion(x_0, t, noise_schedule, noise=None):\n    \"\"\"\n    Applies forward diffusion to input data.\n    \n    Args:\n        x_0: Clean input data\n        t: Timestep\n        noise_schedule: Dictionary containing pre-computed noise parameters\n        noise: Optional pre-generated noise\n        \n    Returns:\n        x_t: Noised version of input\n        noise: The noise that was added\n    \"\"\"\n    t_shape = (-1,) + (1,) * (x_0.ndim - 1)\n    _ts = t.view(*t_shape)\n    if noise is None:\n        noise = torch.randn_like(x_0)\n    assert _ts.max() &lt; len(noise_schedule[\"alphas_cumprod\"]), f\"t={_ts.max()} is larger than the length of noise_schedule: {len(noise_schedule['alphas_cumprod'])}\"\n    alpha_prod_t = noise_schedule[\"alphas_cumprod\"][_ts]\n    x_t = (alpha_prod_t ** 0.5) * x_0 + ((1 - alpha_prod_t) ** 0.5) * noise\n    return x_t, noise\n\nLet’s take a look at the process in action.\n\nx_0 = next(iter(val_dataloader))\nx_0 = x_0.to(device)\nx_t_list = []\ncommon_noise = torch.randn_like(x_0)\n\nfig, axs = plt.subplots(1, 6, figsize=(20, 3))\nfor i, t in enumerate([0, 50, 100, 200, 500, 999]):\n    t_ = torch.full((x_0.shape[0],), t, device=device)\n    x_t = forward_diffusion(x_0, t_, noise_schedule, noise=common_noise)[0]\n    x_t = x_t.cpu()\n    axs[i].scatter(x_t[:,0], x_t[:,1], color='white', edgecolor='gray', s=5)\n    axs[i].set_axis_off()\n    axs[i].set_title('$q(\\mathbf{x}_{'+str(t)+'})$')\n\n\n\n\n\n\n\n\nAs the noise increases, the orderly spiral quickly fades away as it becomes a disordered pile of points.\n\nVisualize forward diffusion trajectories\nLet’s plot the trajectories of the forward diffusion process. For each data point \\(\\mathbf{x}_0\\), we plot the sequence:\n\\[ \\mathbf{x}_0, \\mathbf{x}_{10}, \\mathbf{x}_{20}, \\ldots, \\mathbf{x}_{990} \\]\nWould they look like straight lines or curves?\n\n# Generate forward diffusion trajectories (would they look like straight lines or curves?)\n\nx_clean = next(iter(val_dataloader))\nx_clean = x_clean.to(device)\n\n# Run forward diffusion\nx_t_list = []\ntorch.manual_seed(0)\ncommon_noise = torch.randn_like(x_clean)\n\nfor t in range(0, 1000, 10):\n    t_ = torch.full((x_clean.shape[0],), t, device=device)\n    x_t = forward_diffusion(x_clean, t_, noise_schedule, noise=common_noise)[0]\n    x_t_list.append(x_t)\n\ntraj = torch.stack(x_t_list).cpu().numpy()\ntraj_set = TrajectorySet(traj)\n_ =traj_set.plot_trajectories(n=60, show_figure=True, figsize=(8, 8))\n\n\n\n\n\n\n\n\n&lt;Figure size 640x480 with 0 Axes&gt;",
    "crumbs": [
      "1.1 Diffusion for a 2D Point Cloud"
    ]
  },
  {
    "objectID": "1_1_Diffusion 2D Toy.html#reverse-denoising",
    "href": "1_1_Diffusion 2D Toy.html#reverse-denoising",
    "title": "Training a Diffusion Model on 2-D Points",
    "section": "Reverse Denoising",
    "text": "Reverse Denoising\nA denoising model learns to reverse this process by predicting the noise at each step.\nReverse denoising uses this trained denoising model to incremenetally remove noise from an image. We’ll define the denoising step for this process now.\n\ndef denoising_step(denoising_model, x_t, t, noise_schedule, thresholding=False):\n    \"\"\"\n    This is the backward diffusion step, with the effect of denoising.\n    \"\"\"\n    if isinstance(t, int):\n        t_tensor = torch.full((x_t.shape[0],), t, device=x_t.device)\n    else:\n        t_tensor = t\n    with torch.no_grad():\n        model_output = denoising_model(t=t_tensor, x=x_t)\n    if hasattr(model_output, \"sample\"):\n        model_output = model_output.sample\n\n    # Extract relevant values from noise_schedule\n    alpha_prod_t = noise_schedule[\"alphas_cumprod\"][t_tensor]\n    # deal with t=0 case where t can be a tensor\n    alpha_prod_t_prev = torch.where(t_tensor &gt; 0,\n                                    noise_schedule[\"alphas_cumprod\"][t_tensor - 1],\n                                    torch.ones_like(t_tensor, device=x_t.device))\n\n    # Reshape alpha_prod_t_prev for proper broadcasting\n    view_shape = (-1,) + (1,) * (x_t.ndim - 1)\n    alpha_prod_t = alpha_prod_t.view(*view_shape)\n    alpha_prod_t_prev = alpha_prod_t_prev.view(*view_shape)\n\n    beta_prod_t = 1 - alpha_prod_t\n    beta_prod_t_prev = 1 - alpha_prod_t_prev\n    current_alpha_t = alpha_prod_t / alpha_prod_t_prev\n    current_beta_t = 1 - current_alpha_t\n\n    # Compute the previous sample mean\n    pred_original_sample = (x_t - beta_prod_t ** 0.5 * model_output) / alpha_prod_t ** 0.5\n\n    # Compute the coefficients for pred_original_sample and current sample\n    pred_original_sample_coeff = (alpha_prod_t_prev ** 0.5 * current_beta_t) / beta_prod_t\n    current_sample_coeff = current_alpha_t ** 0.5 * beta_prod_t_prev / beta_prod_t\n\n    # Compute the previous sample\n    pred_prev_sample = pred_original_sample_coeff * pred_original_sample + current_sample_coeff * x_t\n\n    # Add noise\n    variance = torch.zeros_like(x_t)\n    variance_noise = torch.randn_like(x_t)\n\n    # Handle t=0 case where t can be a tensor\n    non_zero_mask = (t_tensor != 0).float().view(view_shape)\n    variance = non_zero_mask * ((1 - alpha_prod_t_prev) / (1 - alpha_prod_t) * current_beta_t)\n    variance = torch.clamp(variance, min=1e-20)\n\n    pred_prev_sample = pred_prev_sample + (variance ** 0.5) * variance_noise\n\n    return pred_prev_sample\n\n\ndef generate_samples_by_denoising(denoising_model, x_T, noise_schedule, n_T, device, thresholding=False, seed=0, return_full_trajectory=False, silent=False):\n    \"\"\"\n    This is the generation process.\n    \"\"\"\n    torch.manual_seed(seed)\n\n    x_t = x_T.to(device)\n    pbar = tqdm(range(n_T - 1, -1, -1), disable=silent)\n    if return_full_trajectory:\n        x_t_list = []\n    for t in pbar:\n        x_t = denoising_step(denoising_model, x_t, t, noise_schedule, thresholding)\n        if not silent:\n            pbar.set_postfix({\"std\": x_t.std().item()})\n        if return_full_trajectory:\n            x_t_list.append(x_t)\n\n    if return_full_trajectory:\n        return torch.stack(x_t_list).cpu()\n    else:\n        return x_t.cpu()",
    "crumbs": [
      "1.1 Diffusion for a 2D Point Cloud"
    ]
  },
  {
    "objectID": "1_1_Diffusion 2D Toy.html#training-code",
    "href": "1_1_Diffusion 2D Toy.html#training-code",
    "title": "Training a Diffusion Model on 2-D Points",
    "section": "Training code",
    "text": "Training code\nWe’ve picked our data, created our dataloaders, and defined our noise schedule, foward diffusion, and reverse diffusion steps. Now, the only thing left to do is to train our diffusion model.\n\ndef train(model: nn.Module, optimizer: torch.optim.Optimizer, steps: int=100, silent: bool=False) -&gt; float:\n  criterion = MSELoss()\n  model.train()\n  if not silent:\n    print(\"Training on device:\", device)\n  max_train_steps = steps\n\n  loss = None\n  step = 0\n  while step &lt; max_train_steps:\n    progress_bar = tqdm(train_dataloader, disable=silent)\n    for x_0 in progress_bar:\n      x_0 = x_0.float().to(device)  # x_0 is the clean data to teach the model to generate\n      optimizer.zero_grad()\n\n      true_noise = common_noise = torch.randn(x_0.shape).to(device)\n      t = torch.randint(0, config.num_denoising_steps, (x_0.shape[0],), device=device).long()\n      x_t, _ = forward_diffusion(x_0, t, noise_schedule, noise=common_noise)\n\n      predicted_noise = model(t=t, x=x_t)\n\n      loss = criterion(predicted_noise, true_noise)\n      loss.backward()\n      torch.nn.utils.clip_grad_norm_(model.parameters(), 1)  # try commenting it out\n      optimizer.step()\n\n      if not silent:\n        progress_bar.set_postfix({\"loss\": loss.cpu().item()})\n\n      step += 1\n\n      if step &gt;= max_train_steps:\n        if not silent:\n          print(f\"Reached the max training steps:\", max_train_steps)\n        break\n\n  return loss\n\n# Define the model and optimizer\nmodel3 = Model3(\n    dim_in=2,\n    dim_out=2,\n    hidden_features=[128, 128, 256],\n    num_timesteps=config.num_denoising_steps,\n).to(device)\nprint(f\"model params: {sum(p.numel() for p in model3.parameters())}\")\n\nmodel params: 281354\n\n\n\nVisualize denoising trajectories\nBefore the model is trained, what would the denoising trajectories look like?\n\n# visualize denoising trajectories\ndef visualize_denoising_trajectories(model, n=16):\n    x_T = torch.randn(n, 2)\n    traj = generate_samples_by_denoising(model, x_T, noise_schedule, n_T=1000, device=device, return_full_trajectory=True).cpu().detach().numpy()\n    traj = traj[::-50]\n    traj_set = TrajectorySet(traj)\n    _ = traj_set.plot_trajectories(n=n, show_figure=True, figsize=(8, 8))\n\nvisualize_denoising_trajectories(model3, n=120)\n\n\n\n\n\n\n\n\n\n\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\nFirst, train the model for 100 steps.\n\n# Train the model\nmodel3_optimizer = optim.AdamW(model3.parameters(), lr=config.learning_rate, weight_decay=config.weight_decay)\nmodel3_loss = train(model3, model3_optimizer, steps=100)\n\nTraining on device: cuda\n\n\n\n\n\nReached the max training steps: 100",
    "crumbs": [
      "1.1 Diffusion for a 2D Point Cloud"
    ]
  },
  {
    "objectID": "1_1_Diffusion 2D Toy.html#model-2",
    "href": "1_1_Diffusion 2D Toy.html#model-2",
    "title": "Training a Diffusion Model on 2-D Points",
    "section": "Model 2",
    "text": "Model 2\n\nmodel2 = Model2(features=2, hidden_features=[512, 256, 256]).to(device)\nprint(f\"model params: {sum(p.numel() for p in model2.parameters())}\")\n\nmodel2_optimizer = optim.AdamW(model2.parameters(), lr=config.learning_rate, weight_decay=config.weight_decay)\nmodel2_loss = train(model2, model2_optimizer, steps=5000)\nprint(\"model2_loss:\", model2_loss.item())\nvisualize_sampled_data(model2)\n\nmodel params: 215554\nTraining on device: cuda\n\n\n\n\n\n\n\n\nReached the max training steps: 5000\nmodel2_loss: 0.362407386302948",
    "crumbs": [
      "1.1 Diffusion for a 2D Point Cloud"
    ]
  },
  {
    "objectID": "1_1_Diffusion 2D Toy.html#model-3",
    "href": "1_1_Diffusion 2D Toy.html#model-3",
    "title": "Training a Diffusion Model on 2-D Points",
    "section": "Model 3",
    "text": "Model 3\n\nmodel3 = Model3(\n    dim_in=2,\n    dim_out=2,\n    hidden_features=[128, 128, 128],\n    num_timesteps=config.num_denoising_steps,\n).to(device)\nprint(f\"model params: {sum(p.numel() for p in model3.parameters())}\")\n\nmodel3_optimizer = optim.AdamW(model3.parameters(), lr=config.learning_rate, weight_decay=config.weight_decay)\nmodel3_loss = train(model3, model3_optimizer, steps=5000)\nprint(\"model3_loss:\", model3_loss.item())\nvisualize_sampled_data(model3)\n\nmodel params: 182410\nTraining on device: cuda\n\n\n\n\n\n\n\n\nReached the max training steps: 5000\nmodel3_loss: 0.2797243297100067\n\n\n\n\n\n\n\n\n\n\n\n\n\nimport torch\n\n# Sample points from the model\ndef generate_points(model):\n    with torch.no_grad():\n        x_T = torch.randn(128, 2)\n        x_sampled = generate_samples_by_denoising(model, x_T, noise_schedule, n_T=1000, device=device)\n    return x_sampled.cpu().numpy()\n\n\n# The target spiral points for comparison\ntarget_spiral = next(iter(train_dataloader))\n\ngenerated_points_1 = generate_points(model1)\ngenerated_points_1 = np.clip(generated_points_1, -3.9, 3.9)\nchamfer_dist = chamfer_distance(generated_points_1, target_spiral, direction='bi')\nprint(\"Model 1 Size:\", sum(p.numel() for p in model1.parameters()), \"Chamfer Distance:\", chamfer_dist)\n\n\ngenerated_points_2 = generate_points(model2)\nchamfer_dist = chamfer_distance(generated_points_2, target_spiral, direction='bi')\nprint(\"Model 2 Size:\", sum(p.numel() for p in model2.parameters()), \"Chamfer Distance:\", chamfer_dist)\n\n# Calculate Chamfer distance\ngenerated_points_3 = generate_points(model3)\nchamfer_dist = chamfer_distance(generated_points_3, target_spiral, direction='bi')\nprint(\"Model 3 Size:\", sum(p.numel() for p in model3.parameters()), \"Chamfer Distance:\", chamfer_dist)\n\n# # visualize the sampled images side by side\ndef visualize_sampled_data_side_by_side(models, generated_points):\n    fig, axs = plt.subplots(1, len(models) + 1, figsize=(5 * len(models), 5))\n    # Add ground truth\n    axs[0].scatter(target_spiral[:, 0], target_spiral[:, 1], color='white', edgecolor='gray', s=5)\n    axs[0].set_title(\"Ground truth\")\n\n    for i, (model, points) in enumerate(zip(models, generated_points)):\n        axs[i+1].scatter(points[:,0], points[:,1], color='white', edgecolor='gray', s=5)\n        axs[i+1].set_title(model.__class__.__name__)\n\nvisualize_sampled_data_side_by_side([model1, model2, model3], [generated_points_1, generated_points_2, generated_points_3])\n\n\n\n\nModel 1 Size: 265730 Chamfer Distance: 1.13694783735912\n\n\n\n\n\nModel 2 Size: 281602 Chamfer Distance: 0.640580538617274\n\n\n\n\n\nModel 3 Size: 182410 Chamfer Distance: 0.200290446480198",
    "crumbs": [
      "1.1 Diffusion for a 2D Point Cloud"
    ]
  },
  {
    "objectID": "1_1_Diffusion 2D Toy.html#discussion-questions",
    "href": "1_1_Diffusion 2D Toy.html#discussion-questions",
    "title": "Training a Diffusion Model on 2-D Points",
    "section": "Discussion questions",
    "text": "Discussion questions\n\nWhat do you think is the role of forward diffusion for training?\nWhat is the role of time embeddings?\nDo you think the model architecture matters for the quality of the generated samples?",
    "crumbs": [
      "1.1 Diffusion for a 2D Point Cloud"
    ]
  },
  {
    "objectID": "1_1_a_refactor.html",
    "href": "1_1_a_refactor.html",
    "title": "Organize and Refactor",
    "section": "",
    "text": "Let’s pause for a moment and think about what we’ve done. We’ve built a diffusion model that can generate 2D spiral data.\nThere are still many questions that we haven’t answered:\nTo answer these questions, we need to do experiments that require numerous code changes. However, the notebook is quite long even for this simple task. Let’s refactor it to make it more modular and easier to understand.",
    "crumbs": [
      "1.1a Refactor"
    ]
  },
  {
    "objectID": "1_1_a_refactor.html#the-refactoring",
    "href": "1_1_a_refactor.html#the-refactoring",
    "title": "Organize and Refactor",
    "section": "The refactoring",
    "text": "The refactoring\nThere are two types of code in the notebook. The first type is stable and can be reused for other tasks. The second type is experimental and specific to the task of this notebook.\n\nStable (library) code\n\nMain components of the diffusion model training pipeline:\n\nData and model setup:\n\nDataLoader setup (load_data)\nModel architectures (Model1, Model2, Model3)\nThe training loop (train)\n\nDiffusion specific components:\n\nForward diffusion (forward_diffusion)\nDenoising step (denoising_step)\nNoise schedule creation (create_noise_schedule)\nSample generation (generate_samples_by_denoising)\n\n\nEvaluation and visualization tools:\n\nChamfer distance calculation (chamfer_distance)\nVisualization utilities (TrajectorySet class)\n\n\n\n\nExperimental code\n\nCustom and flexible visualization of intermediate results\nHyperparameter choices and training configurations",
    "crumbs": [
      "1.1a Refactor"
    ]
  },
  {
    "objectID": "1_1_a_refactor.html#file-structure",
    "href": "1_1_a_refactor.html#file-structure",
    "title": "Organize and Refactor",
    "section": "File structure",
    "text": "File structure\nThe refactored code can be organized into the following files:\nlib_1_1/\n    config.py\n    data.py\n    model.py\n    diffusion.py\n    eval.py\n    training_loop.py\n    visualization.py\nAfter refactoring, we want to test that the training still works, with a much smaller notebook.\n\n%load_ext autoreload\n%autoreload 2\n\n\nfrom lib_1_1 import config, model, diffusion, data, eval, visualization\nfrom lib_1_1.training_loop import train\n\n\ndevice = \"cuda\"\ntraining_config = config.TrainingConfig()\ntrain_dataloader, val_dataloader = data.load_data(training_config)\nmodel1 = model.Model1(\n    hidden_features=[512, 256, 256],\n    num_timesteps=training_config.num_denoising_steps\n).to(device)\nmodel2 = model.Model2(\n    hidden_features=[512, 256, 256],\n    num_timesteps=training_config.num_denoising_steps\n).to(device)\nmodel3 = model.Model3(\n    hidden_features=[128, 128, 128],\n    num_timesteps=training_config.num_denoising_steps\n).to(device)\nprint(f\"model1 size: {sum(p.numel() for p in model1.parameters())}\")\nprint(f\"model2 size: {sum(p.numel() for p in model2.parameters())}\")\nprint(f\"model3 size: {sum(p.numel() for p in model3.parameters())}\")\n\n\nmodel1 size: 199682\nmodel2 size: 215554\nmodel3 size: 182410\n\n\nRun a quick training as a sanity check:\n\nimport torch\n\nnoise_schedule = diffusion.create_noise_schedule(n_T=training_config.num_denoising_steps, device=device)\noptimizer3 = torch.optim.Adam(model3.parameters(), lr=training_config.learning_rate)\ntrain(\n    model3, train_dataloader, noise_schedule, optimizer3, steps=10,\n    device=device, num_denoising_steps=training_config.num_denoising_steps\n)\n\nTraining on device: cuda\n\n\n\n\n\nReached the max training steps: 10\n\n\ntensor(1.0369, device='cuda:0', grad_fn=&lt;MseLossBackward0&gt;)\n\n\n\nVisualize forward diffusion\n\nimport matplotlib.pyplot as plt\nfrom lib_1_1.diffusion import forward_diffusion\n\nx_0 = next(iter(val_dataloader))\nx_0 = x_0.to(device)\nx_t_list = []\ncommon_noise = torch.randn_like(x_0)\n\nfig, axs = plt.subplots(1, 6, figsize=(20, 3))\nfor i, t in enumerate([0, 50, 100, 200, 500, 999]):\n    t_ = torch.full((x_0.shape[0],), t, device=device)\n    x_t = forward_diffusion(x_0, t_, noise_schedule, noise=common_noise)[0]\n    x_t = x_t.cpu()\n    axs[i].scatter(x_t[:,0], x_t[:,1], color='white', edgecolor='gray', s=5)\n    axs[i].set_axis_off()\n    axs[i].set_title('$q(\\mathbf{x}_{'+str(t)+'})$')\n\n\n\n\n\n\n\n\n\n\nVisualize forward diffusion trajectories\nLet’s plot the trajectories of the forward diffusion process. For each data point \\(\\mathbf{x}_0\\), we plot the sequence:\n\\[ \\mathbf{x}_0, \\mathbf{x}_{10}, \\mathbf{x}_{20}, \\ldots, \\mathbf{x}_{990} \\]\n\n# Generate forward diffusion trajectories (would they look like straight lines or curves?)\nfrom lib_1_1.visualization import TrajectorySet\n\nx_clean = next(iter(val_dataloader))\nx_clean = x_clean.to(device)\n\n# Run forward diffusion\nx_t_list = []\ntorch.manual_seed(0)\ncommon_noise = torch.randn_like(x_clean)\n\nfor t in range(0, 1000, 10):\n    t_ = torch.full((x_clean.shape[0],), t, device=device)\n    x_t = forward_diffusion(x_clean, t_, noise_schedule, noise=common_noise)[0]\n    x_t_list.append(x_t)\n\ntraj = torch.stack(x_t_list).cpu().numpy()\ntraj_set = TrajectorySet(traj)\n_ = traj_set.plot_trajectories(n=60, show_figure=True, figsize=(8, 8))\n\n\n\n\n\n\n\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\n\nVisualize denoising trajectories\n\nfrom lib_1_1.visualization import visualize_denoising_model\n\nvisualize_denoising_model(\n    model3,\n    noise_schedule,\n    num_samples=32,\n    timestep_size=50,\n    num_denoising_steps=training_config.num_denoising_steps,\n    device=device\n)\n\n  0%|          | 0/1000 [00:00&lt;?, ?it/s, std=1.95]100%|██████████| 1000/1000 [00:01&lt;00:00, 569.47it/s, std=233]\n\n\n\n\n\n\n\n\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\ntrain(\n    model3, train_dataloader, noise_schedule, optimizer3, steps=1000,\n    device=device, num_denoising_steps=training_config.num_denoising_steps\n)\n\nTraining on device: cuda\n\n\n\n\n\nReached the max training steps: 1000\n\n\ntensor(0.3241, device='cuda:0', grad_fn=&lt;MseLossBackward0&gt;)\n\n\n\nvisualize_denoising_model(\n    model3,\n    noise_schedule,\n    num_samples=32,\n    timestep_size=50,\n    num_denoising_steps=training_config.num_denoising_steps,\n    device=device\n)\n\n  0%|          | 0/1000 [00:00&lt;?, ?it/s, std=1.06] 100%|██████████| 1000/1000 [00:01&lt;00:00, 570.73it/s, std=2.14]\n\n\n\n\n\n\n\n\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\nfrom lib_1_1.visualization import visualize_sampled_data\n\nvisualize_sampled_data(model3, noise_schedule, num_samples=128, device=device)\n\n  0%|          | 0/1000 [00:00&lt;?, ?it/s, std=0.951]100%|██████████| 1000/1000 [00:01&lt;00:00, 572.56it/s, std=2.13]",
    "crumbs": [
      "1.1a Refactor"
    ]
  },
  {
    "objectID": "1_1_a_refactor.html#animate-the-learning-process",
    "href": "1_1_a_refactor.html#animate-the-learning-process",
    "title": "Organize and Refactor",
    "section": "Animate the learning process",
    "text": "Animate the learning process\n\nfrom lib_1_1.diffusion import generate_samples_by_denoising\nfrom tqdm import tqdm\n\nmodel3 = model.Model3(\n    hidden_features=[128, 128, 256],\n    num_timesteps=training_config.num_denoising_steps,\n).to(device)\nmodel3_optimizer = torch.optim.Adam(model3.parameters(), lr=training_config.learning_rate)\n# model3_optimizer = torch.optim.AdamW(model3.parameters(), lr=training_config.learning_rate, weight_decay=training_config.weight_decay)\n\nsamples = []\nsteps_to_checkpoint = [0, 100, 200, 300, 400, 500, 600, 700, 800, 900]\nall_trajs = []\n\npbar = tqdm(range(len(steps_to_checkpoint)))\nfor i in pbar:\n    if i &gt; 0:\n        steps_to_train = steps_to_checkpoint[i] - steps_to_checkpoint[i-1]\n    else:\n        steps_to_train = i\n    model3_loss = train(\n        model3, train_dataloader,\n        noise_schedule, model3_optimizer,\n        steps=steps_to_train, silent=True,\n        device=device, num_denoising_steps=training_config.num_denoising_steps\n    )\n    if i &gt; 0:\n        pbar.set_description(f\"loss: {model3_loss:.4f}\")\n    torch.manual_seed(0)\n    traj = generate_samples_by_denoising(\n        model3, torch.randn(128, 2), noise_schedule, n_T=1000, device=device, return_full_trajectory=True, silent=True)\n    traj = traj.detach().cpu().numpy()\n    samples.append(traj[0])\n    traj = traj[::20, :, :]\n    traj_set = TrajectorySet(traj)\n    all_trajs.append(traj_set)\n\n  0%|          | 0/10 [00:00&lt;?, ?it/s]loss: 0.2669: 100%|██████████| 10/10 [00:16&lt;00:00,  1.66s/it]\n\n\n\nvisualize_sampled_data(model3, noise_schedule, num_samples=128, device=device)\n\n  0%|          | 0/1000 [00:00&lt;?, ?it/s, std=0.933]100%|██████████| 1000/1000 [00:01&lt;00:00, 571.12it/s, std=2.01]\n\n\n\n\n\n\n\n\n\n\nimport matplotlib.animation as animation\nfrom IPython.display import HTML\n\nfig, ax = plt.subplots()\nsc = ax.scatter(samples[i][:, 0], samples[i][:, 1], color='white', edgecolor='gray', s=5)\nax.set_xlim(-5, 5)\nax.set_ylim(-5, 5)\nfig.suptitle(\"Samples\")\n\ndef update(i):\n    sc.set_offsets(samples[i])\n    return sc,\n\nani = animation.FuncAnimation(fig, update, frames=range(len(steps_to_checkpoint)), interval=1000, blit=True, repeat=False)\nHTML(ani.to_jshtml())\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# A shared state to store the frames for the vector field animation.\n_state = {\n    \"frames\": {},\n}\n\nfrom ipywidgets import interact\n\ndef show_traj_at_learning_step(i=0):\n    train_steps = steps_to_checkpoint[i]\n    bytes_value = all_trajs[i].plot_trajectories(\n        n=12, show_figure=True, figsize=(6, 6),\n        with_ticks=True, title=f\"train steps = {train_steps}\",\n        xlim=(-5, 5), ylim=(-5, 7),\n        with_lines=True,\n    )\n    _state[\"frames\"][train_steps] = bytes_value\n\ninteract(show_traj_at_learning_step, i=(0, len(all_trajs)-1, 1))\n\n\n\n\n&lt;function __main__.show_traj_at_learning_step(i=0)&gt;",
    "crumbs": [
      "1.1a Refactor"
    ]
  },
  {
    "objectID": "1_1_a_refactor.html#compare-model-performance",
    "href": "1_1_a_refactor.html#compare-model-performance",
    "title": "Organize and Refactor",
    "section": "Compare model performance",
    "text": "Compare model performance\n\nmodel1 = model.Model1(\n    hidden_features=[512, 256, 256],\n    num_timesteps=training_config.num_denoising_steps\n).to(device)\nmodel1_optimizer = torch.optim.Adam(model1.parameters(), lr=training_config.learning_rate)\nmodel1_loss = train(model1, train_dataloader, noise_schedule, model1_optimizer, steps=5000, device=device, num_denoising_steps=training_config.num_denoising_steps)\n\nmodel2 = model.Model2(\n    hidden_features=[512, 256, 256],\n    num_timesteps=training_config.num_denoising_steps\n).to(device)\nmodel2_optimizer = torch.optim.Adam(model2.parameters(), lr=training_config.learning_rate)\nmodel2_loss = train(model2, train_dataloader, noise_schedule, model2_optimizer, steps=5000, device=device, num_denoising_steps=training_config.num_denoising_steps)\n\nmodel3 = model.Model3(\n    hidden_features=[128, 128, 128],\n    num_timesteps=training_config.num_denoising_steps\n).to(device)\nmodel3_optimizer = torch.optim.Adam(model3.parameters(), lr=training_config.learning_rate)\nmodel3_loss = train(model3, train_dataloader, noise_schedule, model3_optimizer, steps=5000, device=device, num_denoising_steps=training_config.num_denoising_steps)\n\nTraining on device: cuda\n\n\n\n\n\nReached the max training steps: 5000\nTraining on device: cuda\n\n\n\n\n\nReached the max training steps: 5000\nTraining on device: cuda\n\n\n\n\n\nReached the max training steps: 5000\n\n\n\nimport torch\nimport numpy as np\nfrom lib_1_1.eval import chamfer_distance\n\n\n# The target spiral points for comparison\ntarget_spiral = next(iter(train_dataloader))\n\ngenerated_points_1 = generate_samples_by_denoising(model1, torch.randn(128, 2), noise_schedule, n_T=1000, device=device)\ngenerated_points_1 = np.clip(generated_points_1.cpu().numpy(), -3.9, 3.9)\nchamfer_dist = chamfer_distance(generated_points_1, target_spiral, direction='bi')\nprint(\"Model 1 Size:\", sum(p.numel() for p in model1.parameters()), \"Chamfer Distance:\", chamfer_dist)\n\ngenerated_points_2 = generate_samples_by_denoising(model2, torch.randn(128, 2), noise_schedule, n_T=1000, device=device)\ngenerated_points_2 = generated_points_2.cpu().numpy()\nchamfer_dist = chamfer_distance(generated_points_2, target_spiral, direction='bi')\nprint(\"Model 2 Size:\", sum(p.numel() for p in model2.parameters()), \"Chamfer Distance:\", chamfer_dist)\n\ngenerated_points_3 = generate_samples_by_denoising(model3, torch.randn(128, 2), noise_schedule, n_T=1000, device=device)\ngenerated_points_3 = generated_points_3.cpu().numpy()\nchamfer_dist = chamfer_distance(generated_points_3, target_spiral, direction='bi')\nprint(\"Model 3 Size:\", sum(p.numel() for p in model3.parameters()), \"Chamfer Distance:\", chamfer_dist)\n\n# # visualize the sampled images side by side\ndef visualize_sampled_data_side_by_side(models, generated_points):\n    fig, axs = plt.subplots(1, len(models) + 1, figsize=(5 * len(models), 5))\n    # Add ground truth\n    axs[0].scatter(target_spiral[:, 0], target_spiral[:, 1], color='white', edgecolor='gray', s=5)\n    axs[0].set_title(\"Ground truth\")\n\n    for i, (model, points) in enumerate(zip(models, generated_points)):\n        axs[i+1].scatter(points[:,0], points[:,1], color='white', edgecolor='gray', s=5)\n        axs[i+1].set_title(model.__class__.__name__)\n\nvisualize_sampled_data_side_by_side([model1, model2, model3], [generated_points_1, generated_points_2, generated_points_3])\n\n  0%|          | 0/1000 [00:00&lt;?, ?it/s, std=1.49]100%|██████████| 1000/1000 [00:00&lt;00:00, 1016.28it/s, std=62.6]\n\n\nModel 1 Size: 199682 Chamfer Distance: 1.546387922825363\n\n\n100%|██████████| 1000/1000 [00:01&lt;00:00, 970.83it/s, std=1.93]\n\n\nModel 2 Size: 215554 Chamfer Distance: 0.37767705435093313\n\n\n100%|██████████| 1000/1000 [00:01&lt;00:00, 556.81it/s, std=2.01]\n\n\nModel 3 Size: 182410 Chamfer Distance: 0.2109826764526811",
    "crumbs": [
      "1.1a Refactor"
    ]
  },
  {
    "objectID": "1_1_a_refactor.html#conclusion",
    "href": "1_1_a_refactor.html#conclusion",
    "title": "Organize and Refactor",
    "section": "Conclusion",
    "text": "Conclusion\nWith refactoring, we were able to replicate the functionality of the original notebook, while making it much shorter and easier to understand.\nNext, we are ready to do a series of experiments to understand the impact of hyperparameters in the training algorithm.",
    "crumbs": [
      "1.1a Refactor"
    ]
  },
  {
    "objectID": "1_2_Flow Matching 2D Toy.html",
    "href": "1_2_Flow Matching 2D Toy.html",
    "title": "Training a Flow Matching Model for 2D Point Generation",
    "section": "",
    "text": "In this tutorial, you’ll learn how to train a flow matching model to generate spirals of 2-D points.\nWe’ll explore the different components of flow matching models and their functions, as well as compare the quality of generated results using different model architectures.",
    "crumbs": [
      "1.2 Flow Matching for a 2D Point Cloud"
    ]
  },
  {
    "objectID": "1_2_Flow Matching 2D Toy.html#what-youll-learn",
    "href": "1_2_Flow Matching 2D Toy.html#what-youll-learn",
    "title": "Training a Flow Matching Model for 2D Point Generation",
    "section": "What you’ll learn",
    "text": "What you’ll learn\n\nHow flow matching works conceptually\nThree different model architectures for implementing flow matching\nHow to visualize and evaluate the generated results\nBest practices for training flow matching models",
    "crumbs": [
      "1.2 Flow Matching for a 2D Point Cloud"
    ]
  },
  {
    "objectID": "1_2_Flow Matching 2D Toy.html#prerequisites",
    "href": "1_2_Flow Matching 2D Toy.html#prerequisites",
    "title": "Training a Flow Matching Model for 2D Point Generation",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nBasic understanding of PyTorch\nFamiliarity with neural networks and optimization\n\n\nfrom dataclasses import dataclass\nimport math\nfrom typing import Dict, Tuple\n\nfrom matplotlib import pyplot as plt\nfrom sklearn.datasets import make_swiss_roll\nimport torch\nfrom torch import nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom torch.nn import MSELoss\nfrom torch.utils.data import DataLoader\nfrom tqdm.auto import tqdm\nfrom torchdyn.core import NeuralODE",
    "crumbs": [
      "1.2 Flow Matching for a 2D Point Cloud"
    ]
  },
  {
    "objectID": "1_2_Flow Matching 2D Toy.html#dataloaders",
    "href": "1_2_Flow Matching 2D Toy.html#dataloaders",
    "title": "Training a Flow Matching Model for 2D Point Generation",
    "section": "Dataloaders",
    "text": "Dataloaders\nBefore training the model, we need to define some hyperparameters and create our train and validation dataloaders.\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n@dataclass\nclass TrainingConfig:\n    batch_size: int = 256 # batch size\n    learning_rate: float = 5e-4 # initial learning rate\n    weight_decay: float = 1e-6 # weight decay\n    num_denoising_steps: int = 1000 # number of timesteps\n\n\ndef load_data(config: TrainingConfig) -&gt; Tuple[DataLoader, DataLoader]:\n    \"\"\"\n    Load the data and return the train and validation dataloaders.\n\n    Args:\n      config: TrainingConfig object.\n    Returns:\n      train_dataloader: DataLoader for the training data.\n      val_dataloader: DataLoader for the validation data.\n    \"\"\"\n    n = int(1e+6)\n    x, _ = make_swiss_roll(n_samples=n, noise=0)\n    x = x[:, [0, 2]]\n    scaling = 2\n    x = (x - x.mean()) / x.std() * scaling\n    x_train = x[:int(n * 0.8), :]\n    x_val = x[int(n * 0.8):, :]\n\n    class SimpleDataset:\n      def __init__(self, data):\n        self.data = data\n\n      def __len__(self):\n        return len(self.data)\n\n      def __getitem__(self, i):\n        return self.data[i]\n\n    train_dataset = SimpleDataset(x_train)\n    val_dataset = SimpleDataset(x_val)\n    train_dataloader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True, num_workers=0)\n    val_dataloader = DataLoader(val_dataset, batch_size=config.batch_size, shuffle=False, num_workers=0)\n\n    return train_dataloader, val_dataloader\n\n\nconfig = TrainingConfig()\ntrain_dataloader, val_dataloader = load_data(config)\nfirst_batch = next(iter(train_dataloader))\nprint(\"batch shape:\", first_batch.shape)\n\nbatch shape: torch.Size([256, 2])",
    "crumbs": [
      "1.2 Flow Matching for a 2D Point Cloud"
    ]
  },
  {
    "objectID": "1_2_Flow Matching 2D Toy.html#conditional-flow-matcher",
    "href": "1_2_Flow Matching 2D Toy.html#conditional-flow-matcher",
    "title": "Training a Flow Matching Model for 2D Point Generation",
    "section": "Conditional Flow Matcher",
    "text": "Conditional Flow Matcher\n\nfrom typing import Union\n\n\nclass ConditionalFlowMatcher:\n    \"\"\"Base class for conditional flow matching methods. This class implements the independent\n    conditional flow matching methods from [1] and serves as a parent class for all other flow\n    matching methods.\n\n    It implements:\n    - Drawing data from gaussian probability path N(t * x1 + (1 - t) * x0, sigma) function\n    - conditional flow matching ut(x1|x0) = x1 - x0\n    - score function $\\nabla log p_t(x|x0, x1)$\n    \"\"\"\n\n    def __init__(self, sigma: Union[float, int] = 0.0):\n        r\"\"\"Initialize the ConditionalFlowMatcher class. It requires the hyper-parameter $\\sigma$.\n\n        Parameters\n        ----------\n        sigma : Union[float, int]\n        \"\"\"\n        self.sigma = sigma\n\n    def compute_mu_t(self, x0, x1, t):\n        \"\"\"\n        Compute the mean of the probability path N(t * x1 + (1 - t) * x0, sigma), see (Eq.14) [1].\n\n        Parameters\n        ----------\n        x0 : Tensor, shape (bs, *dim)\n            represents the source minibatch\n        x1 : Tensor, shape (bs, *dim)\n            represents the target minibatch\n        t : FloatTensor, shape (bs)\n\n        Returns\n        -------\n        mean mu_t: t * x1 + (1 - t) * x0\n\n        References\n        ----------\n        [1] Improving and Generalizing Flow-Based Generative Models with minibatch optimal transport, Preprint, Tong et al.\n        \"\"\"\n        t = pad_t_like_x(t, x0)\n        return t * x1 + (1 - t) * x0\n\n    def compute_sigma_t(self, t):\n        \"\"\"\n        Compute the standard deviation of the probability path N(t * x1 + (1 - t) * x0, sigma), see (Eq.14) [1].\n\n        Parameters\n        ----------\n        t : FloatTensor, shape (bs)\n\n        Returns\n        -------\n        standard deviation sigma\n\n        References\n        ----------\n        [1] Improving and Generalizing Flow-Based Generative Models with minibatch optimal transport, Preprint, Tong et al.\n        \"\"\"\n        del t\n        return self.sigma\n\n    def sample_xt(self, x0, x1, t, epsilon):\n        \"\"\"\n        Draw a sample from the probability path N(t * x1 + (1 - t) * x0, sigma), see (Eq.14) [1].\n\n        Parameters\n        ----------\n        x0 : Tensor, shape (bs, *dim)\n            represents the source minibatch\n        x1 : Tensor, shape (bs, *dim)\n            represents the target minibatch\n        t : FloatTensor, shape (bs)\n        epsilon : Tensor, shape (bs, *dim)\n            noise sample from N(0, 1)\n\n        Returns\n        -------\n        xt : Tensor, shape (bs, *dim)\n\n        References\n        ----------\n        [1] Improving and Generalizing Flow-Based Generative Models with minibatch optimal transport, Preprint, Tong et al.\n        \"\"\"\n        mu_t = self.compute_mu_t(x0, x1, t)\n        sigma_t = self.compute_sigma_t(t)\n        sigma_t = pad_t_like_x(sigma_t, x0)\n        return mu_t + sigma_t * epsilon\n\n    def compute_conditional_flow(self, x0, x1, t, xt):\n        \"\"\"\n        Compute the conditional vector field ut(x1|x0) = x1 - x0, see Eq.(15) [1].\n\n        Parameters\n        ----------\n        x0 : Tensor, shape (bs, *dim)\n            represents the source minibatch\n        x1 : Tensor, shape (bs, *dim)\n            represents the target minibatch\n        t : FloatTensor, shape (bs)\n        xt : Tensor, shape (bs, *dim)\n            represents the samples drawn from probability path pt\n\n        Returns\n        -------\n        ut : conditional vector field ut(x1|x0) = x1 - x0\n\n        References\n        ----------\n        [1] Improving and Generalizing Flow-Based Generative Models with minibatch optimal transport, Preprint, Tong et al.\n        \"\"\"\n        del t, xt\n        return x1 - x0\n\n    def sample_noise_like(self, x):\n        return torch.randn_like(x)\n\n    def sample_location_and_conditional_flow(self, x0, x1, t=None, return_noise=False):\n        \"\"\"\n        Compute the sample xt (drawn from N(t * x1 + (1 - t) * x0, sigma))\n        and the conditional vector field ut(x1|x0) = x1 - x0, see Eq.(15) [1].\n\n        Parameters\n        ----------\n        x0 : Tensor, shape (bs, *dim)\n            represents the source minibatch\n        x1 : Tensor, shape (bs, *dim)\n            represents the target minibatch\n        (optionally) t : Tensor, shape (bs)\n            represents the time levels\n            if None, drawn from uniform [0,1]\n        return_noise : bool\n            return the noise sample epsilon\n\n\n        Returns\n        -------\n        t : FloatTensor, shape (bs)\n        xt : Tensor, shape (bs, *dim)\n            represents the samples drawn from probability path pt\n        ut : conditional vector field ut(x1|x0) = x1 - x0\n        (optionally) eps: Tensor, shape (bs, *dim) such that xt = mu_t + sigma_t * epsilon\n\n        References\n        ----------\n        [1] Improving and Generalizing Flow-Based Generative Models with minibatch optimal transport, Preprint, Tong et al.\n        \"\"\"\n        if t is None:\n            t = torch.rand(x0.shape[0]).type_as(x0)\n        assert len(t) == x0.shape[0], f\"t has to have batch size dimension, got {len(t)}\"\n\n        eps = self.sample_noise_like(x0)\n        xt = self.sample_xt(x0, x1, t, eps)\n        ut = self.compute_conditional_flow(x0, x1, t, xt)\n        if return_noise:\n            return t, xt, ut, eps\n        else:\n            return t, xt, ut\n\n    def compute_lambda(self, t):\n        \"\"\"Compute the lambda function, see Eq.(23) [3].\n\n        Parameters\n        ----------\n        t : FloatTensor, shape (bs)\n\n        Returns\n        -------\n        lambda : score weighting function\n\n        References\n        ----------\n        [4] Simulation-free Schrodinger bridges via score and flow matching, Preprint, Tong et al.\n        \"\"\"\n        sigma_t = self.compute_sigma_t(t)\n        return 2 * sigma_t / (self.sigma**2 + 1e-8)\n\n\n\nimport numpy as np\nimport ot as pot\nimport warnings\nfrom functools import partial\n\nclass OTPlanSampler:\n    \"\"\"OTPlanSampler implements sampling coordinates according to an OT plan (wrt squared Euclidean\n    cost) with different implementations of the plan calculation.\"\"\"\n\n    def __init__(\n        self,\n        method: str,\n        reg: float = 0.05,\n        reg_m: float = 1.0,\n        normalize_cost: bool = False,\n        num_threads: Union[int, str] = 1,\n        warn: bool = True,\n    ) -&gt; None:\n        \"\"\"Initialize the OTPlanSampler class.\n\n        Parameters\n        ----------\n        method: str\n            choose which optimal transport solver you would like to use.\n            Currently supported are [\"exact\", \"sinkhorn\", \"unbalanced\",\n            \"partial\"] OT solvers.\n        reg: float, optional\n            regularization parameter to use for Sinkhorn-based iterative solvers.\n        reg_m: float, optional\n            regularization weight for unbalanced Sinkhorn-knopp solver.\n        normalize_cost: bool, optional\n            normalizes the cost matrix so that the maximum cost is 1. Helps\n            stabilize Sinkhorn-based solvers. Should not be used in the vast\n            majority of cases.\n        num_threads: int or str, optional\n            number of threads to use for the \"exact\" OT solver. If \"max\", uses\n            the maximum number of threads.\n        warn: bool, optional\n            if True, raises a warning if the algorithm does not converge\n        \"\"\"\n        # ot_fn should take (a, b, M) as arguments where a, b are marginals and\n        # M is a cost matrix\n        if method == \"exact\":\n            self.ot_fn = partial(pot.emd, numThreads=num_threads)\n        elif method == \"sinkhorn\":\n            self.ot_fn = partial(pot.sinkhorn, reg=reg)\n        elif method == \"unbalanced\":\n            self.ot_fn = partial(pot.unbalanced.sinkhorn_knopp_unbalanced, reg=reg, reg_m=reg_m)\n        elif method == \"partial\":\n            self.ot_fn = partial(pot.partial.entropic_partial_wasserstein, reg=reg)\n        else:\n            raise ValueError(f\"Unknown method: {method}\")\n        self.reg = reg\n        self.reg_m = reg_m\n        self.normalize_cost = normalize_cost\n        self.warn = warn\n\n    def get_map(self, x0, x1):\n        \"\"\"Compute the OT plan (wrt squared Euclidean cost) between a source and a target\n        minibatch.\n\n        Parameters\n        ----------\n        x0 : Tensor, shape (bs, *dim)\n            represents the source minibatch\n        x1 : Tensor, shape (bs, *dim)\n            represents the source minibatch\n\n        Returns\n        -------\n        p : numpy array, shape (bs, bs)\n            represents the OT plan between minibatches\n        \"\"\"\n        a, b = pot.unif(x0.shape[0]), pot.unif(x1.shape[0])\n        if x0.dim() &gt; 2:\n            x0 = x0.reshape(x0.shape[0], -1)\n        if x1.dim() &gt; 2:\n            x1 = x1.reshape(x1.shape[0], -1)\n        x1 = x1.reshape(x1.shape[0], -1)\n        M = torch.cdist(x0, x1) ** 2\n        if self.normalize_cost:\n            M = M / M.max()  # should not be normalized when using minibatches\n        p = self.ot_fn(a, b, M.detach().cpu().numpy())\n        if not np.all(np.isfinite(p)):\n            print(\"ERROR: p is not finite\")\n            print(p)\n            print(\"Cost mean, max\", M.mean(), M.max())\n            print(x0, x1)\n        if np.abs(p.sum()) &lt; 1e-8:\n            if self.warn:\n                warnings.warn(\"Numerical errors in OT plan, reverting to uniform plan.\")\n            p = np.ones_like(p) / p.size\n        return p\n\n    def sample_map(self, pi, batch_size, replace=True):\n        r\"\"\"Draw source and target samples from pi  $(x,z) \\sim \\pi$\n\n        Parameters\n        ----------\n        pi : numpy array, shape (bs, bs)\n            represents the source minibatch\n        batch_size : int\n            represents the OT plan between minibatches\n        replace : bool\n            represents sampling or without replacement from the OT plan\n\n        Returns\n        -------\n        (i_s, i_j) : tuple of numpy arrays, shape (bs, bs)\n            represents the indices of source and target data samples from $\\pi$\n        \"\"\"\n        p = pi.flatten()\n        p = p / p.sum()\n        choices = np.random.choice(\n            pi.shape[0] * pi.shape[1], p=p, size=batch_size, replace=replace\n        )\n        return np.divmod(choices, pi.shape[1])\n\n    def sample_plan(self, x0, x1, replace=True):\n        r\"\"\"Compute the OT plan $\\pi$ (wrt squared Euclidean cost) between a source and a target\n        minibatch and draw source and target samples from pi $(x,z) \\sim \\pi$\n\n        Parameters\n        ----------\n        x0 : Tensor, shape (bs, *dim)\n            represents the source minibatch\n        x1 : Tensor, shape (bs, *dim)\n            represents the source minibatch\n        replace : bool\n            represents sampling or without replacement from the OT plan\n\n        Returns\n        -------\n        x0[i] : Tensor, shape (bs, *dim)\n            represents the source minibatch drawn from $\\pi$\n        x1[j] : Tensor, shape (bs, *dim)\n            represents the source minibatch drawn from $\\pi$\n        \"\"\"\n        pi = self.get_map(x0, x1)\n        i, j = self.sample_map(pi, x0.shape[0], replace=replace)\n        return x0[i], x1[j]\n\n    def sample_plan_with_labels(self, x0, x1, y0=None, y1=None, replace=True):\n        r\"\"\"Compute the OT plan $\\pi$ (wrt squared Euclidean cost) between a source and a target\n        minibatch and draw source and target labeled samples from pi $(x,z) \\sim \\pi$\n\n        Parameters\n        ----------\n        x0 : Tensor, shape (bs, *dim)\n            represents the source minibatch\n        x1 : Tensor, shape (bs, *dim)\n            represents the target minibatch\n        y0 : Tensor, shape (bs)\n            represents the source label minibatch\n        y1 : Tensor, shape (bs)\n            represents the target label minibatch\n        replace : bool\n            represents sampling or without replacement from the OT plan\n\n        Returns\n        -------\n        x0[i] : Tensor, shape (bs, *dim)\n            represents the source minibatch drawn from $\\pi$\n        x1[j] : Tensor, shape (bs, *dim)\n            represents the target minibatch drawn from $\\pi$\n        y0[i] : Tensor, shape (bs, *dim)\n            represents the source label minibatch drawn from $\\pi$\n        y1[j] : Tensor, shape (bs, *dim)\n            represents the target label minibatch drawn from $\\pi$\n        \"\"\"\n        pi = self.get_map(x0, x1)\n        i, j = self.sample_map(pi, x0.shape[0], replace=replace)\n        return (\n            x0[i],\n            x1[j],\n            y0[i] if y0 is not None else None,\n            y1[j] if y1 is not None else None,\n        )\n\n    def sample_trajectory(self, X):\n        \"\"\"Compute the OT trajectories between different sample populations moving from the source\n        to the target distribution.\n\n        Parameters\n        ----------\n        X : Tensor, (bs, times, *dim)\n            different populations of samples moving from the source to the target distribution.\n\n        Returns\n        -------\n        to_return : Tensor, (bs, times, *dim)\n            represents the OT sampled trajectories over time.\n        \"\"\"\n        times = X.shape[1]\n        pis = []\n        for t in range(times - 1):\n            pis.append(self.get_map(X[:, t], X[:, t + 1]))\n\n        indices = [np.arange(X.shape[0])]\n        for pi in pis:\n            j = []\n            for i in indices[-1]:\n                j.append(np.random.choice(pi.shape[1], p=pi[i] / pi[i].sum()))\n            indices.append(np.array(j))\n\n        to_return = []\n        for t in range(times):\n            to_return.append(X[:, t][indices[t]])\n        to_return = np.stack(to_return, axis=1)\n        return to_return\n\n\nclass ExactOptimalTransportConditionalFlowMatcher(ConditionalFlowMatcher):\n    \"\"\"Child class for optimal transport conditional flow matching method. This class implements\n    the OT-CFM methods from [1] and inherits the ConditionalFlowMatcher parent class.\n\n    It overrides the sample_location_and_conditional_flow.\n    \"\"\"\n\n    def __init__(self, sigma: Union[float, int] = 0.0):\n        r\"\"\"Initialize the ConditionalFlowMatcher class. It requires the hyper-parameter $\\sigma$.\n\n        Parameters\n        ----------\n        sigma : Union[float, int]\n        ot_sampler: exact OT method to draw couplings (x0, x1) (see Eq.(17) [1]).\n        \"\"\"\n        super().__init__(sigma)\n        self.ot_sampler = OTPlanSampler(method=\"exact\")\n\n    def sample_location_and_conditional_flow(self, x0, x1, t=None, return_noise=False):\n        r\"\"\"\n        Compute the sample xt (drawn from N(t * x1 + (1 - t) * x0, sigma))\n        and the conditional vector field ut(x1|x0) = x1 - x0, see Eq.(15) [1]\n        with respect to the minibatch OT plan $\\Pi$.\n\n        Parameters\n        ----------\n        x0 : Tensor, shape (bs, *dim)\n            represents the source minibatch\n        x1 : Tensor, shape (bs, *dim)\n            represents the target minibatch\n        (optionally) t : Tensor, shape (bs)\n            represents the time levels\n            if None, drawn from uniform [0,1]\n        return_noise : bool\n            return the noise sample epsilon\n\n        Returns\n        -------\n        t : FloatTensor, shape (bs)\n        xt : Tensor, shape (bs, *dim)\n            represents the samples drawn from probability path pt\n        ut : conditional vector field ut(x1|x0) = x1 - x0\n        (optionally) epsilon : Tensor, shape (bs, *dim) such that xt = mu_t + sigma_t * epsilon\n\n        References\n        ----------\n        [1] Improving and Generalizing Flow-Based Generative Models with minibatch optimal transport, Preprint, Tong et al.\n        \"\"\"\n        x0, x1 = self.ot_sampler.sample_plan(x0, x1)\n        return super().sample_location_and_conditional_flow(x0, x1, t, return_noise)\n\n    def guided_sample_location_and_conditional_flow(\n        self, x0, x1, y0=None, y1=None, t=None, return_noise=False\n    ):\n        r\"\"\"\n        Compute the sample xt (drawn from N(t * x1 + (1 - t) * x0, sigma))\n        and the conditional vector field ut(x1|x0) = x1 - x0, see Eq.(15) [1]\n        with respect to the minibatch OT plan $\\Pi$.\n\n        Parameters\n        ----------\n        x0 : Tensor, shape (bs, *dim)\n            represents the source minibatch\n        x1 : Tensor, shape (bs, *dim)\n            represents the target minibatch\n        y0 : Tensor, shape (bs) (default: None)\n            represents the source label minibatch\n        y1 : Tensor, shape (bs) (default: None)\n            represents the target label minibatch\n        (optionally) t : Tensor, shape (bs)\n            represents the time levels\n            if None, drawn from uniform [0,1]\n        return_noise : bool\n            return the noise sample epsilon\n\n        Returns\n        -------\n        t : FloatTensor, shape (bs)\n        xt : Tensor, shape (bs, *dim)\n            represents the samples drawn from probability path pt\n        ut : conditional vector field ut(x1|x0) = x1 - x0\n        (optionally) epsilon : Tensor, shape (bs, *dim) such that xt = mu_t + sigma_t * epsilon\n\n        References\n        ----------\n        [1] Improving and Generalizing Flow-Based Generative Models with minibatch optimal transport, Preprint, Tong et al.\n        \"\"\"\n        x0, x1, y0, y1 = self.ot_sampler.sample_plan_with_labels(x0, x1, y0, y1)\n        if return_noise:\n            t, xt, ut, eps = super().sample_location_and_conditional_flow(x0, x1, t, return_noise)\n            return t, xt, ut, y0, y1, eps\n        else:\n            t, xt, ut = super().sample_location_and_conditional_flow(x0, x1, t, return_noise)\n            return t, xt, ut, y0, y1\n\n2024-12-10 20:31:20.781305: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n2024-12-10 20:31:20.792884: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-12-10 20:31:20.805317: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-12-10 20:31:20.808998: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2024-12-10 20:31:20.820170: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2024-12-10 20:31:21.579569: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n\n\n\ndef generate_samples_with_flow_matching(\n        denoising_model, device, num_samples: int = 8,\n        data_dims = [2,], parallel: bool = False, seed: int = 0,\n        clip_min: float = -1.0, clip_max: float = 1.0,\n        num_timesteps: int = 100,\n    ):\n    \"\"\"Generate samples.\n\n    Parameters\n    ----------\n    model:\n        represents the neural network that we want to generate samples from\n    parallel: bool\n        represents the parallel training flag. Torchdyn only runs on 1 GPU, we need to send the models from several GPUs to 1 GPU.\n    savedir: str\n        represents the path where we want to save the generated images\n    step: int\n        represents the current step of training\n    \"\"\"\n    model = denoising_model\n    \n    \n    if parallel:\n        import copy\n        model = copy.deepcopy(denoising_model)\n        # Send the models from GPU to CPU for inference with NeuralODE from Torchdyn\n        model = model.to(device)\n\n    with torch.no_grad():\n        torch.manual_seed(seed)\n        node = NeuralODE(model, solver=\"euler\", sensitivity=\"adjoint\")\n        \n        with torch.no_grad():\n            traj = node.trajectory(\n                torch.randn(num_samples, *data_dims, device=device),\n                t_span=torch.linspace(0, 1, num_timesteps, device=device),\n            )\n            traj = traj[-1, :].view([-1, *data_dims]).clip(clip_min, clip_max)\n            # traj = traj / 2 + 0.5\n    \n    return traj  # range is expected to be [0, 1]?\n\n\nFM = ConditionalFlowMatcher(sigma=0)\n# FM = ExactOptimalTransportConditionalFlowMatcher(sigma=0)\n\n\nHow does flow matching plan the route?\n\ntorch.linspace(0, 1, 10)\n\ntensor([0.0000, 0.1111, 0.2222, 0.3333, 0.4444, 0.5556, 0.6667, 0.7778, 0.8889,\n        1.0000])\n\n\n\ndef plot_flow_matching_trajectories(FM, n_examples=30):\n    # set a commmon xlim and ylim\n    xlim = [-3.7, 3.7]\n    ylim = [-3.7, 3.7]\n    # clean data (spiral)\n    x_clean = next(iter(val_dataloader))[:n_examples]\n\n    # noisy data\n    x_noisy = torch.randn_like(x_clean)\n\n    # Sample multiple timesteps to visualize flow progression\n    num_timesteps = 10\n    t_grid = torch.linspace(0, 1, num_timesteps)\n    x_ts = []\n    u_ts = []\n\n    for t in t_grid:\n        t_batch = t.repeat(x_clean.shape[0])\n        np.random.seed(0)\n        t, x_t, u_t = FM.sample_location_and_conditional_flow(x0=x_noisy, x1=x_clean, t=t_batch)\n        x_ts.append(x_t.detach().cpu().numpy())\n        u_ts.append(u_t.detach().cpu().numpy())\n\n    # Create visualization\n    fig, axs = plt.subplots(2, 2, figsize=(12, 12))\n\n    # Plot initial and final distributions\n    axs[0, 0].scatter(x_noisy[:, 0], x_noisy[:, 1], color='blue', alpha=0.5, label='Initial')\n    axs[0, 0].scatter(x_clean[:, 0], x_clean[:, 1], color='red', alpha=0.5, label='Target') \n    axs[0, 0].set_title(\"Initial and Target Distributions\")\n    axs[0, 0].legend()\n    axs[0, 0].set_xlim(xlim)\n    axs[0, 0].set_ylim(ylim)\n\n    # ======================================\n    ## Now show the full trajectories\n    # ======================================\n    axs[0, 1].scatter(x_ts[-1][:, 0], x_ts[-1][:, 1], color='red', alpha=0.5, label='Target (t=1)')\n    axs[0, 1].scatter(x_ts[0][:, 0], x_ts[0][:, 1], color='blue', alpha=0.5, label='Initial (t=0)')\n    \n    # Sample a few timesteps to show progression\n    sample_ts = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n    for t in sample_ts:\n        current_points = np.array(x_ts[t])\n        prev_points = np.array(x_ts[t-1])\n        # Calculate color interpolation factor (0 to 1)\n        factor = t / (len(x_ts)-1)\n        # Interpolate between blue (0,0,1) and red (1,0,0)\n        color = (factor, 0, 1-factor)\n        axs[0, 1].scatter(current_points[:, 0], current_points[:, 1], color=color, alpha=0.3, label=f't=0.{t}')\n        # connect the dots: previous point to current point\n        assert prev_points.shape == (n_examples, 2), f\"prev_traj.shape={prev_points.shape}\"\n        \n        for i_point in range(len(current_points)):\n            # Plot the line segment from previous point to current point\n            axs[0, 1].plot(\n                [prev_points[i_point, 0], current_points[i_point, 0]],\n                [prev_points[i_point, 1], current_points[i_point, 1]],\n                'k-', alpha=0.1\n            )\n    \n    axs[0, 1].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n    axs[0, 1].set_title(\"Flow Matching Full Trajectories\")\n    axs[0, 1].set_xlim(xlim)\n    axs[0, 1].set_ylim(ylim)\n\n    # Plot vector field at t=0\n    x_0 = x_ts[0]\n    u_0 = u_ts[0]\n    axs[1, 0].quiver(x_0[:, 0], x_0[:, 1], u_0[:, 0], u_0[:, 1], \n                alpha=0.5, scale=30, width=0.003)\n    axs[1, 0].scatter(x_0[:, 0], x_0[:, 1], c='k', alpha=0.2, s=1)\n    axs[1, 0].set_title(\"Vector Field (t=0)\")\n    axs[1, 0].set_xlim(xlim)\n    axs[1, 0].set_ylim(ylim)\n\n    # Plot vector field at t=0.5\n    mid_idx = len(x_ts)//2\n    x_mid = x_ts[mid_idx]\n    u_mid = u_ts[mid_idx]\n    axs[1, 1].quiver(x_mid[:, 0], x_mid[:, 1], u_mid[:, 0], u_mid[:, 1], \n                alpha=0.5, scale=30, width=0.003)\n    axs[1, 1].scatter(x_mid[:, 0], x_mid[:, 1], c='k', alpha=0.2, s=1)\n    axs[1, 1].set_title(\"Vector Field (t=0.5)\")\n    axs[1, 1].set_xlim(xlim)\n    axs[1, 1].set_ylim(ylim)\n\n    plt.tight_layout()\n\n\nplot_flow_matching_trajectories(FM, n_examples=30)\n\n\n\n\n\n\n\n\n\n# Drawing more samples\nplot_flow_matching_trajectories(FM, n_examples=100)\n\n\n\n\n\n\n\n\n\n# This is the \"teaching\".\nplot_flow_matching_trajectories(ExactOptimalTransportConditionalFlowMatcher(), n_examples=100)\n\n\n\n\n\n\n\n\n\n\nVisualizing the denoising flow\n\nfrom typing import Tuple\n\n\nclass TrajectorySet:\n    def __init__(self, embeddings):\n        \"\"\"\n        Managing a set of trajectories, each of which is a sequence of embeddings.\n\n        Parameters\n        ----------\n        embeddings: (n_timesteps, n_samples, *embedding_dims). This assumes\n            the first dimension is time. And it is ordered from t=0 to t=n_timesteps-1.\n            With t=0 representing the clean data and t=n_timesteps-1 representing the noise.\n\n        \"\"\"\n        self.embeddings = embeddings\n        self.embeddings_2d = None\n    \n    def run_tsne(self, n_components: int = 2, seed: int = 0, **kwargs):\n        \"\"\"Run t-SNE on the embeddings.\n        \"\"\"\n        print(f\"Running t-SNE on {self.embeddings.shape} embeddings...\")\n        from sklearn.manifold import TSNE\n        tsne = TSNE(n_components=n_components, random_state=seed, **kwargs)\n        flattened_embeddings = self.embeddings.reshape(-1, self.embeddings.shape[-1])\n        flattened_embeddings_2d = tsne.fit_transform(flattened_embeddings)\n        self.embeddings_2d = flattened_embeddings_2d.reshape(self.embeddings.shape[0], self.embeddings.shape[1], -1)\n        print(f\"t-SNE done. Shape of 2D embeddings: {self.embeddings_2d.shape}\")\n        return self.embeddings_2d\n    \n    def plot_trajectories(\n            self,\n            n: int = 10,\n            show_figure: bool = False,\n            noise_color: Tuple[float, float, float] = (0, 0, 1),  # blue\n            data_color: Tuple[float, float, float] = (1, 0, 0),  # red\n            figsize: tuple = (6, 6),\n            xlim: Tuple[float, float] = None,\n            ylim: Tuple[float, float] = None,\n            with_ticks: bool = False,\n            title: str = None,\n            tsne_seed: int = 0,\n            **kwargs):\n        \"\"\"Plot trajectories of some selected samples.\n\n        This assumes the first dimension is time. And it is ordered from t=0 to t=n_timesteps-1.\n        With t=0 representing the clean data and t=n_timesteps-1 representing the noise.\n\n        Parameters\n        ----------\n        n: int\n            number of samples to plot\n        figsize: tuple\n            figure size\n        kwargs:\n            other keyword arguments for matplotlib.pyplot.scatter\n        \"\"\"\n        import numpy as np\n        import matplotlib.pyplot as plt\n\n        colors = []\n        for t in range(self.embeddings.shape[0]):\n            # interpolate between noise_color and data_color\n            factor = t / (self.embeddings.shape[0] - 1)\n            colors.append(np.array(noise_color) * factor + np.array(data_color) * (1 - factor))\n        colors = np.array(colors)\n        \n        if self.embeddings_2d is None:\n            if self.embeddings.shape[2] == 2:\n                self.embeddings_2d = self.embeddings\n            else:\n                self.embeddings_2d = self.run_tsne(seed=tsne_seed)\n\n        traj = self.embeddings_2d[:, :n, :]\n        g = plt.figure(figsize=figsize)\n        plt.scatter(traj[0, :n, 0], traj[0, :n, 1], s=10, alpha=0.8, c=\"red\")  # real\n        plt.scatter(traj[-1, :n, 0], traj[-1, :n, 1], s=4, alpha=1, c=\"blue\")  # noise\n        plt.scatter(traj[:, :n, 0], traj[:, :n, 1], s=0.5, alpha=0.7, c=colors.repeat(n, axis=0))  # \"olive\"\n        plt.plot(traj[:, :n, 0], traj[:, :n, 1], c=\"olive\", alpha=0.3)\n        if xlim is not None:\n            plt.xlim(xlim)\n        if ylim is not None:\n            plt.ylim(ylim)\n        plt.legend([\"Data\", \"Noise\", \"Intermediate Samples (color coded)\", \"Flow trajectory\"], loc=\"upper right\")\n        if not with_ticks:\n            plt.xticks([])\n            plt.yticks([])\n        elif xlim is not None and ylim is not None:\n            plt.xticks(xlim)\n            plt.yticks(ylim)\n        if title is not None:\n            plt.title(title)\n        if show_figure:\n            plt.show()\n        \n        plt.tight_layout()\n        # save to bytes (png)\n        import io\n\n        bytes_io = io.BytesIO()\n        g.savefig(bytes_io, format=\"png\")\n        return bytes_io.getvalue()\n        \n        # # return the figure\n        # return plt.gcf()\n\ndef generate_trajectories_from_denoising_model(model, num_samples=100, num_timesteps=100, data_dims=[2,], seed=0, device=device):\n    node = NeuralODE(model, solver=\"euler\", sensitivity=\"adjoint\")\n    torch.manual_seed(seed)\n    with torch.no_grad():\n        traj = node.trajectory(\n            torch.randn(num_samples, *data_dims, device=device),\n            t_span=torch.linspace(0, 1, num_timesteps, device=device),\n        )\n    traj = traj.cpu().numpy()\n    # invert the time dimension to make it start from clean data to noise\n    traj = traj[::-1, ...]\n    traj_set = TrajectorySet(traj)\n    return traj_set\n\n\ndef visualize_denoising_flow(model, n=100, seed=0):\n    traj_set = generate_trajectories_from_denoising_model(model, num_samples=n, seed=seed)\n    traj_set.plot_trajectories(n=n, show_figure=True, figsize=(8, 8))\n\n\n# This is the \"learning\".\nvisualize_denoising_flow(model3, n=100)\n\n\n\n\n\n\n\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\n# A shared state to store the frames for the vector field animation.\n_state = {\n    \"frames\": {},\n}\n\ndef visualize_denoising_vector_field(model, t=0, num_samples=32, num_timesteps=100, seed=0):\n    # visualize the vector field of the denoising model\n    # The denoising model is a velocity field.\n    # Given a grid of points, we can visualize the vector field at these points.\n    x_grid = torch.linspace(-3.6, 3.6, 20)\n    y_grid = torch.linspace(-3.6, 3.6, 20)\n    x_grid, y_grid = torch.meshgrid(x_grid, y_grid)\n    points = torch.stack([x_grid, y_grid], dim=-1)\n    points = points.view(-1, 2).to(device)\n    t_int = int(t * num_timesteps)\n    t = t * torch.ones(points.shape[0]).to(device)\n    with torch.no_grad():\n        v = model(t=t, x=points)\n        v = v.cpu().numpy()\n    # plot the vector field\n    points = points.cpu().numpy()\n    x_grid = x_grid.cpu().numpy()\n    y_grid = y_grid.cpu().numpy()\n\n    if \"traj\" not in _state:\n        node = NeuralODE(model, solver=\"euler\", sensitivity=\"adjoint\")\n        data_dims = [2,]\n        torch.manual_seed(seed)\n        with torch.no_grad():\n            _state[\"traj\"] = node.trajectory(\n                torch.randn(num_samples, *data_dims, device=device),\n                t_span=torch.linspace(0, 1, num_timesteps, device=device),\n            ).cpu().numpy()\n    \n    traj = _state[\"traj\"]\n    g = plt.figure(figsize=(6, 6))\n    plt.quiver(x_grid, y_grid, v[:, 0], v[:, 1], alpha=0.5, scale=30, width=0.003)\n    plt.scatter(points[:, 0], points[:, 1], c='k', alpha=0.2, s=1)\n    # overlay with the points at the current timestep\n    plt.scatter(traj[t_int, :, 0], traj[t_int, :, 1], c='blue', edgecolor='gray', alpha=0.5, s=25)\n    plt.title(f\"t={t_int}\")\n    plt.show()\n    \n    # save the figure into a numpy array\n    # g = plt.gcf()\n    # save figure as an image\n    import io\n    bytes_io = io.BytesIO()\n    g.savefig(fname=bytes_io, format=\"png\")\n    _state[\"frames\"][t_int] = bytes_io.getvalue()\n    # return g\n\n\n# create an interactive widget to visualize the vector field at different timesteps\nfrom ipywidgets import interact, fixed\n\ninteract(visualize_denoising_vector_field, model=fixed(model3), t=(0.0, 0.99, 0.01), num_samples=fixed(100), num_timesteps=fixed(100), seed=fixed(0))\n\n\n\n\n\n&lt;function __main__.visualize_denoising_vector_field(model, t=0, num_samples=32, num_timesteps=100, seed=0)&gt;\n\n\n\n# Generate a GIF from the frames\nfrom PIL import Image\nimport io\n# _state[\"frames\"] has bytes objects (png images)\nframes = list(_state[\"frames\"].values())\nframes = [Image.open(io.BytesIO(frame)).resize((360, 360)) for frame in frames]\nframes[0].save(\"initial_denoising_vector_field.gif\", save_all=True, append_images=frames[1:], duration=500, loop=0)",
    "crumbs": [
      "1.2 Flow Matching for a 2D Point Cloud"
    ]
  },
  {
    "objectID": "1_2_Flow Matching 2D Toy.html#training-code",
    "href": "1_2_Flow Matching 2D Toy.html#training-code",
    "title": "Training a Flow Matching Model for 2D Point Generation",
    "section": "Training code",
    "text": "Training code\nWe’ve picked our data, created our dataloaders, and defined our noise schedule, foward diffusion, and reverse diffusion steps. Now, the only thing left to do is to train our diffusion model.\n\ndef train(model: nn.Module, optimizer: torch.optim.Optimizer, steps: int=100, quiet: bool=False) -&gt; float:\n  model.train()\n  if not quiet:\n    print(\"Training on device:\", device)\n  max_train_steps = steps\n\n  step = 0\n  loss = None\n  while step &lt; max_train_steps:\n\n    if quiet:\n      progress_bar = train_dataloader\n    else:\n      progress_bar = tqdm(train_dataloader, total=min(max_train_steps - step, len(train_dataloader)))\n    for x_1 in progress_bar:\n      optimizer.zero_grad()\n      x_1 = x_1.float().to(device)\n\n      x_0 = torch.randn_like(x_1).to(device)\n      t, x_t, u_t = FM.sample_location_and_conditional_flow(x_0, x_1)\n\n      v_t = model(t=t, x=x_t)\n      v_t = v_t.sample if hasattr(v_t, \"sample\") else v_t\n      loss = torch.mean((v_t - u_t) ** 2)  # MSE loss\n      loss.backward()\n      optimizer.step()\n\n      if not quiet:\n        progress_bar.set_postfix({\"loss\": loss.cpu().item()})\n\n      step += 1\n\n      if step &gt;= max_train_steps:\n        if not quiet:\n          print(f\"Reached the max training steps:\", max_train_steps)\n        break\n\n  return loss\n\n# Define the model and optimizer\nmodel3 = Model3(\n    dim_in=2,\n    dim_out=2,\n    dim_hids=[128, 128, 128],  # [128, 128, 256],\n    num_timesteps=config.num_denoising_steps,\n).to(device)\nprint(f\"model params: {sum(p.numel() for p in model3.parameters())}\")\nmodel3_optimizer = optim.AdamW(model3.parameters(), lr=config.learning_rate, weight_decay=config.weight_decay)\n\n# Train the model\nmodel3_loss = train(model3, model3_optimizer, steps=1000)\n\nmodel params: 182410\nTraining on device: cuda\n\n\n\n\n\nReached the max training steps: 1000",
    "crumbs": [
      "1.2 Flow Matching for a 2D Point Cloud"
    ]
  },
  {
    "objectID": "1_2_Flow Matching 2D Toy.html#model-1",
    "href": "1_2_Flow Matching 2D Toy.html#model-1",
    "title": "Training a Flow Matching Model for 2D Point Generation",
    "section": "Model 1",
    "text": "Model 1\n\nmodel1 = Model1().to(device)\nprint(f\"model params: {sum(p.numel() for p in model1.parameters())}\")\n\nmodel1_optimizer = optim.AdamW(model1.parameters(), lr=config.learning_rate, weight_decay=config.weight_decay)\nmodel1_loss = train(model1, model1_optimizer, steps=10000)\nprint(\"model1_loss:\", model1_loss.item())\n_ = visualize_sampled_data(model1)\n\nmodel params: 265730\nTraining on device: cuda\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReached the max training steps: 10000\nmodel1_loss: 3.6429638862609863",
    "crumbs": [
      "1.2 Flow Matching for a 2D Point Cloud"
    ]
  },
  {
    "objectID": "1_2_Flow Matching 2D Toy.html#model-2",
    "href": "1_2_Flow Matching 2D Toy.html#model-2",
    "title": "Training a Flow Matching Model for 2D Point Generation",
    "section": "Model 2",
    "text": "Model 2\n\nmodel2 = Model2(features=2, hidden_features=[256, 256, 512]).to(device)\nprint(f\"model params: {sum(p.numel() for p in model2.parameters())}\")\n\nmodel2_optimizer = optim.AdamW(model2.parameters(), lr=config.learning_rate, weight_decay=config.weight_decay)\nmodel2_loss = train(model2, model2_optimizer, steps=10000)\nprint(\"model2_loss:\", model2_loss.item())\n_ = visualize_sampled_data(model2)\n\nmodel params: 207362\nTraining on device: cuda\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReached the max training steps: 10000\nmodel2_loss: 2.836972236633301",
    "crumbs": [
      "1.2 Flow Matching for a 2D Point Cloud"
    ]
  },
  {
    "objectID": "1_2_Flow Matching 2D Toy.html#model-3",
    "href": "1_2_Flow Matching 2D Toy.html#model-3",
    "title": "Training a Flow Matching Model for 2D Point Generation",
    "section": "Model 3",
    "text": "Model 3\n\nmodel3 = Model3(\n    dim_in=2,\n    dim_out=2,\n    dim_hids=[128, 128, 128],\n    num_timesteps=config.num_denoising_steps,\n).to(device)\nprint(f\"model params: {sum(p.numel() for p in model3.parameters())}\")\n\nmodel3_optimizer = optim.AdamW(model3.parameters(), lr=config.learning_rate, weight_decay=config.weight_decay)\nmodel3_loss = train(model3, model3_optimizer, steps=10000)\nprint(\"model3_loss:\", model3_loss.item())\n_ = visualize_sampled_data(model3)\n\nmodel params: 182410\nTraining on device: cuda\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReached the max training steps: 10000\nmodel3_loss: 2.9682607650756836\n\n\n\n\n\n\n\n\n\n\nimport torch\n\n# Sample points from the model\ndef generate_points(model):\n    with torch.no_grad():\n        x_T = torch.randn(128, 2)\n        x_sampled = generate_samples_with_flow_matching(model, device, num_samples=128, parallel=False, seed=0, clip_min=-3.6, clip_max=3.6)\n    return x_sampled.cpu().numpy()\n\n\n# The target spiral points for comparison\ntarget_spiral = next(iter(train_dataloader))\nchamfer_dist = {}\n\ngenerated_points_1 = generate_points(model1)\ngenerated_points_1 = np.clip(generated_points_1, -3, 3)\nchamfer_dist[1] = chamfer_distance(generated_points_1, target_spiral, direction='bi')\nprint(\"Model 1 Chamfer Distance:\", chamfer_dist[1])\n\n\ngenerated_points_2 = generate_points(model2)\nchamfer_dist[2] = chamfer_distance(generated_points_2, target_spiral, direction='bi')\nprint(\"Model 2 Chamfer Distance:\", chamfer_dist[2])\n\n# Calculate Chamfer distance\ngenerated_points_3 = generate_points(model3)\nchamfer_dist[3] = chamfer_distance(generated_points_3, target_spiral, direction='bi')\nprint(\"Model 3 Chamfer Distance:\", chamfer_dist[3])\n\n# # visualize the sampled images side by side\ndef visualize_sampled_data_side_by_side(models, generated_points):\n    fig, axs = plt.subplots(1, len(models) + 1, figsize=(5 * len(models), 5))\n    # Add ground truth\n    axs[0].scatter(target_spiral[:, 0], target_spiral[:, 1], color='white', edgecolor='gray', s=5)\n    axs[0].set_title(\"Ground truth\")\n\n    for i, (model, points) in enumerate(zip(models, generated_points)):\n        axs[i+1].scatter(points[:,0], points[:,1], color='white', edgecolor='gray', s=5)\n        axs[i+1].set_title(f\"{model.__class__.__name__}, Chamfer: {chamfer_dist[i+1]:.2f}\")\n\nvisualize_sampled_data_side_by_side([model1, model2, model3], [generated_points_1, generated_points_2, generated_points_3])\n\nModel 1 Chamfer Distance: 0.9377454986740738\nModel 2 Chamfer Distance: 0.2691695609444996\nModel 3 Chamfer Distance: 0.17190213142619662\n\n\n\n\n\n\n\n\n\nVisually and quantitatively, Model 3 and Model 2 outpeform Model 1 a large margin. Model 3 is also clearly better than Model 2.\nTry experimenting with the hidden layer configuration and the number of timesteps. You should be able to get all the models to produce perfect spirals by simply increasing the number of parameters or increasing the number of timesteps.",
    "crumbs": [
      "1.2 Flow Matching for a 2D Point Cloud"
    ]
  },
  {
    "objectID": "1_2_Flow Matching 2D Toy.html#questions-for-future-exploration",
    "href": "1_2_Flow Matching 2D Toy.html#questions-for-future-exploration",
    "title": "Training a Flow Matching Model for 2D Point Generation",
    "section": "Questions for future exploration",
    "text": "Questions for future exploration\n\nHow does the model size matter (number of parameters, controlled by the number of layers, and number of hidden units), within the same class of arch?\nHow does the number of timesteps matter?\nWhat other designs of flow plans are there in recent literature? Can you bring them to this toy example?",
    "crumbs": [
      "1.2 Flow Matching for a 2D Point Cloud"
    ]
  },
  {
    "objectID": "3_1_fid.html",
    "href": "3_1_fid.html",
    "title": "Quality evaluation of an image generation model",
    "section": "",
    "text": "(under construction)",
    "crumbs": [
      "3.1 Evaluation: FID"
    ]
  },
  {
    "objectID": "3_1_fid.html#fid",
    "href": "3_1_fid.html#fid",
    "title": "Quality evaluation of an image generation model",
    "section": "FID",
    "text": "FID\nThe Chamfer distance that we used in the previous notebook is a distance between point clouds. It is based on Euclidean distance, which works for low-dimensional data such as 2D points. For high-dimensional data such as images, the Euclidean distance is not a good measure, because the distance between two images can be high even if they are very similar.\nThe Fréchet Inception Distance (FID) is a metric that measures the distance between two distributions of images. A lower score means better quality. It is based on the Fréchet distance, which is a distance between Gaussian distributions. “Inception” here refers to a pre-trained Inception-v3 model, which is a convolutional neural network that was trained to classify images into 1000 categories. We can use the intermediate features of this model to measure the distance between two distributions of images.\nThe limitations of FID include: - It assumes Gaussian distribution of features. - It is sensitive to sample size and requires large sample sizes for stable estimates, which means it is computationally expensive.\nOther metrics include: - Inception Score (IS): measures both the diversity and quality of generated images. It may give misleading scores for specialized datasets (e.g., medical images, abstract art). - LPIPS (Learned Perceptual Image Patch Similarity). It is good at measureing individual image quality, and more suitable for tasks like style transfer, image reconstruction. - PSNR (Peak Signal-to-Noise Ratio). It measures pixel-level reconstruction quality,and does not measure perceptual quality. - SSIM (Structural Similarity Index). It measures image quality based on luminance and contrast, and does not measure perceptual quality. - Arena-style human evaluation: compare generated images from different models side by side, and ask human experts to choose the better one.\nWe are going to use the packageclean-fid. As a santity check for the metric, let’s take a pre-trained model from huggingface with known good quality, and see what the FID score looks like.",
    "crumbs": [
      "3.1 Evaluation: FID"
    ]
  },
  {
    "objectID": "compare_vlms.html",
    "href": "compare_vlms.html",
    "title": "Compare VLMs",
    "section": "",
    "text": "The VLM dashboard has a list of VLMs and their performance scores.\nWe want to pick a small-ish (under 8B) model that is sufficient to generate captions for animal face images in the AFHQ dataset."
  },
  {
    "objectID": "compare_vlms.html#install-dependencies",
    "href": "compare_vlms.html#install-dependencies",
    "title": "Compare VLMs",
    "section": "Install Dependencies",
    "text": "Install Dependencies\n\n%load_ext autoreload\n%autoreload 2\n\n\n!pip install peft==0.13.2 flash_attn==2.7.* qwen_vl_utils==0.0.8 transformers==4.47.* autoawq==0.2.* jinja2==3.1.4 xformers==0.0.28 bitsandbytes==0.45.0\n\n\nDefaulting to user installation because normal site-packages is not writeable\nWARNING: Ignoring invalid distribution ~vidia-cusparse-cu12 (/home/ubuntu/.local/lib/python3.11/site-packages)\nWARNING: Ignoring invalid distribution ~vidia-cusparse-cu12 (/home/ubuntu/.local/lib/python3.11/site-packages)\nRequirement already satisfied: peft==0.13.2 in /home/ubuntu/.local/lib/python3.11/site-packages (0.13.2)\nRequirement already satisfied: flash_attn==2.7.* in /home/ubuntu/.local/lib/python3.11/site-packages (2.7.0.post2)\nRequirement already satisfied: qwen_vl_utils==0.0.8 in /home/ubuntu/.local/lib/python3.11/site-packages (0.0.8)\nRequirement already satisfied: transformers==4.47.* in /home/ubuntu/.local/lib/python3.11/site-packages (4.47.0)\nRequirement already satisfied: autoawq==0.2.* in /home/ubuntu/.local/lib/python3.11/site-packages (0.2.7.post2)\nRequirement already satisfied: jinja2==3.1.4 in /home/ubuntu/.local/lib/python3.11/site-packages (3.1.4)\nCollecting xformers==0.0.28\n  Downloading xformers-0.0.28-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (1.0 kB)\nRequirement already satisfied: bitsandbytes==0.45.0 in /home/ubuntu/.local/lib/python3.11/site-packages (0.45.0)\nRequirement already satisfied: numpy&gt;=1.17 in /home/ubuntu/.local/lib/python3.11/site-packages (from peft==0.13.2) (1.26.4)\nRequirement already satisfied: packaging&gt;=20.0 in /opt/miniconda/lib/python3.11/site-packages (from peft==0.13.2) (23.1)\nRequirement already satisfied: psutil in /home/ubuntu/.local/lib/python3.11/site-packages (from peft==0.13.2) (5.9.8)\nRequirement already satisfied: pyyaml in /home/ubuntu/.local/lib/python3.11/site-packages (from peft==0.13.2) (6.0.1)\nRequirement already satisfied: torch&gt;=1.13.0 in /home/ubuntu/.local/lib/python3.11/site-packages (from peft==0.13.2) (2.5.1)\nRequirement already satisfied: tqdm in /home/ubuntu/.local/lib/python3.11/site-packages (from peft==0.13.2) (4.66.5)\nRequirement already satisfied: accelerate&gt;=0.21.0 in /home/ubuntu/.local/lib/python3.11/site-packages (from peft==0.13.2) (0.33.0)\nRequirement already satisfied: safetensors in /home/ubuntu/.local/lib/python3.11/site-packages (from peft==0.13.2) (0.4.2)\nRequirement already satisfied: huggingface-hub&gt;=0.17.0 in /home/ubuntu/.local/lib/python3.11/site-packages (from peft==0.13.2) (0.24.5)\nRequirement already satisfied: einops in /home/ubuntu/.local/lib/python3.11/site-packages (from flash_attn==2.7.*) (0.7.0)\nRequirement already satisfied: av in /home/ubuntu/.local/lib/python3.11/site-packages (from qwen_vl_utils==0.0.8) (14.0.0)\nRequirement already satisfied: pillow in /home/ubuntu/.local/lib/python3.11/site-packages (from qwen_vl_utils==0.0.8) (10.2.0)\nRequirement already satisfied: requests in /home/ubuntu/.local/lib/python3.11/site-packages (from qwen_vl_utils==0.0.8) (2.32.3)\nRequirement already satisfied: filelock in /home/ubuntu/.local/lib/python3.11/site-packages (from transformers==4.47.*) (3.13.3)\nRequirement already satisfied: regex!=2019.12.17 in /home/ubuntu/.local/lib/python3.11/site-packages (from transformers==4.47.*) (2023.12.25)\nRequirement already satisfied: tokenizers&lt;0.22,&gt;=0.21 in /home/ubuntu/.local/lib/python3.11/site-packages (from transformers==4.47.*) (0.21.0)\nRequirement already satisfied: triton in /home/ubuntu/.local/lib/python3.11/site-packages (from autoawq==0.2.*) (3.1.0)\nRequirement already satisfied: typing-extensions&gt;=4.8.0 in /home/ubuntu/.local/lib/python3.11/site-packages (from autoawq==0.2.*) (4.10.0)\nRequirement already satisfied: datasets&gt;=2.20 in /home/ubuntu/.local/lib/python3.11/site-packages (from autoawq==0.2.*) (2.21.0)\nRequirement already satisfied: zstandard in /opt/miniconda/lib/python3.11/site-packages (from autoawq==0.2.*) (0.19.0)\nRequirement already satisfied: MarkupSafe&gt;=2.0 in /home/ubuntu/.local/lib/python3.11/site-packages (from jinja2==3.1.4) (2.1.5)\nCollecting torch&gt;=1.13.0 (from peft==0.13.2)\n  Using cached torch-2.4.1-cp311-cp311-manylinux1_x86_64.whl.metadata (26 kB)\nRequirement already satisfied: sympy in /home/ubuntu/.local/lib/python3.11/site-packages (from torch&gt;=1.13.0-&gt;peft==0.13.2) (1.13.1)\nRequirement already satisfied: networkx in /home/ubuntu/.local/lib/python3.11/site-packages (from torch&gt;=1.13.0-&gt;peft==0.13.2) (3.2.1)\nRequirement already satisfied: fsspec in /home/ubuntu/.local/lib/python3.11/site-packages (from torch&gt;=1.13.0-&gt;peft==0.13.2) (2024.3.1)\nCollecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch&gt;=1.13.0-&gt;peft==0.13.2)\n  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch&gt;=1.13.0-&gt;peft==0.13.2)\n  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch&gt;=1.13.0-&gt;peft==0.13.2)\n  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/ubuntu/.local/lib/python3.11/site-packages (from torch&gt;=1.13.0-&gt;peft==0.13.2) (9.1.0.70)\nCollecting nvidia-cublas-cu12==12.1.3.1 (from torch&gt;=1.13.0-&gt;peft==0.13.2)\n  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.0.2.54 (from torch&gt;=1.13.0-&gt;peft==0.13.2)\n  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.2.106 (from torch&gt;=1.13.0-&gt;peft==0.13.2)\n  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch&gt;=1.13.0-&gt;peft==0.13.2)\n  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch&gt;=1.13.0-&gt;peft==0.13.2)\n  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-nccl-cu12==2.20.5 (from torch&gt;=1.13.0-&gt;peft==0.13.2)\n  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\nCollecting nvidia-nvtx-cu12==12.1.105 (from torch&gt;=1.13.0-&gt;peft==0.13.2)\n  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\nCollecting triton (from autoawq==0.2.*)\n  Using cached triton-3.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\nRequirement already satisfied: nvidia-nvjitlink-cu12 in /home/ubuntu/.local/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107-&gt;torch&gt;=1.13.0-&gt;peft==0.13.2) (12.4.127)\nRequirement already satisfied: pyarrow&gt;=15.0.0 in /home/ubuntu/.local/lib/python3.11/site-packages (from datasets&gt;=2.20-&gt;autoawq==0.2.*) (17.0.0)\nRequirement already satisfied: dill&lt;0.3.9,&gt;=0.3.0 in /home/ubuntu/.local/lib/python3.11/site-packages (from datasets&gt;=2.20-&gt;autoawq==0.2.*) (0.3.8)\nRequirement already satisfied: pandas in /home/ubuntu/.local/lib/python3.11/site-packages (from datasets&gt;=2.20-&gt;autoawq==0.2.*) (2.2.2)\nRequirement already satisfied: xxhash in /home/ubuntu/.local/lib/python3.11/site-packages (from datasets&gt;=2.20-&gt;autoawq==0.2.*) (3.5.0)\nRequirement already satisfied: multiprocess in /home/ubuntu/.local/lib/python3.11/site-packages (from datasets&gt;=2.20-&gt;autoawq==0.2.*) (0.70.16)\nRequirement already satisfied: aiohttp in /home/ubuntu/.local/lib/python3.11/site-packages (from datasets&gt;=2.20-&gt;autoawq==0.2.*) (3.10.5)\nRequirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /opt/miniconda/lib/python3.11/site-packages (from requests-&gt;qwen_vl_utils==0.0.8) (2.0.4)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in /opt/miniconda/lib/python3.11/site-packages (from requests-&gt;qwen_vl_utils==0.0.8) (3.4)\nRequirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in /opt/miniconda/lib/python3.11/site-packages (from requests-&gt;qwen_vl_utils==0.0.8) (1.26.18)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /opt/miniconda/lib/python3.11/site-packages (from requests-&gt;qwen_vl_utils==0.0.8) (2023.7.22)\nRequirement already satisfied: aiohappyeyeballs&gt;=2.3.0 in /home/ubuntu/.local/lib/python3.11/site-packages (from aiohttp-&gt;datasets&gt;=2.20-&gt;autoawq==0.2.*) (2.4.0)\nRequirement already satisfied: aiosignal&gt;=1.1.2 in /home/ubuntu/.local/lib/python3.11/site-packages (from aiohttp-&gt;datasets&gt;=2.20-&gt;autoawq==0.2.*) (1.3.1)\nRequirement already satisfied: attrs&gt;=17.3.0 in /home/ubuntu/.local/lib/python3.11/site-packages (from aiohttp-&gt;datasets&gt;=2.20-&gt;autoawq==0.2.*) (23.2.0)\nRequirement already satisfied: frozenlist&gt;=1.1.1 in /home/ubuntu/.local/lib/python3.11/site-packages (from aiohttp-&gt;datasets&gt;=2.20-&gt;autoawq==0.2.*) (1.4.1)\nRequirement already satisfied: multidict&lt;7.0,&gt;=4.5 in /home/ubuntu/.local/lib/python3.11/site-packages (from aiohttp-&gt;datasets&gt;=2.20-&gt;autoawq==0.2.*) (6.0.5)\nRequirement already satisfied: yarl&lt;2.0,&gt;=1.0 in /home/ubuntu/.local/lib/python3.11/site-packages (from aiohttp-&gt;datasets&gt;=2.20-&gt;autoawq==0.2.*) (1.9.4)\nRequirement already satisfied: python-dateutil&gt;=2.8.2 in /home/ubuntu/.local/lib/python3.11/site-packages (from pandas-&gt;datasets&gt;=2.20-&gt;autoawq==0.2.*) (2.9.0.post0)\nRequirement already satisfied: pytz&gt;=2020.1 in /home/ubuntu/.local/lib/python3.11/site-packages (from pandas-&gt;datasets&gt;=2.20-&gt;autoawq==0.2.*) (2024.1)\nRequirement already satisfied: tzdata&gt;=2022.7 in /home/ubuntu/.local/lib/python3.11/site-packages (from pandas-&gt;datasets&gt;=2.20-&gt;autoawq==0.2.*) (2024.1)\nRequirement already satisfied: mpmath&lt;1.4,&gt;=1.1.0 in /home/ubuntu/.local/lib/python3.11/site-packages (from sympy-&gt;torch&gt;=1.13.0-&gt;peft==0.13.2) (1.3.0)\nRequirement already satisfied: six&gt;=1.5 in /home/ubuntu/.local/lib/python3.11/site-packages (from python-dateutil&gt;=2.8.2-&gt;pandas-&gt;datasets&gt;=2.20-&gt;autoawq==0.2.*) (1.16.0)\nDownloading xformers-0.0.28-cp311-cp311-manylinux_2_28_x86_64.whl (16.7 MB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 16.7/16.7 MB 14.8 MB/s eta 0:00:00a 0:00:01\nUsing cached torch-2.4.1-cp311-cp311-manylinux1_x86_64.whl (797.1 MB)\nUsing cached triton-3.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.4 MB)\nUsing cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\nUsing cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\nUsing cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\nUsing cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\nUsing cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\nUsing cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\nUsing cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\nUsing cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\nUsing cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\nUsing cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\nWARNING: Ignoring invalid distribution ~vidia-cusparse-cu12 (/home/ubuntu/.local/lib/python3.11/site-packages)\nInstalling collected packages: triton, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusolver-cu12, torch, xformers\n  Attempting uninstall: triton\n    WARNING: Ignoring invalid distribution ~vidia-cusparse-cu12 (/home/ubuntu/.local/lib/python3.11/site-packages)\n    Found existing installation: triton 3.1.0\n    Uninstalling triton-3.1.0:\n      Successfully uninstalled triton-3.1.0\n  Attempting uninstall: nvidia-nvtx-cu12\n    WARNING: Ignoring invalid distribution ~vidia-cusparse-cu12 (/home/ubuntu/.local/lib/python3.11/site-packages)\n    Found existing installation: nvidia-nvtx-cu12 12.4.127\n    Uninstalling nvidia-nvtx-cu12-12.4.127:\n      Successfully uninstalled nvidia-nvtx-cu12-12.4.127\n  Attempting uninstall: nvidia-nccl-cu12\n    WARNING: Ignoring invalid distribution ~vidia-cusparse-cu12 (/home/ubuntu/.local/lib/python3.11/site-packages)\n    Found existing installation: nvidia-nccl-cu12 2.21.5\n    Uninstalling nvidia-nccl-cu12-2.21.5:\n      Successfully uninstalled nvidia-nccl-cu12-2.21.5\n  Attempting uninstall: nvidia-cusparse-cu12\n    WARNING: Ignoring invalid distribution ~vidia-cusparse-cu12 (/home/ubuntu/.local/lib/python3.11/site-packages)\n    Found existing installation: nvidia-cusparse-cu12 12.3.1.170\n    Uninstalling nvidia-cusparse-cu12-12.3.1.170:\n      Successfully uninstalled nvidia-cusparse-cu12-12.3.1.170\n  Attempting uninstall: nvidia-curand-cu12\n    WARNING: Ignoring invalid distribution ~vidia-cusparse-cu12 (/home/ubuntu/.local/lib/python3.11/site-packages)\n    Found existing installation: nvidia-curand-cu12 10.3.5.147\n    Uninstalling nvidia-curand-cu12-10.3.5.147:\n      Successfully uninstalled nvidia-curand-cu12-10.3.5.147\n  Attempting uninstall: nvidia-cufft-cu12\n    WARNING: Ignoring invalid distribution ~vidia-cusparse-cu12 (/home/ubuntu/.local/lib/python3.11/site-packages)\n    Found existing installation: nvidia-cufft-cu12 11.2.1.3\n    Uninstalling nvidia-cufft-cu12-11.2.1.3:\n      Successfully uninstalled nvidia-cufft-cu12-11.2.1.3\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    WARNING: Ignoring invalid distribution ~vidia-cusparse-cu12 (/home/ubuntu/.local/lib/python3.11/site-packages)\n    Found existing installation: nvidia-cuda-runtime-cu12 12.4.127\n    Uninstalling nvidia-cuda-runtime-cu12-12.4.127:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.4.127\n  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n    WARNING: Ignoring invalid distribution ~vidia-cusparse-cu12 (/home/ubuntu/.local/lib/python3.11/site-packages)\n    Found existing installation: nvidia-cuda-nvrtc-cu12 12.4.127\n    Uninstalling nvidia-cuda-nvrtc-cu12-12.4.127:\n      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.4.127\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    WARNING: Ignoring invalid distribution ~vidia-cusparse-cu12 (/home/ubuntu/.local/lib/python3.11/site-packages)\n    Found existing installation: nvidia-cuda-cupti-cu12 12.4.127\n    Uninstalling nvidia-cuda-cupti-cu12-12.4.127:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.4.127\n  Attempting uninstall: nvidia-cublas-cu12\n    WARNING: Ignoring invalid distribution ~vidia-cusparse-cu12 (/home/ubuntu/.local/lib/python3.11/site-packages)\n    Found existing installation: nvidia-cublas-cu12 12.4.5.8\n    Uninstalling nvidia-cublas-cu12-12.4.5.8:\n      Successfully uninstalled nvidia-cublas-cu12-12.4.5.8\n  Attempting uninstall: nvidia-cusolver-cu12\n    WARNING: Ignoring invalid distribution ~vidia-cusparse-cu12 (/home/ubuntu/.local/lib/python3.11/site-packages)\n    Found existing installation: nvidia-cusolver-cu12 11.6.1.9\n    Uninstalling nvidia-cusolver-cu12-11.6.1.9:\n      Successfully uninstalled nvidia-cusolver-cu12-11.6.1.9\n  Attempting uninstall: torch\n    WARNING: Ignoring invalid distribution ~vidia-cusparse-cu12 (/home/ubuntu/.local/lib/python3.11/site-packages)\n    Found existing installation: torch 2.5.1\n    Uninstalling torch-2.5.1:\n      Successfully uninstalled torch-2.5.1\n  Attempting uninstall: xformers\n    WARNING: Ignoring invalid distribution ~vidia-cusparse-cu12 (/home/ubuntu/.local/lib/python3.11/site-packages)\n    Found existing installation: xformers 0.0.28.post3\n    Uninstalling xformers-0.0.28.post3:\n      Successfully uninstalled xformers-0.0.28.post3\nWARNING: Ignoring invalid distribution ~vidia-cusparse-cu12 (/home/ubuntu/.local/lib/python3.11/site-packages)\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntorchvision 0.20.1 requires torch==2.5.1, but you have torch 2.4.1 which is incompatible.\nSuccessfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvtx-cu12-12.1.105 torch-2.4.1 triton-3.0.0 xformers-0.0.28\nWARNING: Ignoring invalid distribution ~vidia-cusparse-cu12 (/home/ubuntu/.local/lib/python3.11/site-packages)\nWARNING: Ignoring invalid distribution ~vidia-cusparse-cu12 (/home/ubuntu/.local/lib/python3.11/site-packages)\n\n\n\nimport transformers\nimport jinja2\nimport bitsandbytes\n\nprint(transformers.__version__)\nprint(jinja2.__version__)\nprint(bitsandbytes.__version__)\njinja2.parser\n\n4.47.0\n3.1.4\n0.45.0\n\n\n&lt;module 'jinja2.parser' from '/home/ubuntu/.local/lib/python3.11/site-packages/jinja2/parser.py'&gt;"
  },
  {
    "objectID": "compare_vlms.html#load-images",
    "href": "compare_vlms.html#load-images",
    "title": "Compare VLMs",
    "section": "Load Images",
    "text": "Load Images\n\nfrom torch.utils.data import Dataset\nfrom datasets import load_dataset\nfrom torchvision import transforms\nimport torch\nfrom transformers import AutoModel, AutoTokenizer\n\n\ndataset_name = \"zzsi/afhq64_16k\"\n\n\nclass HuggingFaceDataset(Dataset):\n    def __init__(self, dataset_path: str, transform=None):\n        self.dataset = load_dataset(dataset_path, split=\"train\")\n        self.transform = transform\n        self.image_key = self.find_image_key()\n\n    def find_image_key(self) -&gt; str:\n        # Check if the dataset has the \"image\" key\n        # NOTE: Can exapnd this to other common keys if needed\n        if \"image\" in self.dataset[0].keys():\n            return \"image\"\n        raise KeyError(\"Dataset does not have an 'image' key\")\n\n    def __len__(self):\n        return len(self.dataset)\n\n    def __getitem__(self, idx):\n        image = self.dataset[idx][self.image_key]\n        image = image.convert(\"RGB\")  # Convert to RGB to ensure 3 channels\n        # By default, set label to 0 to conform to current expected batch format\n        label = 0\n        if self.transform:\n            image = self.transform(image)\n        return image, label\n\n\n\ntransforms_list = [\n    transforms.ToTensor(),\n    # transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n]\n    \ntransform = transforms.Compose(transforms_list)\nfull_dataset = HuggingFaceDataset(dataset_name, transform=transform)\n\n\nprint(f\"dataset has {len(full_dataset)} images\")\nprint(f\"first image shape: {full_dataset[0][0].shape}\")\n\ndataset has 14630 images\nfirst image shape: torch.Size([3, 64, 64])\n\n\n\nfrom matplotlib import pyplot as plt\nplt.figure(figsize=(3, 3))\nfirst_image = full_dataset[0][0].permute(1, 2, 0).numpy()\nplt.imshow(first_image)\nplt.show()\n\n\n\n\n\n\n\n\n\n# normalize the image to [0, 1]\nimport numpy as np\nfrom PIL import Image\n\nfirst_image = (first_image - first_image.min()) / max(first_image.max() - first_image.min(), 1e-6)\npil_image = Image.fromarray((first_image * 255).astype(np.uint8))\npil_image.save(\"first_image.jpg\")"
  },
  {
    "objectID": "compare_vlms.html#qwen-vl",
    "href": "compare_vlms.html#qwen-vl",
    "title": "Compare VLMs",
    "section": "QWEN-VL",
    "text": "QWEN-VL\n\nfrom transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\nfrom qwen_vl_utils import process_vision_info\n\n# default: Load the model on the available device(s)\nmodel = Qwen2VLForConditionalGeneration.from_pretrained(\n    \"Qwen/Qwen2-VL-2B-Instruct-AWQ\", torch_dtype=\"auto\", device_map=\"auto\"\n)\n\n# We recommend enabling flash_attention_2 for better acceleration and memory saving, especially in multi-image and video scenarios.\n# model = Qwen2VLForConditionalGeneration.from_pretrained(\n#     \"Qwen/Qwen2-VL-2B-Instruct-AWQ\",\n#     torch_dtype=torch.bfloat16,\n#     attn_implementation=\"flash_attention_2\",\n#     device_map=\"auto\",\n# )\n\n# default processer\nprocessor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct-AWQ\")\n\n# The default range for the number of visual tokens per image in the model is 4-16384. You can set min_pixels and max_pixels according to your needs, such as a token count range of 256-1280, to balance speed and memory usage.\n# min_pixels = 256*28*28\n# max_pixels = 1280*28*28\n# processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct-AWQ\", min_pixels=min_pixels, max_pixels=max_pixels)\n\n\n2024-12-06 14:56:55.329856: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n2024-12-06 14:56:55.342432: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-12-06 14:56:55.357560: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-12-06 14:56:55.362094: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2024-12-06 14:56:55.373408: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2024-12-06 14:56:56.085600: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\nWe suggest you to set `torch_dtype=torch.float16` for better efficiency with AWQ.\n`Qwen2VLRotaryEmbedding` can now be fully parameterized by passing the model config through the `config` argument. All other arguments will be removed in v4.46\n\n\n\n# base64 encoded image using jpeg\nimport base64\nimport io\nfrom PIL import Image\nimport numpy as np\n\n# first write the image to a bytes object\nimage_bytes = io.BytesIO()\n\npil_image.save(image_bytes, format=\"JPEG\")\nimage_bytes = image_bytes.getvalue()\nprefix = \"data:image/jpeg;base64,\"\nimage_base64 = prefix + base64.b64encode(image_bytes).decode('utf-8')\n\n\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\n                \"type\": \"image\",\n                \"image\": image_base64,\n                # \"image\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg\",\n            },\n            {\"type\": \"text\", \"text\": \"Describe this image.\"},\n        ],\n    }\n]\n\n# Preprocess the inputs\n\n\n# Preparation for inference\ntext = processor.apply_chat_template(\n    messages, tokenize=False, add_generation_prompt=True\n)\nimage_inputs, video_inputs = process_vision_info(messages)\n\ninputs = processor(\n    text=[text],\n    images=image_inputs,\n    videos=video_inputs,\n    padding=True,\n    return_tensors=\"pt\",\n)\ninputs = inputs.to(\"cuda\")\n\n# Inference: Generation of the output\ngenerated_ids = model.generate(**inputs, max_new_tokens=128)\ngenerated_ids_trimmed = [\n    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n]\noutput_text = processor.batch_decode(\n    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n)\nprint(output_text)\n\n[\"The image depicts a cat with a long, flowing coat that appears to be a Maine Coon breed. The cat has a fluffy, dense fur that is predominantly white with some darker markings, particularly around the eyes and around the neck. The cat's eyes are large and expressive, and it has a long, curved tail that is slightly curled. The cat's ears are pointed and have a distinct, fluffy texture. The overall appearance of the cat suggests it is well-groomed and healthy.\"]"
  },
  {
    "objectID": "compare_vlms.html#cogvlm",
    "href": "compare_vlms.html#cogvlm",
    "title": "Compare VLMs",
    "section": "CogVLM",
    "text": "CogVLM\n\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nMODEL_PATH = \"THUDM/cogvlm2-llama3-chat-19B-int4\"\nDEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\nTORCH_TYPE = torch.bfloat16 if torch.cuda.is_available() and torch.cuda.get_device_capability()[\n    0] &gt;= 8 else torch.float16\n\ntokenizer = AutoTokenizer.from_pretrained(\n    MODEL_PATH,\n    trust_remote_code=True\n)\nmodel = AutoModelForCausalLM.from_pretrained(\n    MODEL_PATH,\n    torch_dtype=TORCH_TYPE,\n    trust_remote_code=True,\n    low_cpu_mem_usage=True,\n).eval()\n\ntext_only_template = \"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: {} ASSISTANT:\"\n\nquery = \"Describe this image.\"\nhistory = []\n\ninput_by_model = model.build_conversation_input_ids(\n                tokenizer,\n                query=query,\n                history=history,\n                images=[pil_image],\n                template_version='chat'\n            )\n\ninputs = {\n    'input_ids': input_by_model['input_ids'].unsqueeze(0).to(DEVICE),\n    'token_type_ids': input_by_model['token_type_ids'].unsqueeze(0).to(DEVICE),\n    'attention_mask': input_by_model['attention_mask'].unsqueeze(0).to(DEVICE),\n    'images': [[input_by_model['images'][0].to(DEVICE).to(TORCH_TYPE)]] if pil_image is not None else None,\n}\ngen_kwargs = {\n    \"max_new_tokens\": 128,\n    \"pad_token_id\": 128002,\n}\n\nwith torch.no_grad():\n    outputs = model.generate(**inputs, **gen_kwargs)\n    outputs = outputs[:, inputs['input_ids'].shape[1]:]\n    response = tokenizer.decode(outputs[0])\n    response = response.split(\"&lt;|end_of_text|&gt;\")[0]\n    print(\"\\nCogVLM2:\", response)\n\nUnused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in &lt;class 'transformers.utils.quantization_config.BitsAndBytesConfig'&gt;.\n\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[9], line 44\n     38 gen_kwargs = {\n     39     \"max_new_tokens\": 128,\n     40     \"pad_token_id\": 128002,\n     41 }\n     43 with torch.no_grad():\n---&gt; 44     outputs = model.generate(**inputs, **gen_kwargs)\n     45     outputs = outputs[:, inputs['input_ids'].shape[1]:]\n     46     response = tokenizer.decode(outputs[0])\n\nFile ~/.local/lib/python3.11/site-packages/torch/utils/_contextlib.py:116, in context_decorator.&lt;locals&gt;.decorate_context(*args, **kwargs)\n    113 @functools.wraps(func)\n    114 def decorate_context(*args, **kwargs):\n    115     with ctx_factory():\n--&gt; 116         return func(*args, **kwargs)\n\nFile ~/.local/lib/python3.11/site-packages/transformers/generation/utils.py:2252, in GenerationMixin.generate(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\n   2244     input_ids, model_kwargs = self._expand_inputs_for_generation(\n   2245         input_ids=input_ids,\n   2246         expand_size=generation_config.num_return_sequences,\n   2247         is_encoder_decoder=self.config.is_encoder_decoder,\n   2248         **model_kwargs,\n   2249     )\n   2251     # 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\n-&gt; 2252     result = self._sample(\n   2253         input_ids,\n   2254         logits_processor=prepared_logits_processor,\n   2255         stopping_criteria=prepared_stopping_criteria,\n   2256         generation_config=generation_config,\n   2257         synced_gpus=synced_gpus,\n   2258         streamer=streamer,\n   2259         **model_kwargs,\n   2260     )\n   2262 elif generation_mode in (GenerationMode.BEAM_SAMPLE, GenerationMode.BEAM_SEARCH):\n   2263     # 11. prepare beam search scorer\n   2264     beam_scorer = BeamSearchScorer(\n   2265         batch_size=batch_size,\n   2266         num_beams=generation_config.num_beams,\n   (...)\n   2271         max_length=generation_config.max_length,\n   2272     )\n\nFile ~/.local/lib/python3.11/site-packages/transformers/generation/utils.py:3257, in GenerationMixin._sample(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\n   3254     outputs = model_forward(**model_inputs, return_dict=True)\n   3256 # synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\n-&gt; 3257 model_kwargs = self._update_model_kwargs_for_generation(\n   3258     outputs,\n   3259     model_kwargs,\n   3260     is_encoder_decoder=self.config.is_encoder_decoder,\n   3261 )\n   3262 if synced_gpus and this_peer_finished:\n   3263     continue\n\nFile ~/.cache/huggingface/modules/transformers_modules/THUDM/cogvlm2-llama3-chat-19B-int4/119df232ab9fca4a1be87f95c239d7b9a765032e/modeling_cogvlm.py:710, in CogVLMForCausalLM._update_model_kwargs_for_generation(self, outputs, model_kwargs, is_encoder_decoder, standardize_cache_format)\n    702 def _update_model_kwargs_for_generation(\n    703         self,\n    704         outputs: \"ModelOutput\",\n   (...)\n    708 ) -&gt; Dict[str, Any]:\n    709     # update past_key_values\n--&gt; 710     model_kwargs[\"past_key_values\"] = self._extract_past_from_model_output(\n    711         outputs, standardize_cache_format=standardize_cache_format\n    712     )\n    713     if getattr(outputs, \"state\", None) is not None:\n    714         model_kwargs[\"state\"] = outputs.state\n\nTypeError: GenerationMixin._extract_past_from_model_output() got an unexpected keyword argument 'standardize_cache_format'"
  },
  {
    "objectID": "compare_vlms.html#try-blip-2",
    "href": "compare_vlms.html#try-blip-2",
    "title": "Compare VLMs",
    "section": "Try Blip-2",
    "text": "Try Blip-2"
  },
  {
    "objectID": "compare_vlms.html#try-minicpm-v-2",
    "href": "compare_vlms.html#try-minicpm-v-2",
    "title": "Compare VLMs",
    "section": "Try MiniCPM-V-2",
    "text": "Try MiniCPM-V-2\n\nmodel_name = 'openbmb/MiniCPM-V-2'\nmodel = AutoModel.from_pretrained(model_name, trust_remote_code=True,\n    attn_implementation='sdpa', torch_dtype=torch.bfloat16) # sdpa or flash_attention_2, no eager\n# For Nvidia GPUs support BF16 (like A100, H100, RTX3090)\nmodel = model.to(device='cuda', dtype=torch.bfloat16)\n# For Nvidia GPUs do NOT support BF16 (like V100, T4, RTX2080)\n#model = model.to(device='cuda', dtype=torch.float16)\n# For Mac with MPS (Apple silicon or AMD GPUs).\n# Run with `PYTORCH_ENABLE_MPS_FALLBACK=1 python test.py`\n#model = model.to(device='mps', dtype=torch.float16)\n\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n_ = model.eval()\n\n\n\n\n\nimport numpy as np\nfrom PIL import Image\n\nprint(first_image.shape, first_image.max() * 255)\n\npil_image = Image.fromarray((first_image * 255).astype(np.uint8))\n\nquestion = 'What is in the image?'\nmsgs = [{'role': 'user', 'content': question}]\n\nres, context, _ = model.chat(\n    image=pil_image,\n    msgs=msgs,\n    tokenizer=tokenizer,\n    context=None,\n    # sampling=True,\n    # temperature=0.7\n)\nprint(res)\n\n(64, 64, 3) 166.00000530481339\n\n\n\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\nCell In[19], line 11\n      8 question = 'What is in the image?'\n      9 msgs = [{'role': 'user', 'content': question}]\n---&gt; 11 res, context, _ = model.chat(\n     12     image=pil_image,\n     13     msgs=msgs,\n     14     tokenizer=tokenizer,\n     15     context=None,\n     16     # sampling=True,\n     17     # temperature=0.7\n     18 )\n     19 print(res)\n\nFile ~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1729, in Module.__getattr__(self, name)\n   1727     if name in modules:\n   1728         return modules[name]\n-&gt; 1729 raise AttributeError(f\"'{type(self).__name__}' object has no attribute '{name}'\")\n\nAttributeError: 'Qwen2VLForConditionalGeneration' object has no attribute 'chat'"
  }
]