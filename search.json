[
  {
    "objectID": "visual_story.html",
    "href": "visual_story.html",
    "title": "A Visual Story of Diffusion and Flow matching",
    "section": "",
    "text": "Diffusion and flow matching are techniques behind frontier generative models for image, video, music, protein structure and more.\nOn the one hand, developing such models seems daunting. They cost TBs of data, millions of GPU hours, and require a lot of expertise.\nOn the other hand, the core ideas of diffusion and flow matching are surprisingly simple. Better yet, the recipe stays quite the same, whether you want to generate simple 2D points, images, protein structures, whether you train on hundreds of examples or millions, whether you want to prompt it with text, reference images, key points etc. This means we can familiarize ourselves with the core ideas using toy examples and simple tasks, and scale up to real-world problems and even frontier models in a particular domain. This is why we wrote the tutorials.\nThe core ideas can be understood through a visual story, without diving into technical details.",
    "crumbs": [
      "Home",
      "A Visual Story of Diffusion and Flow matching"
    ]
  },
  {
    "objectID": "visual_story.html#good-data-gone-bad-the-teaching",
    "href": "visual_story.html#good-data-gone-bad-the-teaching",
    "title": "A Visual Story of Diffusion and Flow matching",
    "section": "1. Good data gone bad: the teaching",
    "text": "1. Good data gone bad: the teaching\nIf our task is to generate 2D points that follows a swiss roll pattern, the good data are the red points forming the swiss roll. The random (bad) data are blue points that can be easily sampled from a Gaussian distribution.\n\nNow let’s literally “connect the dots” to form a path from a good data point to a random data point. One way to do it is to use a straight line. We try to pair up red points with closest blue points, so that the lines do not cross each other a lot. This way, the data points do not need to travel a long distance to reach the target.\nAlong the way, we set up “milestones” as intermediate points. In the top-right section of the figure, the intermediate points are color coded to turn from blue to red. We want to teach a machine learning model to follow the path.\nWe need to teach the model which direction to go from where the data point is at. Our paths are straight lines, so along each path, the correct direction is simply pointing from the random point to the target point. In the bottom-left section of the figure, the arrows depict the correct directions for each data point when they just start the journey. That is, when they are at the random points. In the bottom-right section, the arrows depict the correct directions when they have traveled the path halfway.\n\nRe-formulate the problem\nBy showing the paths that good, clean data take to gradually turns into completely corrupted data, we break up a challenging task into small steps. To generate clean data is not necessarily a one step process. Instead, one can take a small step at a time. This incremental task is easier for the model to learn.\nIn other words, the original generative task has the following input and output:\nInput: prompt\n\nOutput: data (image, video, audio, etc.)\nWe re-formulate it into a somewhat easier problem for AI to learn:\nInput 1: prompt\nInput 2: corrupted data (with noise)\nInput 3: the time step (a.k.a. the noise level, indicating the proportion of noise still in the corrupted data)\n\nOutput: the added noise (or the direction from the corrupted data to clean data)\nWe prepare training examples like the above, and use it to train a denoiser model that can predict and remove the added noise.\nThis is the teaching.",
    "crumbs": [
      "Home",
      "A Visual Story of Diffusion and Flow matching"
    ]
  },
  {
    "objectID": "visual_story.html#they-find-their-way-back-the-learning",
    "href": "visual_story.html#they-find-their-way-back-the-learning",
    "title": "A Visual Story of Diffusion and Flow matching",
    "section": "2. They find their way back: the learning",
    "text": "2. They find their way back: the learning\nIf we teach well, the learning part becomes easier. Apply the standard supervised learning (or behavior copying), and you will cook some tasty models.\n\nWhat does the model learn?\nThe denoiser model learns to map out the direction for any time and location. Time means the current progress of the journey from 0% to 100%, and location means the current state of the data point. These directions over time and location forms a dynamic vector field that changes over “time”. We can visualize the dynamic vector field as a set of animated arrows.\nBefore learning, the vector field does not do much. In the following animation that barely moves, the tiny dots arranged in a grid are arrows that have not learned the directions. At time goes by from 0 to 100%, the random points are still almost at the same place.\n\n\n\n\n\n\n\n\n\nAfter learning, the effect is impressive. The vector field first seems to move all points towards the center, further away from the original swiss roll, but then the points enter the highway which guides them back to the original swiss roll.\n\n\n\n\n\n\n\n\n\n\n\nThe progress of learning\nAs the denoiser model learns, the dynamic vector field also evolves over training steps. To see the learning progress, let’s plot the trajectories of the data points as the model is trained with more and more steps.\n\n\n\n\n\nLearning progress\n\n\n\n\nIn early steps, the trajectories are straight lines. Then they start to curve. The model somehow finds that it is easier to first detour towards the center and then zoom back to the target. This is not what we taught the model – the student emerges with a better strategy.\nAdventure and mystery abound. To experience the full story, how about rolling up your sleeves and diving into the hands-on tutorials?",
    "crumbs": [
      "Home",
      "A Visual Story of Diffusion and Flow matching"
    ]
  },
  {
    "objectID": "skypilot.html",
    "href": "skypilot.html",
    "title": "Skypilot",
    "section": "",
    "text": "Install kind.\nInstall kubectl.\nInstall helm.\nInstall NVIDIA container toolkit.\n\nAnd configure containerd for kubenetes. sudo nvidia-ctk runtime configure --runtime=containerd sudo systemctl restart containerd.\n\nInstall NVIDIA visible devices as volume mounts.\n\n&gt; pip install skypilot[kubernetes]\n&gt; sky local up\n\nkubectl is not installed. Please install kubectl and try again. Installation instructions: https://kubernetes.io/docs/tasks/tools/install-kubectl-linux/\nNVIDIA is not set as the default runtime for Docker. To fix, run: sudo nvidia-ctk runtime configure –runtime=docker –set-as-default sudo systemctl restart docker\nNVIDIA visible devices are not set as volume mounts in container runtime. To fix, run: sudo sed -i ‘/accept-nvidia-visible-devices-as-volume-mounts/c-nvidia-visible-devices-as-volume-mounts = true’ /etc/nvidia-container-runtime/config.toml\nhelm is not installed. Please install helm and try again. Installation instructions: https://helm.sh/docs/intro/install/. Or simply:\n\ncurl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash"
  },
  {
    "objectID": "5_0_scale_up.html",
    "href": "5_0_scale_up.html",
    "title": "Scaling up",
    "section": "",
    "text": "In the next set of tutorials, we’ll be scaling up our training recipes for diffusion and flow matching. There are several dimensions of scaling:\n\nData dimension: for images this means increasing the resolution.\nModel size: the number of parameters in the denoising model.\nDataset size: the number of examples in the training set.\nCompute: the amount of compute (e.g. H100 hours) and the number of GPUs for parallel training.",
    "crumbs": [
      "Home",
      "Scaling up: generate bigger and more diverse images",
      "Scaling up"
    ]
  },
  {
    "objectID": "2_3.html",
    "href": "2_3.html",
    "title": "Compare GPUs",
    "section": "",
    "text": "As each training run goes from hours to days or weeks, the choice of GPU becomes more important. We want to get a high quality model. We want to have fast iteration speed: time is valuable. We want to keep all the training experiments under a budget.\nHere is a comparison training on two popular GPUs on Lambda Labs: A10 and H100."
  },
  {
    "objectID": "2_3.html#a10",
    "href": "2_3.html#a10",
    "title": "Compare GPUs",
    "section": "A10",
    "text": "A10"
  },
  {
    "objectID": "2_3.html#h100",
    "href": "2_3.html#h100",
    "title": "Compare GPUs",
    "section": "H100",
    "text": "H100"
  },
  {
    "objectID": "inception_embedding_and_tsne.html",
    "href": "inception_embedding_and_tsne.html",
    "title": "Library code for trajectory visualization",
    "section": "",
    "text": "from cleanfid.inception_pytorch import fid_inception_v3\n\ninception_v3 = fid_inception_v3()\n_ = inception_v3.eval()\n\n/home/ubuntu/.local/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/home/ubuntu/.local/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n  warnings.warn(msg)\nfrom typing import Tuple\n\n\nclass TrajectorySet:\n    def __init__(self, embeddings):\n        \"\"\"\n        Managing a set of trajectories, each of which is a sequence of embeddings.\n\n        Parameters\n        ----------\n        embeddings: (n_timesteps, n_samples, *embedding_dims). This assumes\n            the first dimension is time. And it is ordered from t=0 to t=n_timesteps-1.\n            With t=0 representing the clean data and t=n_timesteps-1 representing the noise.\n\n        \"\"\"\n        self.embeddings = embeddings\n        self.embeddings_2d = None\n    \n    def run_tsne(self, n_components: int = 2, seed: int = 0, **kwargs):\n        \"\"\"Run t-SNE on the embeddings.\n        \"\"\"\n        print(f\"Running t-SNE on {self.embeddings.shape} embeddings...\")\n        from sklearn.manifold import TSNE\n        tsne = TSNE(n_components=n_components, random_state=seed, **kwargs)\n        flattened_embeddings = self.embeddings.reshape(-1, self.embeddings.shape[-1])\n        flattened_embeddings_2d = tsne.fit_transform(flattened_embeddings)\n        self.embeddings_2d = flattened_embeddings_2d.reshape(self.embeddings.shape[0], self.embeddings.shape[1], -1)\n        print(f\"t-SNE done. Shape of 2D embeddings: {self.embeddings_2d.shape}\")\n        return self.embeddings_2d\n    \n    def plot_trajectories(\n            self,\n            n: int = 10,\n            show_figure: bool = False,\n            noise_color: Tuple[float, float, float] = (0, 0, 1),  # blue\n            data_color: Tuple[float, float, float] = (1, 0, 0),  # red\n            figsize: tuple = (6, 6),\n            with_ticks: bool = False,\n            tsne_seed: int = 0,\n            **kwargs):\n        \"\"\"Plot trajectories of some selected samples.\n\n        This assumes the first dimension is time. And it is ordered from t=0 to t=n_timesteps-1.\n        With t=0 representing the clean data and t=n_timesteps-1 representing the noise.\n\n        Parameters\n        ----------\n        n: int\n            number of samples to plot\n        figsize: tuple\n            figure size\n        kwargs:\n            other keyword arguments for matplotlib.pyplot.scatter\n        \"\"\"\n        import numpy as np\n        import matplotlib.pyplot as plt\n\n        colors = []\n        for t in range(self.embeddings.shape[0]):\n            # interpolate between noise_color and data_color\n            factor = t / (self.embeddings.shape[0] - 1)\n            colors.append(np.array(noise_color) * factor + np.array(data_color) * (1 - factor))\n        colors = np.array(colors)\n        \n        if self.embeddings_2d is None:\n            if self.embeddings.shape[2] == 2:\n                self.embeddings_2d = self.embeddings\n            else:\n                self.embeddings_2d = self.run_tsne(seed=tsne_seed)\n\n        traj = self.embeddings_2d[:, :n, :]\n        plt.figure(figsize=figsize)\n        plt.scatter(traj[0, :n, 0], traj[0, :n, 1], s=10, alpha=0.8, c=\"red\")  # real\n        plt.scatter(traj[-1, :n, 0], traj[-1, :n, 1], s=4, alpha=1, c=\"blue\")  # noise\n        plt.scatter(traj[:, :n, 0], traj[:, :n, 1], s=0.5, alpha=0.7, c=colors.repeat(n, axis=0))  # \"olive\"\n        plt.plot(traj[:, :n, 0], traj[:, :n, 1], c=\"olive\", alpha=0.3)\n        plt.legend([\"Data\", \"Noise\", \"Intermediate Samples (color coded)\", \"Flow trajectory\"])\n        if not with_ticks:\n            plt.xticks([])\n            plt.yticks([])\n        if show_figure:\n            plt.show()\n        else:\n            plt.close()\n        \n        # return the figure\n        return plt.gcf()"
  },
  {
    "objectID": "inception_embedding_and_tsne.html#library-code-for-dataset-loading",
    "href": "inception_embedding_and_tsne.html#library-code-for-dataset-loading",
    "title": "Library code for trajectory visualization",
    "section": "Library code for dataset loading",
    "text": "Library code for dataset loading\n\nfrom torch.utils.data import Dataset\nimport torchvision.transforms as transforms\nfrom datasets import load_dataset\n\n\nclass HuggingFaceDataset(Dataset):\n    def __init__(self, dataset_path: str, transform=None):\n        self.dataset = load_dataset(dataset_path, split=\"train\")\n        self.transform = transform or self.default_transform\n        self.image_key = self.find_image_key()\n    \n    @property\n    def default_transform(self):\n        # ToTensor()\n        return transforms.ToTensor()\n\n    def find_image_key(self) -&gt; str:\n        # Check if the dataset has the \"image\" key\n        # NOTE: Can exapnd this to other common keys if needed\n        if \"image\" in self.dataset[0].keys():\n            return \"image\"\n        raise KeyError(\"Dataset does not have an 'image' key\")\n\n    def __len__(self):\n        return len(self.dataset)\n\n    def __getitem__(self, idx):\n        image = self.dataset[idx][self.image_key]\n        image = image.convert(\"RGB\")  # Convert to RGB to ensure 3 channels\n        # By default, set label to 0 to conform to current expected batch format\n        label = 0\n        if self.transform:\n            image = self.transform(image)\n        return image, label\n\n\nfrom torch.utils.data import DataLoader\nimport torch\nimport numpy as np\n\nbatch_size = 32\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ntransforms_list = [\n    transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n]\ntransform = transforms.Compose(transforms_list)\ninception_v3 = inception_v3.to(device)\nafhq_dataset = HuggingFaceDataset(\"zzsi/afhq64_16k\", transform=transform)\ndataloader = DataLoader(\n    afhq_dataset, batch_size=batch_size, shuffle=False, num_workers=2\n)\n\nreal_embeddings = []\nimages = next(iter(dataloader))[0]\nreal_images = images.clone()\nimages = images.to(device)\nresize = transforms.Resize((224, 224))\nwith torch.no_grad():\n    images = resize(images)\n    features = inception_v3(images)\nreal_embeddings.append(features.cpu().numpy())\n\nreal_embeddings = np.concatenate(real_embeddings, axis=0)\n\n\nnp.random.seed(0)\nnoise_images = torch.randn_like(images)\nnoise_images = noise_images.to(device)\nwith torch.no_grad():\n    noise_images = resize(noise_images)\n    features = inception_v3(noise_images)\nnoise_embeddings = features.cpu().numpy()\n\n\nreal_embeddings.shape, noise_embeddings.shape\n\n((32, 1008), (32, 1008))\n\n\n\n# t-sne\nfrom sklearn.manifold import TSNE\n\ntsne = TSNE(n_components=2, random_state=0)\nreal_and_noise_embeddings = np.concatenate([real_embeddings, noise_embeddings], axis=0)\nembeddings_2d = tsne.fit_transform(real_and_noise_embeddings)\n\n\n\n# visualize\nimport matplotlib.pyplot as plt\n\nplt.scatter(embeddings_2d[:real_embeddings.shape[0], 0], embeddings_2d[:real_embeddings.shape[0], 1], c=\"red\", label=\"real\")\nplt.scatter(embeddings_2d[real_embeddings.shape[0]:, 0], embeddings_2d[real_embeddings.shape[0]:, 1], c=\"blue\", label=\"noise\")\nplt.legend(loc=\"upper right\")\nplt.xlim(-7, 5)\nplt.ylim(-5, 5)\nplt.title(\"t-SNE of Real and Noise Embeddings\")\nplt.show()"
  },
  {
    "objectID": "inception_embedding_and_tsne.html#forward-diffusion",
    "href": "inception_embedding_and_tsne.html#forward-diffusion",
    "title": "Library code for trajectory visualization",
    "section": "Forward diffusion",
    "text": "Forward diffusion\n\ndef forward_diffusion(x_0, t, noise_schedule, noise=None):\n    _ts = t.view(-1, 1, 1, 1)\n    if noise is None:\n        noise = torch.randn_like(x_0)\n    assert _ts.max() &lt; len(noise_schedule[\"alphas_cumprod\"]), f\"t={_ts.max()} is larger than the length of noise_schedule: {len(noise_schedule['alphas_cumprod'])}\"\n    alpha_prod_t = noise_schedule[\"alphas_cumprod\"][_ts]\n    x_t = (alpha_prod_t ** 0.5) * x_0 + ((1 - alpha_prod_t) ** 0.5) * noise\n    return x_t, noise\n\n\nfrom typing import Dict\nbeta_min, beta_max = 1e-4, 0.02\n# beta_min, beta_max = 1e-4, 1\n# beta_min, beta_max = 0, 0.02\n\ndef create_noise_schedule(n_T: int, device: torch.device) -&gt; Dict[str, torch.Tensor]:\n    betas = torch.linspace(beta_min, beta_max, n_T).to(device)\n    alphas = 1. - betas\n    alphas_cumprod = torch.cumprod(alphas, axis=0).to(device)\n    alphas_cumprod_prev = torch.cat([torch.ones(1).to(device), alphas_cumprod[:-1].to(device)])\n    sqrt_recip_alphas = torch.sqrt(1.0 / alphas).to(device)\n    sqrt_alphas_cumprod = torch.sqrt(alphas_cumprod).to(device)\n    sqrt_one_minus_alphas_cumprod = torch.sqrt(1. - alphas_cumprod)\n    posterior_variance = betas * (1. - alphas_cumprod_prev) / (1. - alphas_cumprod)\n\n    return {\n        \"betas\": betas,\n        \"alphas\": alphas,\n        \"alphas_cumprod\": alphas_cumprod,\n        \"sqrt_recip_alphas\": sqrt_recip_alphas,\n        \"sqrt_alphas_cumprod\": sqrt_alphas_cumprod,\n        \"sqrt_one_minus_alphas_cumprod\": sqrt_one_minus_alphas_cumprod,\n        \"posterior_variance\": posterior_variance,\n    }\n\nnoise_schedule = create_noise_schedule(1000, \"cpu\")\n\n\nnoised_images = {}\ncolors = {}\n# for t in [0, 50, 100, 200, 300, 500, 800]:\ntorch.manual_seed(0)\ncommon_noise = torch.randn_like(real_images)\nfor t in range(0, 1000, 10):\n    noised_images[t] = forward_diffusion(real_images.cpu(), torch.tensor([t]), noise_schedule, common_noise)[0]\n    alpha_accum = noise_schedule[\"alphas_cumprod\"][t]\n    # interpolate between red (1,0,0) and blue (0,0,1)\n    #   small alpha_accum is blue, large is red\n    colors[t] = (alpha_accum, 0, 1 - alpha_accum)\n\nnoised_images[0][0].shape\n\ntorch.Size([3, 64, 64])\n\n\n\nnoised_embeddings = {}\nfor t in noised_images:\n    with torch.no_grad():\n        noised_images[t] = resize(noised_images[t])\n        features = inception_v3(noised_images[t].to(device))\n    noised_embeddings[t] = features.cpu().numpy()\n\n\nall_embeddings = np.concatenate([real_embeddings, *noised_embeddings.values()], axis=0)\nprint(f\"all_embeddings.shape: {all_embeddings.shape}\")\nembeddings_2d = tsne.fit_transform(all_embeddings)\nreal_embeddings_2d = embeddings_2d[:real_embeddings.shape[0]]\n# put them back to the original order\nnoised_embeddings_2d = {}\noffset = real_embeddings.shape[0]\nfor t in noised_embeddings:\n    noised_embeddings_2d[t] = embeddings_2d[offset:offset+len(noised_embeddings[t])]\n    offset += len(noised_embeddings[t])\n\n# plot the real embeddings\n# plt.scatter(real_embeddings_2d[:, 0], real_embeddings_2d[:, 1], c=\"red\", label=\"real\")\n# show color gradient from blue to red\ntrajs = []\nfor t in noised_embeddings_2d:\n    noised_embeddings_this_t_2d = noised_embeddings_2d[t]\n    trajs.append(noised_embeddings_this_t_2d)\ntrajs = np.array(trajs)\ncolor_for_trajs = []\nfor t in noised_embeddings_2d:\n    color_for_trajs.append(colors[t])\ncolor_for_trajs = np.array(color_for_trajs)\nprint(trajs.shape)\n\nall_embeddings.shape: (3232, 1008)\n(100, 32, 2)\n\n\n\n_ = TrajectorySet(trajs).plot_trajectories(n=8, show_figure=True)\n\n\n\n\n\n\n\n\n&lt;Figure size 640x480 with 0 Axes&gt;"
  },
  {
    "objectID": "inception_embedding_and_tsne.html#what-about-flow-matching",
    "href": "inception_embedding_and_tsne.html#what-about-flow-matching",
    "title": "Library code for trajectory visualization",
    "section": "What about flow matching?",
    "text": "What about flow matching?\n\nFlow matching utils\n\nfrom typing import Union\n\n\ndef pad_t_like_x(t, x):\n    \"\"\"Function to reshape the time vector t by the number of dimensions of x.\n\n    Parameters\n    ----------\n    x : Tensor, shape (bs, *dim)\n        represents the source minibatch\n    t : FloatTensor, shape (bs)\n\n    Returns\n    -------\n    t : Tensor, shape (bs, number of x dimensions)\n\n    Example\n    -------\n    x: Tensor (bs, C, W, H)\n    t: Vector (bs)\n    pad_t_like_x(t, x): Tensor (bs, 1, 1, 1)\n    \"\"\"\n    if isinstance(t, (float, int)):\n        return t\n    return t.reshape(-1, *([1] * (x.dim() - 1)))\n\nclass ConditionalFlowMatcher:\n    \"\"\"Base class for conditional flow matching methods. This class implements the independent\n    conditional flow matching methods from [1] and serves as a parent class for all other flow\n    matching methods.\n\n    It implements:\n    - Drawing data from gaussian probability path N(t * x1 + (1 - t) * x0, sigma) function\n    - conditional flow matching ut(x1|x0) = x1 - x0\n    - score function $\\nabla log p_t(x|x0, x1)$\n    \"\"\"\n\n    def __init__(self, sigma: Union[float, int] = 0.0):\n        r\"\"\"Initialize the ConditionalFlowMatcher class. It requires the hyper-parameter $\\sigma$.\n\n        Parameters\n        ----------\n        sigma : Union[float, int]\n        \"\"\"\n        self.sigma = sigma\n\n    def compute_mu_t(self, x0, x1, t):\n        \"\"\"\n        Compute the mean of the probability path N(t * x1 + (1 - t) * x0, sigma), see (Eq.14) [1].\n\n        Parameters\n        ----------\n        x0 : Tensor, shape (bs, *dim)\n            represents the source minibatch\n        x1 : Tensor, shape (bs, *dim)\n            represents the target minibatch\n        t : FloatTensor, shape (bs)\n\n        Returns\n        -------\n        mean mu_t: t * x1 + (1 - t) * x0\n\n        References\n        ----------\n        [1] Improving and Generalizing Flow-Based Generative Models with minibatch optimal transport, Preprint, Tong et al.\n        \"\"\"\n        t = pad_t_like_x(t, x0)\n        return t * x1 + (1 - t) * x0\n\n    def compute_sigma_t(self, t):\n        \"\"\"\n        Compute the standard deviation of the probability path N(t * x1 + (1 - t) * x0, sigma), see (Eq.14) [1].\n\n        Parameters\n        ----------\n        t : FloatTensor, shape (bs)\n\n        Returns\n        -------\n        standard deviation sigma\n\n        References\n        ----------\n        [1] Improving and Generalizing Flow-Based Generative Models with minibatch optimal transport, Preprint, Tong et al.\n        \"\"\"\n        del t\n        return self.sigma\n\n    def sample_xt(self, x0, x1, t, epsilon):\n        \"\"\"\n        Draw a sample from the probability path N(t * x1 + (1 - t) * x0, sigma), see (Eq.14) [1].\n\n        Parameters\n        ----------\n        x0 : Tensor, shape (bs, *dim)\n            represents the source minibatch\n        x1 : Tensor, shape (bs, *dim)\n            represents the target minibatch\n        t : FloatTensor, shape (bs)\n        epsilon : Tensor, shape (bs, *dim)\n            noise sample from N(0, 1)\n\n        Returns\n        -------\n        xt : Tensor, shape (bs, *dim)\n\n        References\n        ----------\n        [1] Improving and Generalizing Flow-Based Generative Models with minibatch optimal transport, Preprint, Tong et al.\n        \"\"\"\n        mu_t = self.compute_mu_t(x0, x1, t)\n        sigma_t = self.compute_sigma_t(t)\n        sigma_t = pad_t_like_x(sigma_t, x0)\n        return mu_t + sigma_t * epsilon\n\n    def compute_conditional_flow(self, x0, x1, t, xt):\n        \"\"\"\n        Compute the conditional vector field ut(x1|x0) = x1 - x0, see Eq.(15) [1].\n\n        Parameters\n        ----------\n        x0 : Tensor, shape (bs, *dim)\n            represents the source minibatch\n        x1 : Tensor, shape (bs, *dim)\n            represents the target minibatch\n        t : FloatTensor, shape (bs)\n        xt : Tensor, shape (bs, *dim)\n            represents the samples drawn from probability path pt\n\n        Returns\n        -------\n        ut : conditional vector field ut(x1|x0) = x1 - x0\n\n        References\n        ----------\n        [1] Improving and Generalizing Flow-Based Generative Models with minibatch optimal transport, Preprint, Tong et al.\n        \"\"\"\n        del t, xt\n        return x1 - x0\n\n    def sample_noise_like(self, x):\n        return torch.randn_like(x)\n\n    def sample_location_and_conditional_flow(self, x0, x1, t=None, return_noise=False):\n        \"\"\"\n        Compute the sample xt (drawn from N(t * x1 + (1 - t) * x0, sigma))\n        and the conditional vector field ut(x1|x0) = x1 - x0, see Eq.(15) [1].\n\n        Parameters\n        ----------\n        x0 : Tensor, shape (bs, *dim)\n            represents the source minibatch\n        x1 : Tensor, shape (bs, *dim)\n            represents the target minibatch\n        (optionally) t : Tensor, shape (bs)\n            represents the time levels\n            if None, drawn from uniform [0,1]\n        return_noise : bool\n            return the noise sample epsilon\n\n\n        Returns\n        -------\n        t : FloatTensor, shape (bs)\n        xt : Tensor, shape (bs, *dim)\n            represents the samples drawn from probability path pt\n        ut : conditional vector field ut(x1|x0) = x1 - x0\n        (optionally) eps: Tensor, shape (bs, *dim) such that xt = mu_t + sigma_t * epsilon\n\n        References\n        ----------\n        [1] Improving and Generalizing Flow-Based Generative Models with minibatch optimal transport, Preprint, Tong et al.\n        \"\"\"\n        if t is None:\n            t = torch.rand(x0.shape[0]).type_as(x0)\n        assert len(t) == x0.shape[0], f\"t has to have batch size dimension, got {len(t)}\"\n\n        eps = self.sample_noise_like(x0)\n        xt = self.sample_xt(x0, x1, t, eps)\n        ut = self.compute_conditional_flow(x0, x1, t, xt)\n        if return_noise:\n            return t, xt, ut, eps\n        else:\n            return t, xt, ut\n\n    def compute_lambda(self, t):\n        \"\"\"Compute the lambda function, see Eq.(23) [3].\n\n        Parameters\n        ----------\n        t : FloatTensor, shape (bs)\n\n        Returns\n        -------\n        lambda : score weighting function\n\n        References\n        ----------\n        [4] Simulation-free Schrodinger bridges via score and flow matching, Preprint, Tong et al.\n        \"\"\"\n        sigma_t = self.compute_sigma_t(t)\n        return 2 * sigma_t / (self.sigma**2 + 1e-8)\n\n\nimport numpy as np\nimport ot as pot\nimport warnings\nfrom functools import partial\n\nclass OTPlanSampler:\n    \"\"\"OTPlanSampler implements sampling coordinates according to an OT plan (wrt squared Euclidean\n    cost) with different implementations of the plan calculation.\"\"\"\n\n    def __init__(\n        self,\n        method: str,\n        reg: float = 0.05,\n        reg_m: float = 1.0,\n        normalize_cost: bool = False,\n        num_threads: Union[int, str] = 1,\n        warn: bool = True,\n    ) -&gt; None:\n        \"\"\"Initialize the OTPlanSampler class.\n\n        Parameters\n        ----------\n        method: str\n            choose which optimal transport solver you would like to use.\n            Currently supported are [\"exact\", \"sinkhorn\", \"unbalanced\",\n            \"partial\"] OT solvers.\n        reg: float, optional\n            regularization parameter to use for Sinkhorn-based iterative solvers.\n        reg_m: float, optional\n            regularization weight for unbalanced Sinkhorn-knopp solver.\n        normalize_cost: bool, optional\n            normalizes the cost matrix so that the maximum cost is 1. Helps\n            stabilize Sinkhorn-based solvers. Should not be used in the vast\n            majority of cases.\n        num_threads: int or str, optional\n            number of threads to use for the \"exact\" OT solver. If \"max\", uses\n            the maximum number of threads.\n        warn: bool, optional\n            if True, raises a warning if the algorithm does not converge\n        \"\"\"\n        # ot_fn should take (a, b, M) as arguments where a, b are marginals and\n        # M is a cost matrix\n        if method == \"exact\":\n            self.ot_fn = partial(pot.emd, numThreads=num_threads)\n        elif method == \"sinkhorn\":\n            self.ot_fn = partial(pot.sinkhorn, reg=reg)\n        elif method == \"unbalanced\":\n            self.ot_fn = partial(pot.unbalanced.sinkhorn_knopp_unbalanced, reg=reg, reg_m=reg_m)\n        elif method == \"partial\":\n            self.ot_fn = partial(pot.partial.entropic_partial_wasserstein, reg=reg)\n        else:\n            raise ValueError(f\"Unknown method: {method}\")\n        self.reg = reg\n        self.reg_m = reg_m\n        self.normalize_cost = normalize_cost\n        self.warn = warn\n\n    def get_map(self, x0, x1):\n        \"\"\"Compute the OT plan (wrt squared Euclidean cost) between a source and a target\n        minibatch.\n\n        Parameters\n        ----------\n        x0 : Tensor, shape (bs, *dim)\n            represents the source minibatch\n        x1 : Tensor, shape (bs, *dim)\n            represents the source minibatch\n\n        Returns\n        -------\n        p : numpy array, shape (bs, bs)\n            represents the OT plan between minibatches\n        \"\"\"\n        a, b = pot.unif(x0.shape[0]), pot.unif(x1.shape[0])\n        if x0.dim() &gt; 2:\n            x0 = x0.reshape(x0.shape[0], -1)\n        if x1.dim() &gt; 2:\n            x1 = x1.reshape(x1.shape[0], -1)\n        x1 = x1.reshape(x1.shape[0], -1)\n        M = torch.cdist(x0, x1) ** 2\n        if self.normalize_cost:\n            M = M / M.max()  # should not be normalized when using minibatches\n        p = self.ot_fn(a, b, M.detach().cpu().numpy())\n        if not np.all(np.isfinite(p)):\n            print(\"ERROR: p is not finite\")\n            print(p)\n            print(\"Cost mean, max\", M.mean(), M.max())\n            print(x0, x1)\n        if np.abs(p.sum()) &lt; 1e-8:\n            if self.warn:\n                warnings.warn(\"Numerical errors in OT plan, reverting to uniform plan.\")\n            p = np.ones_like(p) / p.size\n        return p\n\n    def sample_map(self, pi, batch_size, replace=True):\n        r\"\"\"Draw source and target samples from pi  $(x,z) \\sim \\pi$\n\n        Parameters\n        ----------\n        pi : numpy array, shape (bs, bs)\n            represents the source minibatch\n        batch_size : int\n            represents the OT plan between minibatches\n        replace : bool\n            represents sampling or without replacement from the OT plan\n\n        Returns\n        -------\n        (i_s, i_j) : tuple of numpy arrays, shape (bs, bs)\n            represents the indices of source and target data samples from $\\pi$\n        \"\"\"\n        p = pi.flatten()\n        p = p / p.sum()\n        choices = np.random.choice(\n            pi.shape[0] * pi.shape[1], p=p, size=batch_size, replace=replace\n        )\n        return np.divmod(choices, pi.shape[1])\n\n    def sample_plan(self, x0, x1, replace=True):\n        r\"\"\"Compute the OT plan $\\pi$ (wrt squared Euclidean cost) between a source and a target\n        minibatch and draw source and target samples from pi $(x,z) \\sim \\pi$\n\n        Parameters\n        ----------\n        x0 : Tensor, shape (bs, *dim)\n            represents the source minibatch\n        x1 : Tensor, shape (bs, *dim)\n            represents the source minibatch\n        replace : bool\n            represents sampling or without replacement from the OT plan\n\n        Returns\n        -------\n        x0[i] : Tensor, shape (bs, *dim)\n            represents the source minibatch drawn from $\\pi$\n        x1[j] : Tensor, shape (bs, *dim)\n            represents the source minibatch drawn from $\\pi$\n        \"\"\"\n        pi = self.get_map(x0, x1)\n        i, j = self.sample_map(pi, x0.shape[0], replace=replace)\n        return x0[i], x1[j]\n\n    def sample_plan_with_labels(self, x0, x1, y0=None, y1=None, replace=True):\n        r\"\"\"Compute the OT plan $\\pi$ (wrt squared Euclidean cost) between a source and a target\n        minibatch and draw source and target labeled samples from pi $(x,z) \\sim \\pi$\n\n        Parameters\n        ----------\n        x0 : Tensor, shape (bs, *dim)\n            represents the source minibatch\n        x1 : Tensor, shape (bs, *dim)\n            represents the target minibatch\n        y0 : Tensor, shape (bs)\n            represents the source label minibatch\n        y1 : Tensor, shape (bs)\n            represents the target label minibatch\n        replace : bool\n            represents sampling or without replacement from the OT plan\n\n        Returns\n        -------\n        x0[i] : Tensor, shape (bs, *dim)\n            represents the source minibatch drawn from $\\pi$\n        x1[j] : Tensor, shape (bs, *dim)\n            represents the target minibatch drawn from $\\pi$\n        y0[i] : Tensor, shape (bs, *dim)\n            represents the source label minibatch drawn from $\\pi$\n        y1[j] : Tensor, shape (bs, *dim)\n            represents the target label minibatch drawn from $\\pi$\n        \"\"\"\n        pi = self.get_map(x0, x1)\n        i, j = self.sample_map(pi, x0.shape[0], replace=replace)\n        return (\n            x0[i],\n            x1[j],\n            y0[i] if y0 is not None else None,\n            y1[j] if y1 is not None else None,\n        )\n\n    def sample_trajectory(self, X):\n        \"\"\"Compute the OT trajectories between different sample populations moving from the source\n        to the target distribution.\n\n        Parameters\n        ----------\n        X : Tensor, (bs, times, *dim)\n            different populations of samples moving from the source to the target distribution.\n\n        Returns\n        -------\n        to_return : Tensor, (bs, times, *dim)\n            represents the OT sampled trajectories over time.\n        \"\"\"\n        times = X.shape[1]\n        pis = []\n        for t in range(times - 1):\n            pis.append(self.get_map(X[:, t], X[:, t + 1]))\n\n        indices = [np.arange(X.shape[0])]\n        for pi in pis:\n            j = []\n            for i in indices[-1]:\n                j.append(np.random.choice(pi.shape[1], p=pi[i] / pi[i].sum()))\n            indices.append(np.array(j))\n\n        to_return = []\n        for t in range(times):\n            to_return.append(X[:, t][indices[t]])\n        to_return = np.stack(to_return, axis=1)\n        return to_return\n\n\nclass ExactOptimalTransportConditionalFlowMatcher(ConditionalFlowMatcher):\n    \"\"\"Child class for optimal transport conditional flow matching method. This class implements\n    the OT-CFM methods from [1] and inherits the ConditionalFlowMatcher parent class.\n\n    It overrides the sample_location_and_conditional_flow.\n    \"\"\"\n\n    def __init__(self, sigma: Union[float, int] = 0.0):\n        r\"\"\"Initialize the ConditionalFlowMatcher class. It requires the hyper-parameter $\\sigma$.\n\n        Parameters\n        ----------\n        sigma : Union[float, int]\n        ot_sampler: exact OT method to draw couplings (x0, x1) (see Eq.(17) [1]).\n        \"\"\"\n        super().__init__(sigma)\n        self.ot_sampler = OTPlanSampler(method=\"exact\")\n\n    def sample_location_and_conditional_flow(self, x0, x1, t=None, return_noise=False):\n        r\"\"\"\n        Compute the sample xt (drawn from N(t * x1 + (1 - t) * x0, sigma))\n        and the conditional vector field ut(x1|x0) = x1 - x0, see Eq.(15) [1]\n        with respect to the minibatch OT plan $\\Pi$.\n\n        Parameters\n        ----------\n        x0 : Tensor, shape (bs, *dim)\n            represents the source minibatch\n        x1 : Tensor, shape (bs, *dim)\n            represents the target minibatch\n        (optionally) t : Tensor, shape (bs)\n            represents the time levels\n            if None, drawn from uniform [0,1]\n        return_noise : bool\n            return the noise sample epsilon\n\n        Returns\n        -------\n        t : FloatTensor, shape (bs)\n        xt : Tensor, shape (bs, *dim)\n            represents the samples drawn from probability path pt\n        ut : conditional vector field ut(x1|x0) = x1 - x0\n        (optionally) epsilon : Tensor, shape (bs, *dim) such that xt = mu_t + sigma_t * epsilon\n\n        References\n        ----------\n        [1] Improving and Generalizing Flow-Based Generative Models with minibatch optimal transport, Preprint, Tong et al.\n        \"\"\"\n        x0, x1 = self.ot_sampler.sample_plan(x0, x1)\n        return super().sample_location_and_conditional_flow(x0, x1, t, return_noise)\n\n    def guided_sample_location_and_conditional_flow(\n        self, x0, x1, y0=None, y1=None, t=None, return_noise=False\n    ):\n        r\"\"\"\n        Compute the sample xt (drawn from N(t * x1 + (1 - t) * x0, sigma))\n        and the conditional vector field ut(x1|x0) = x1 - x0, see Eq.(15) [1]\n        with respect to the minibatch OT plan $\\Pi$.\n\n        Parameters\n        ----------\n        x0 : Tensor, shape (bs, *dim)\n            represents the source minibatch\n        x1 : Tensor, shape (bs, *dim)\n            represents the target minibatch\n        y0 : Tensor, shape (bs) (default: None)\n            represents the source label minibatch\n        y1 : Tensor, shape (bs) (default: None)\n            represents the target label minibatch\n        (optionally) t : Tensor, shape (bs)\n            represents the time levels\n            if None, drawn from uniform [0,1]\n        return_noise : bool\n            return the noise sample epsilon\n\n        Returns\n        -------\n        t : FloatTensor, shape (bs)\n        xt : Tensor, shape (bs, *dim)\n            represents the samples drawn from probability path pt\n        ut : conditional vector field ut(x1|x0) = x1 - x0\n        (optionally) epsilon : Tensor, shape (bs, *dim) such that xt = mu_t + sigma_t * epsilon\n\n        References\n        ----------\n        [1] Improving and Generalizing Flow-Based Generative Models with minibatch optimal transport, Preprint, Tong et al.\n        \"\"\"\n        x0, x1, y0, y1 = self.ot_sampler.sample_plan_with_labels(x0, x1, y0, y1)\n        if return_noise:\n            t, xt, ut, eps = super().sample_location_and_conditional_flow(x0, x1, t, return_noise)\n            return t, xt, ut, y0, y1, eps\n        else:\n            t, xt, ut = super().sample_location_and_conditional_flow(x0, x1, t, return_noise)\n            return t, xt, ut, y0, y1\n\n2024-12-02 23:00:43.531809: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n2024-12-02 23:00:43.543894: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-12-02 23:00:43.556582: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-12-02 23:00:43.560354: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2024-12-02 23:00:43.569758: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2024-12-02 23:00:44.333698: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n\n\n\n\nVisualize flow matching plan\n\nFM = ConditionalFlowMatcher(sigma=0)\n\n\nfrom tqdm import tqdm\n\n\ndef generate_trajectories(FM, num_timesteps=10, n=32, seed=0):\n    x_clean = real_images[:n]\n\n    # noisy data\n    torch.manual_seed(seed)\n    x_noisy = torch.randn_like(x_clean)\n    # Sample multiple timesteps to visualize flow progression\n    t_grid = torch.linspace(0, 1, num_timesteps)\n    x_ts = []\n    # u_ts = []\n\n    for i, t in enumerate(t_grid):\n        t_batch = t.repeat(x_clean.shape[0])\n        np.random.seed(seed)\n        t, x_t, u_t = FM.sample_location_and_conditional_flow(x0=x_clean, x1=x_noisy, t=t_batch)\n        x_ts.append(x_t.detach().cpu().numpy())\n        if i == 0:\n            print(\"x_t images mean (t=0):\", x_t.mean(), \"x_clean images mean:\", x_clean.mean())\n        if i == len(t_grid) - 1:\n            print(\"x_t images mean (t=1):\", x_t.mean(), \"x_noisy images mean:\", x_noisy.mean())\n        # u_ts.append(u_t.detach().cpu().numpy())\n    \n    # First, extract the embeddings\n    print(\"Extracting embeddings...\")\n    x_ts_embeddings = []\n    resize = transforms.Resize((224, 224))\n\n    def to_tensor(x):\n        return torch.from_numpy(x)\n    \n    inception_v3.eval()\n    with torch.no_grad():\n        for x_t in tqdm(x_ts):\n            x_ts_embeddings.append(inception_v3(resize(to_tensor(x_t).to(device))).cpu().numpy())\n\n    all_x_embeddings = np.array(x_ts_embeddings)\n    traj_set = TrajectorySet(all_x_embeddings)\n    return traj_set\n\n\ntraj_set = generate_trajectories(FM, num_timesteps=100, n=16)\nprint(\"clean data mean:\", traj_set.embeddings[0].mean())\n\nx_t images mean (t=0): tensor(-0.2593) x_clean images mean: tensor(-0.2593)\nx_t images mean (t=1): tensor(-0.0035) x_noisy images mean: tensor(-0.0035)\nExtracting embeddings...\n\n\n100%|██████████| 100/100 [00:01&lt;00:00, 94.71it/s]\n\n\nclean data mean: -0.035028137\n\n\n\n\n\n\n_ = traj_set.plot_trajectories(n=16, show_figure=True,  with_ticks=True)\n\n\n\n\n\n\n\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\nFM_ot = ExactOptimalTransportConditionalFlowMatcher(sigma=0)\not_traj_set = generate_trajectories(FM_ot, num_timesteps=100, n=16)\nprint(\"clean data mean:\", ot_traj_set.embeddings[0].mean())\n\nExtracting embeddings...\n\n\n100%|██████████| 100/100 [00:01&lt;00:00, 94.74it/s]\n\n\nclean data mean: -0.03625052\n\n\n\n\n\n\n_ = ot_traj_set.plot_trajectories(n=16, show_figure=True, with_ticks=True)\n\nRunning t-SNE on (100, 16, 1008) embeddings...\nt-SNE done. Shape of 2D embeddings: (100, 16, 2)\n\n\n\n\n\n\n\n\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\ntraj_set = generate_trajectories(FM, num_timesteps=100, n=16)\nprint(\"clean embedding mean:\", traj_set.embeddings[0].mean())\nprint(\"noise embedding mean:\", traj_set.embeddings[-1].mean())\not_traj_set = generate_trajectories(FM_ot, num_timesteps=100, n=16)\nprint(\"OT clean embedding mean:\", ot_traj_set.embeddings[0].mean())\nprint(\"OT noise embedding mean:\", ot_traj_set.embeddings[-1].mean())\n\nx_t images mean (t=0): tensor(-0.2593) x_clean images mean: tensor(-0.2593)\nx_t images mean (t=1): tensor(-0.0035) x_noisy images mean: tensor(-0.0035)\nExtracting embeddings...\n\n\n100%|██████████| 100/100 [00:01&lt;00:00, 95.40it/s]\n\n\nclean embedding mean: -0.035028137\nnoise embedding mean: -0.023954444\nx_t images mean (t=0): tensor(-0.1974) x_clean images mean: tensor(-0.2593)\nx_t images mean (t=1): tensor(-0.0002) x_noisy images mean: tensor(-0.0035)\nExtracting embeddings...\n\n\n100%|██████████| 100/100 [00:01&lt;00:00, 95.34it/s]\n\n\nOT clean embedding mean: -0.03625052\nOT noise embedding mean: -0.024131157"
  },
  {
    "objectID": "inception_embedding_and_tsne.html#visualize-the-images-in-x_t-using-ot-plan",
    "href": "inception_embedding_and_tsne.html#visualize-the-images-in-x_t-using-ot-plan",
    "title": "Library code for trajectory visualization",
    "section": "Visualize the images in x_t using OT plan",
    "text": "Visualize the images in x_t using OT plan\n\nreal_images = real_images[:8]\nx_clean = real_images\nprint(\"x_clean.shape:\", x_clean.shape)\ntorch.manual_seed(0)\nx_noisy = torch.randn_like(x_clean)\nFM_ot = ExactOptimalTransportConditionalFlowMatcher(sigma=0)\nt, x_t, u_t = FM_ot.sample_location_and_conditional_flow(x0=x_clean, x1=x_noisy, t=torch.tensor(0).repeat(x_clean.shape[0]))\n\nx_clean.shape: torch.Size([8, 3, 64, 64])\n\n\n\n# display x_clean\n# Do a subplot with 2 by 4 layout\nfig, axs = plt.subplots(2, 4, figsize=(10, 5))\nfor i in range(2):\n    for j in range(4):  \n        axs[i, j].imshow((x_clean[i * 4 + j].permute(1, 2, 0) + 1))\n        axs[i, j].set_title(\"mean: \" + str(x_clean[i * 4 + j].mean()))\nplt.show()\n\n# display x_t\nfig, axs = plt.subplots(2, 4, figsize=(10, 5))\nfor i in range(2):\n    for j in range(4):\n        axs[i, j].imshow((x_t[i * 4 + j].permute(1, 2, 0) + 1))\n        axs[i, j].set_title(\"mean: \" + str(x_t[i * 4 + j].mean()))\nplt.show()\n\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n\n\n\n\n\n\n\n\n\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers)."
  },
  {
    "objectID": "Nano Diffusion 2D Toy.html",
    "href": "Nano Diffusion 2D Toy.html",
    "title": "Training a Diffusion Model on 2-D Points",
    "section": "",
    "text": "In this notebook, you’ll learn how to train a diffusion model to generate spirals of 2-D points.\nWe’ll explore the different components of diffusion models and their functions, as well as compare the quality of generated results using different model architectures.\nfrom dataclasses import dataclass\nimport math\nfrom typing import Dict, Tuple\n\nfrom matplotlib import pyplot as plt\nfrom sklearn.datasets import make_swiss_roll\nimport torch\nfrom torch import nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom torch.nn import MSELoss\nfrom torch.utils.data import DataLoader\nfrom tqdm.auto import tqdm"
  },
  {
    "objectID": "Nano Diffusion 2D Toy.html#dataloaders",
    "href": "Nano Diffusion 2D Toy.html#dataloaders",
    "title": "Training a Diffusion Model on 2-D Points",
    "section": "Dataloaders",
    "text": "Dataloaders\nBefore training the model, we need to define some hyperparameters and create our train and validation dataloaders.\n\ndevice = torch.device(\"cpu\")\n\n@dataclass\nclass TrainingConfig:\n    batch_size: int = 256 # batch size\n    learning_rate: float = 5e-4 # initial learning rate\n    # learning_rate: float = 1e-3 # initial learning rate\n    weight_decay: float = 1e-6 # weight decay\n    num_denoising_steps: int = 1000 # number of timesteps\n\n\ndef load_data(config: TrainingConfig) -&gt; Tuple[DataLoader, DataLoader]:\n    \"\"\"\n    Load the data and return the train and validation dataloaders.\n\n    Args:\n      config: TrainingConfig object.\n    Returns:\n      train_dataloader: DataLoader for the training data.\n      val_dataloader: DataLoader for the validation data.\n    \"\"\"\n    n = int(1e+6)\n    x, _ = make_swiss_roll(n_samples=n, noise=0)\n    x = x[:, [0, 2]]\n    scaling = 2\n    x = (x - x.mean()) / x.std() * scaling\n    x_train = x[:int(n * 0.8), :]\n    x_val = x[int(n * 0.8):, :]\n\n    class SimpleDataset:\n      def __init__(self, data):\n        self.data = data\n\n      def __len__(self):\n        return len(self.data)\n\n      def __getitem__(self, i):\n        return self.data[i]\n\n    train_dataset = SimpleDataset(x_train)\n    val_dataset = SimpleDataset(x_val)\n    train_dataloader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True, num_workers=0)\n    val_dataloader = DataLoader(val_dataset, batch_size=config.batch_size, shuffle=False, num_workers=0)\n\n    return train_dataloader, val_dataloader\n\n\nconfig = TrainingConfig()\ntrain_dataloader, val_dataloader = load_data(config)\nfirst_batch = next(iter(train_dataloader))\nprint(\"batch shape:\", first_batch.shape)\n\nbatch shape: torch.Size([256, 2])"
  },
  {
    "objectID": "Nano Diffusion 2D Toy.html#forward-diffusion",
    "href": "Nano Diffusion 2D Toy.html#forward-diffusion",
    "title": "Training a Diffusion Model on 2-D Points",
    "section": "Forward Diffusion",
    "text": "Forward Diffusion\nIn forward diffusion, we gradually add noise to an input data sample, where by the end of the process it should be pure noise.\n\ndef forward_diffusion(x_0, t, noise_schedule, noise=None):\n    t_shape = (-1,) + (1,) * (x_0.ndim - 1)\n    _ts = t.view(*t_shape)\n    if noise is None:\n        noise = torch.randn_like(x_0)\n    assert _ts.max() &lt; len(noise_schedule[\"alphas_cumprod\"]), f\"t={_ts.max()} is larger than the length of noise_schedule: {len(noise_schedule['alphas_cumprod'])}\"\n    alpha_prod_t = noise_schedule[\"alphas_cumprod\"][_ts]\n    x_t = (alpha_prod_t ** 0.5) * x_0 + ((1 - alpha_prod_t) ** 0.5) * noise\n    return x_t, noise\n\nLet’s take a look at the process in action.\n\nx_0 = next(iter(val_dataloader))\nx_0 = x_0.to(device)\nx_t_list = []\ncommon_noise = torch.randn_like(x_0)\n\nfig, axs = plt.subplots(1, 6, figsize=(20, 3))\nfor i, t in enumerate(range(0, 1000, 190)):\n    t_ = torch.full((x_0.shape[0],), t, device=device)\n    x_t = forward_diffusion(x_0, t_, noise_schedule, noise=common_noise)[0]\n    x_t = x_t.cpu()\n    axs[i].scatter(x_t[:,0], x_t[:,1], color='white', edgecolor='gray', s=5)\n    axs[i].set_axis_off()\n    axs[i].set_title('$q(\\mathbf{x}_{'+str(t)+'})$')\n\n\n\n\n\n\n\n\nAs the noise increases, the orderly spiral quickly fades away as it becomes a disordered pile of points."
  },
  {
    "objectID": "Nano Diffusion 2D Toy.html#reverse-denoising",
    "href": "Nano Diffusion 2D Toy.html#reverse-denoising",
    "title": "Training a Diffusion Model on 2-D Points",
    "section": "Reverse Denoising",
    "text": "Reverse Denoising\nReverse denoising uses a trained denoising model to incremenetally remove noise from an image. We’ll define the denoising step for this process now, but before we can use it, we’ll need to train a model.\n\ndef denoising_step(denoising_model, x_t, t, noise_schedule, thresholding=False):\n    \"\"\"\n    This is the backward diffusion step, with the effect of denoising.\n    \"\"\"\n    if isinstance(t, int):\n        t_tensor = torch.full((x_t.shape[0],), t, device=x_t.device)\n    else:\n        t_tensor = t\n    with torch.no_grad():\n        model_output = denoising_model(t=t_tensor, x=x_t)\n    if hasattr(model_output, \"sample\"):\n        model_output = model_output.sample\n\n    # Extract relevant values from noise_schedule\n    alpha_prod_t = noise_schedule[\"alphas_cumprod\"][t_tensor]\n    # deal with t=0 case where t can be a tensor\n    alpha_prod_t_prev = torch.where(t_tensor &gt; 0,\n                                    noise_schedule[\"alphas_cumprod\"][t_tensor - 1],\n                                    torch.ones_like(t_tensor, device=x_t.device))\n\n    # Reshape alpha_prod_t_prev for proper broadcasting\n    view_shape = (-1,) + (1,) * (x_t.ndim - 1)\n    alpha_prod_t = alpha_prod_t.view(*view_shape)\n    alpha_prod_t_prev = alpha_prod_t_prev.view(*view_shape)\n\n    beta_prod_t = 1 - alpha_prod_t\n    beta_prod_t_prev = 1 - alpha_prod_t_prev\n    current_alpha_t = alpha_prod_t / alpha_prod_t_prev\n    current_beta_t = 1 - current_alpha_t\n\n    # Compute the previous sample mean\n    pred_original_sample = (x_t - beta_prod_t ** 0.5 * model_output) / alpha_prod_t ** 0.5\n\n    # Compute the coefficients for pred_original_sample and current sample\n    pred_original_sample_coeff = (alpha_prod_t_prev ** 0.5 * current_beta_t) / beta_prod_t\n    current_sample_coeff = current_alpha_t ** 0.5 * beta_prod_t_prev / beta_prod_t\n\n    # Compute the previous sample\n    pred_prev_sample = pred_original_sample_coeff * pred_original_sample + current_sample_coeff * x_t\n\n    # Add noise\n    variance = torch.zeros_like(x_t)\n    variance_noise = torch.randn_like(x_t)\n\n    # Handle t=0 case where t can be a tensor\n    non_zero_mask = (t_tensor != 0).float().view(view_shape)\n    variance = non_zero_mask * ((1 - alpha_prod_t_prev) / (1 - alpha_prod_t) * current_beta_t)\n    variance = torch.clamp(variance, min=1e-20)\n\n    pred_prev_sample = pred_prev_sample + (variance ** 0.5) * variance_noise\n\n    return pred_prev_sample\n\n\ndef generate_samples_by_denoising(denoising_model, x_T, noise_schedule, n_T, device, thresholding=False, seed=0):\n    \"\"\"\n    This is the generation process.\n    \"\"\"\n    torch.manual_seed(seed)\n\n    x_t = x_T.to(device)\n    pbar = tqdm(range(n_T - 1, -1, -1))\n    for t in pbar:\n        x_t = denoising_step(denoising_model, x_t, t, noise_schedule, thresholding)\n        pbar.set_postfix({\"std\": x_t.std().item()})\n\n    # print(\"raw x_t range\", x_t.min(), x_t.max())\n    # x_t = (x_t / 2 + 0.5).clamp(0, 1)\n    # print(\"after clamp\", x_t.min(), x_t.max())\n    return x_t.cpu()"
  },
  {
    "objectID": "Nano Diffusion 2D Toy.html#training-code",
    "href": "Nano Diffusion 2D Toy.html#training-code",
    "title": "Training a Diffusion Model on 2-D Points",
    "section": "Training code",
    "text": "Training code\nWe’ve picked our data, created our dataloaders, and defined our noise schedule, foward diffusion, and reverse diffusion steps. Now, the only thing left to do is to train our diffusion model.\n\ndef train(model: nn.Module, optimizer: torch.optim.Optimizer, steps: int=100) -&gt; float:\n  criterion = MSELoss()\n  model.train()\n  print(\"Training on device:\", device)\n  max_train_steps = steps\n\n  step = 0\n  while step &lt; max_train_steps:\n    progress_bar = tqdm(train_dataloader)\n    for x_0 in progress_bar:\n      x_0 = x_0.float().to(device)  # x_0 is the clean data to teach the model to generate\n      optimizer.zero_grad()\n\n      true_noise = common_noise = torch.randn(x_0.shape).to(device)\n      t = torch.randint(0, config.num_denoising_steps, (x_0.shape[0],), device=device).long()\n      x_t, _ = forward_diffusion(x_0, t, noise_schedule, noise=common_noise)\n\n      predicted_noise = model(t=t, x=x_t)\n\n      loss = criterion(predicted_noise, true_noise)\n      loss.backward()\n      torch.nn.utils.clip_grad_norm_(model.parameters(), 1)  # try commenting it out\n      optimizer.step()\n\n      progress_bar.set_postfix({\"loss\": loss.cpu().item()})\n\n      step += 1\n\n      if step &gt;= max_train_steps:\n        print(f\"Reached the max training steps:\", max_train_steps)\n        break\n\n  return loss\n\n# Define the model and optimizer\nmodel3 = Model3(\n    dim_in=2,\n    dim_out=2,\n    hidden_features=[128, 128, 256],\n    num_timesteps=config.num_denoising_steps,\n).to(device)\nprint(f\"model params: {sum(p.numel() for p in model3.parameters())}\")\nmodel3_optimizer = optim.AdamW(model3.parameters(), lr=config.learning_rate, weight_decay=config.weight_decay)\n\n# Train the model\nmodel3_loss = train(model3, model3_optimizer, steps=100)\n\nmodel params: 281354\nTraining on device: cuda\n\n\n\n\n\nReached the max training steps: 100"
  },
  {
    "objectID": "Nano Diffusion 2D Toy.html#model-2",
    "href": "Nano Diffusion 2D Toy.html#model-2",
    "title": "Training a Diffusion Model on 2-D Points",
    "section": "Model 2",
    "text": "Model 2\n\nmodel2 = Model2(features=2, hidden_features=[512, 512]).to(device)\nprint(f\"model params: {sum(p.numel() for p in model2.parameters())}\")\n\nmodel2_optimizer = optim.AdamW(model2.parameters(), lr=config.learning_rate, weight_decay=config.weight_decay)\nmodel2_loss = train(model2, model2_optimizer, steps=5000)\nprint(\"model2_loss:\", model2_loss.item())\nvisualize_sampled_data(model2)\n\nmodel params: 281602\nTraining on device: cuda\n\n\n\n\n\n\n\n\nReached the max training steps: 5000\nmodel2_loss: 0.4093746542930603"
  },
  {
    "objectID": "Nano Diffusion 2D Toy.html#model-3",
    "href": "Nano Diffusion 2D Toy.html#model-3",
    "title": "Training a Diffusion Model on 2-D Points",
    "section": "Model 3",
    "text": "Model 3\n\nmodel3 = Model3(\n    dim_in=2,\n    dim_out=2,\n    hidden_features=[512, 512],\n    num_timesteps=config.num_denoising_steps,\n).to(device)\nprint(f\"model params: {sum(p.numel() for p in model3.parameters())}\")\n\nmodel3_optimizer = optim.AdamW(model3.parameters(), lr=config.learning_rate, weight_decay=config.weight_decay)\nmodel3_loss = train(model3, model3_optimizer, steps=5000)\nprint(\"model3_loss:\", model3_loss.item())\nvisualize_sampled_data(model3)\n\nmodel params: 1054218\nTraining on device: cuda\n\n\n\n\n\n\n\n\nReached the max training steps: 5000\nmodel3_loss: 0.34647679328918457\n\n\n\n\n\n\n\n\n\n\n\n\n\nimport torch\n\n# Sample points from the model\ndef generate_points(model):\n    with torch.no_grad():\n        x_T = torch.randn(128, 2)\n        x_sampled = generate_samples_by_denoising(model, x_T, noise_schedule, n_T=1000, device=device)\n    return x_sampled.cpu().numpy()\n\n\n# The target spiral points for comparison\ntarget_spiral = next(iter(train_dataloader))\n\ngenerated_points_1 = generate_points(model1)\ngenerated_points_1 = np.clip(generated_points_1, -3, 3)\nchamfer_dist = chamfer_distance(generated_points_1, target_spiral, direction='bi')\nprint(\"Model 1 Chamfer Distance:\", chamfer_dist)\n\n\ngenerated_points_2 = generate_points(model2)\nchamfer_dist = chamfer_distance(generated_points_2, target_spiral, direction='bi')\nprint(\"Model 2 Chamfer Distance:\", chamfer_dist)\n\n# Calculate Chamfer distance\ngenerated_points_3 = generate_points(model3)\nchamfer_dist = chamfer_distance(generated_points_3, target_spiral, direction='bi')\nprint(\"Model 3 Chamfer Distance:\", chamfer_dist)\n\n# # visualize the sampled images side by side\ndef visualize_sampled_data_side_by_side(models, generated_points):\n    fig, axs = plt.subplots(1, len(models) + 1, figsize=(5 * len(models), 5))\n    # Add ground truth\n    axs[0].scatter(target_spiral[:, 0], target_spiral[:, 1], color='white', edgecolor='gray', s=5)\n    axs[0].set_title(\"Ground truth\")\n\n    for i, (model, points) in enumerate(zip(models, generated_points)):\n        axs[i+1].scatter(points[:,0], points[:,1], color='white', edgecolor='gray', s=5)\n        axs[i+1].set_title(model.__class__.__name__)\n\nvisualize_sampled_data_side_by_side([model1, model2, model3], [generated_points_1, generated_points_2, generated_points_3])\n\n\n\n\nModel 1 Chamfer Distance: 0.7526525779908017\n\n\n\n\n\nModel 2 Chamfer Distance: 0.655060008800211\n\n\n\n\n\nModel 3 Chamfer Distance: 0.21780686103388566"
  },
  {
    "objectID": "Nano Diffusion 2D Toy.html#learning-questions",
    "href": "Nano Diffusion 2D Toy.html#learning-questions",
    "title": "Training a Diffusion Model on 2-D Points",
    "section": "Learning questions",
    "text": "Learning questions\n\nDoes neural net arch matter for the quality (measured by Chamfer)?\n\nIngredients (which ones are necessary):\n\nsine embedding of time t\noperator for fusing t and x: concat vs. multiply\nresidual connection\n\narch 1, Simple linear (no sine embedding, concat time and x)\narch 2, Sine embedding, concat time embedding and x\narch 3, TimeLinear (sine embedding, and multiply time embedding with x embedding)\n\nHow does the model size matter (number of layers, number of hidden units), within the same class of arch?\nWhat is the history of sine/cosine positional embeddings? We can include some background material about how people came up with this idea.\nAnimate the trajectory of forward diffusion, and denoising sampling.\nAnimate the learning process, how does trajectories change."
  },
  {
    "objectID": "Image Captioning Lesson.html",
    "href": "Image Captioning Lesson.html",
    "title": "Generating an Image Captions Dataset",
    "section": "",
    "text": "Image captions are natural language descriptions of an image. Models like CLIP and DALL-E were trained on millions of image-caption pairs to enable them to take text as input for generating an image. This is called text-to-image generation, where a model learns to understand the relationship between text and visual elements. The process through which these models learn this semantic text-image relationship is called text conditioning.",
    "crumbs": [
      "Home",
      "Text Conditioning",
      "Generating an Image Captions Dataset"
    ]
  },
  {
    "objectID": "Image Captioning Lesson.html#get-the-data",
    "href": "Image Captioning Lesson.html#get-the-data",
    "title": "Generating an Image Captions Dataset",
    "section": "1. Get the data",
    "text": "1. Get the data\nFirst, we must download our chosen dataset.\n\n# To use the files in the parent directory run this cell\nimport os\nimport sys\nmodule_path = os.path.abspath(os.path.join('..'))\nif module_path not in sys.path:\n    sys.path.append(module_path)\n\n\n# Load raw image data\nfrom src.datasets.hugging_face_dataset import HuggingFaceDataset\n\ndataset_name = \"zzsi/afhq64_16k\"\ndataset = HuggingFaceDataset(dataset_name, 'val')\nprint(f\"len(dataset): {len(dataset)}\")\ndataset[0][0]\n\nlen(dataset): 1500",
    "crumbs": [
      "Home",
      "Text Conditioning",
      "Generating an Image Captions Dataset"
    ]
  },
  {
    "objectID": "Image Captioning Lesson.html#get-the-model",
    "href": "Image Captioning Lesson.html#get-the-model",
    "title": "Generating an Image Captions Dataset",
    "section": "2. Get the model",
    "text": "2. Get the model\nNow, we can use HuggingFace to easily run inference on the PaliGemma model.\n\nfrom transformers import AutoProcessor, PaliGemmaForConditionalGeneration\nimport torch\n\nmodel_id = \"google/paligemma-3b-mix-224\"\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\ndtype = torch.bfloat16\n\nmodel = PaliGemmaForConditionalGeneration.from_pretrained(\n    model_id,\n    torch_dtype=dtype,\n    device_map=device,\n    revision=\"bfloat16\",\n).eval().to(device)\nprocessor = AutoProcessor.from_pretrained(model_id)\n\n2025-01-13 14:33:39.573619: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2025-01-13 14:33:39.648818: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n2025-01-13 14:33:39.979174: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/cuda-11.2/lib64:/usr/local/cuda-11.2/extras/CUPTI/lib64\n2025-01-13 14:33:39.979236: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/cuda-11.2/lib64:/usr/local/cuda-11.2/extras/CUPTI/lib64\n2025-01-13 14:33:39.979240: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\nGemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n`config.hidden_activation` if you want to override this behaviour.\nSee https://github.com/huggingface/transformers/pull/29402 for more details.",
    "crumbs": [
      "Home",
      "Text Conditioning",
      "Generating an Image Captions Dataset"
    ]
  },
  {
    "objectID": "Image Captioning Lesson.html#generate-a-caption-for-a-single-image",
    "href": "Image Captioning Lesson.html#generate-a-caption-for-a-single-image",
    "title": "Generating an Image Captions Dataset",
    "section": "3. Generate a caption for a single image",
    "text": "3. Generate a caption for a single image\nLet’s try to generate a caption for the first image. PaliGemma requires an image and a text prompt. To output caption-like text, we can use the prompt \"&lt;image&gt; caption en\" where the &lt;image&gt; will automatically get replaced by the input image.\n\n# Display the image\nimage = dataset[0][0]\ndisplay(image)\n\n# Setup the model inputs\nprompt = \"&lt;image&gt; caption en\"\nmodel_inputs = processor(text=prompt, images=image, return_tensors=\"pt\").to(model.device)\ninput_len = model_inputs[\"input_ids\"].shape[-1]\n\n# Generate the caption\nwith torch.inference_mode():\n    generation = model.generate(**model_inputs, max_new_tokens=100, do_sample=False)\n    generation = generation[0][input_len:]\n    decoded = processor.decode(generation, skip_special_tokens=True)\n    print(decoded)\n\n\n\n\n\n\n\n\nA white cat with black and brown spots looks directly at the camera. Its eyes are green and its nose is black. The cat's fur is white and its ears are white. The background is blurry.",
    "crumbs": [
      "Home",
      "Text Conditioning",
      "Generating an Image Captions Dataset"
    ]
  },
  {
    "objectID": "Image Captioning Lesson.html#generating-captions-for-the-whole-dataset",
    "href": "Image Captioning Lesson.html#generating-captions-for-the-whole-dataset",
    "title": "Generating an Image Captions Dataset",
    "section": "4. Generating captions for the whole dataset",
    "text": "4. Generating captions for the whole dataset\nHere we only generate captions for the first 5 images so that this doesn’t run for too long. Simply remove the first line.\n\ndataset = list(dataset)[:5] # only process the first 5 images\n\n# Generate captions for each image\ncaptions = []\nfor data in dataset:\n    image = data[0]\n    prompt = \"&lt;image&gt; caption en\"\n    model_inputs = processor(text=prompt, images=image, return_tensors=\"pt\").to(model.device)\n    input_len = model_inputs[\"input_ids\"].shape[-1]\n\n    with torch.inference_mode():\n        generation = model.generate(**model_inputs, max_new_tokens=100, do_sample=False)\n        generation = generation[0][input_len:]\n        decoded = processor.decode(generation, skip_special_tokens=True)\n        captions.append(decoded)\n        print(decoded)\n\nA white cat with black and brown spots looks directly at the camera. Its eyes are green and its nose is black. The cat's fur is white and its ears are white. The background is blurry.\nA close-up of a brown and black cat with green eyes. The cat has a pink nose, white whiskers, and a brown and black coat. The background is blurry. The cat's eyes are green, and its ears are pointy. The cat is looking at the camera.\nA white cat with two different colored eyes, one blue and one yellow, looks directly at the camera. Its eyes are blue on one side and yellow on the other. The cat has a pink nose, white whiskers, and a white chest. Its ears are pink and its fur is white. The cat is standing in front of a white background.\nA calico cat with a white chest and green eyes looks directly at the camera. Its ears are perked up and its whiskers are long. The cat's eyes are blue and its nose is pink. The cat is standing on a blue surface and its fur is black and white.\nA gray cat with yellow eyes looks directly at the camera, its ears perked up. The cat's eyes are glowing yellow, and its whiskers are long and prominent. The cat's fur is gray, and its nose is black. The cat is sitting on the ground, its head tilted slightly back. The background is blurry.\n\n\n\n\n# Plot images with captions\nimport matplotlib.pyplot as plt\nimport textwrap\n\nfig, axs = plt.subplots(len(dataset), 2, figsize=(5, 15))\n\nfor i in range(len(dataset)):\n    axs[i, 0].imshow(dataset[i][0])\n    axs[i, 0].axis('off')\n    caption = textwrap.fill(captions[i], 30)\n    axs[i, 1].text(0.0, 0.5, caption, verticalalignment='center', wrap=True)\n    axs[i, 1].axis('off')",
    "crumbs": [
      "Home",
      "Text Conditioning",
      "Generating an Image Captions Dataset"
    ]
  },
  {
    "objectID": "Image Captioning Lesson.html#write-to-a-json-file",
    "href": "Image Captioning Lesson.html#write-to-a-json-file",
    "title": "Generating an Image Captions Dataset",
    "section": "5. Write to a json file",
    "text": "5. Write to a json file\nWith the captions all generated, the only thing left to is to write them to a json for future use.\n\nimport json\n\n# Save the captions to a JSON file\nwith open(\"captions.json\", \"w\") as f:\n    json.dump(captions, f)",
    "crumbs": [
      "Home",
      "Text Conditioning",
      "Generating an Image Captions Dataset"
    ]
  },
  {
    "objectID": "5_1_vae_and_latent_space.html",
    "href": "5_1_vae_and_latent_space.html",
    "title": "Generating in the Latent Space",
    "section": "",
    "text": "To generate high resolution images, naively training a denoising model on 1024x1024 pixels requires a lot of time and may result in poor quality. To address this, we can compress the image into a latent space of lower dimension. As long as the compression does not lose too much information, the denoising model can be trained to generate in the latent space, and then we can decode the generated latent vectors back to the high resolution image space. Stable Diffusion pioneered this approach.\nA common choice for the compression is to use a variational autoencoder (VAE). The VAE has an encoder and decoder. The encoder compresses the image into a latent vector, and the decoder reconstructs the image from the latent vector. Optionally, people add a vector quantization (VQ) layer to the latent space to convert the continuous latent space into a discrete token ids. This is called a VQ-VAE. Since the denoising model naturally operates in a continuous space, vector quantization is not needed.",
    "crumbs": [
      "Home",
      "Scaling up: generate bigger and more diverse images",
      "Generating in the Latent Space"
    ]
  },
  {
    "objectID": "5_1_vae_and_latent_space.html#training-a-vae",
    "href": "5_1_vae_and_latent_space.html#training-a-vae",
    "title": "Generating in the Latent Space",
    "section": "Training a VAE",
    "text": "Training a VAE\nThe loss function includes a reconstruction loss and a regularization term for the latent space. The reconstruction loss is typically a pixel-wise L2 loss. The regularization term is a KL divergence between the latent space and a normal distribution, which encourages the latent space to be easier for diffusion and flow matching models to generate.\nThis colab notebook is a great tutorial for training a VAE on MNIST, an image dataset of hand-written digits.\nOften times, you do not need to train a VAE from scratch. It is typically sufficient to use a pre-trained VAE that is trained on a large dataset of images. The Stable Diffusion VAEs are a great choice. NVIDIA’s COSMOS tokenizers (another name for VAE and VQ-VAE) are newer and apply to both images and video.",
    "crumbs": [
      "Home",
      "Scaling up: generate bigger and more diverse images",
      "Generating in the Latent Space"
    ]
  },
  {
    "objectID": "5_1_vae_and_latent_space.html#evaluating-a-vaes-reconstruction-quality",
    "href": "5_1_vae_and_latent_space.html#evaluating-a-vaes-reconstruction-quality",
    "title": "Generating in the Latent Space",
    "section": "Evaluating a VAE’s Reconstruction Quality",
    "text": "Evaluating a VAE’s Reconstruction Quality\nLet’s load a pre-trained VAE and evaluate its reconstruction quality on the animal faces dataset.\n\nLoad the Stable Diffusion (SD) VAE\n\nimport torch\nfrom diffusers import AutoencoderKL\n\n# Load Stable Diffusion (SD) VAE\nvae = AutoencoderKL.from_pretrained(\"stabilityai/sd-vae-ft-mse\")\nvae.eval()  # Set to evaluation mode\n\n# Print model structure and size\nprint(\"VAE Model Structure:\")\nprint(vae)\n\n# Calculate and print total number of parameters\ntotal_params = sum(p.numel() for p in vae.parameters())\nprint(f\"Model size (number of parameters): {total_params / 1e6:.2f}M\")\n\nVAE Model Structure:\nAutoencoderKL(\n  (encoder): Encoder(\n    (conv_in): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (down_blocks): ModuleList(\n      (0): DownEncoderBlock2D(\n        (resnets): ModuleList(\n          (0-1): 2 x ResnetBlock2D(\n            (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (nonlinearity): SiLU()\n          )\n        )\n        (downsamplers): ModuleList(\n          (0): Downsample2D(\n            (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2))\n          )\n        )\n      )\n      (1): DownEncoderBlock2D(\n        (resnets): ModuleList(\n          (0): ResnetBlock2D(\n            (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n            (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (nonlinearity): SiLU()\n            (conv_shortcut): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n          )\n          (1): ResnetBlock2D(\n            (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (nonlinearity): SiLU()\n          )\n        )\n        (downsamplers): ModuleList(\n          (0): Downsample2D(\n            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))\n          )\n        )\n      )\n      (2): DownEncoderBlock2D(\n        (resnets): ModuleList(\n          (0): ResnetBlock2D(\n            (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n            (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (nonlinearity): SiLU()\n            (conv_shortcut): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))\n          )\n          (1): ResnetBlock2D(\n            (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n            (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (nonlinearity): SiLU()\n          )\n        )\n        (downsamplers): ModuleList(\n          (0): Downsample2D(\n            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2))\n          )\n        )\n      )\n      (3): DownEncoderBlock2D(\n        (resnets): ModuleList(\n          (0-1): 2 x ResnetBlock2D(\n            (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n            (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (nonlinearity): SiLU()\n          )\n        )\n      )\n    )\n    (mid_block): UNetMidBlock2D(\n      (attentions): ModuleList(\n        (0): Attention(\n          (group_norm): GroupNorm(32, 512, eps=1e-06, affine=True)\n          (to_q): Linear(in_features=512, out_features=512, bias=True)\n          (to_k): Linear(in_features=512, out_features=512, bias=True)\n          (to_v): Linear(in_features=512, out_features=512, bias=True)\n          (to_out): ModuleList(\n            (0): Linear(in_features=512, out_features=512, bias=True)\n            (1): Dropout(p=0.0, inplace=False)\n          )\n        )\n      )\n      (resnets): ModuleList(\n        (0-1): 2 x ResnetBlock2D(\n          (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (nonlinearity): SiLU()\n        )\n      )\n    )\n    (conv_norm_out): GroupNorm(32, 512, eps=1e-06, affine=True)\n    (conv_act): SiLU()\n    (conv_out): Conv2d(512, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  )\n  (decoder): Decoder(\n    (conv_in): Conv2d(4, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (up_blocks): ModuleList(\n      (0-1): 2 x UpDecoderBlock2D(\n        (resnets): ModuleList(\n          (0-2): 3 x ResnetBlock2D(\n            (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n            (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (nonlinearity): SiLU()\n          )\n        )\n        (upsamplers): ModuleList(\n          (0): Upsample2D(\n            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          )\n        )\n      )\n      (2): UpDecoderBlock2D(\n        (resnets): ModuleList(\n          (0): ResnetBlock2D(\n            (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n            (conv1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (nonlinearity): SiLU()\n            (conv_shortcut): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n          )\n          (1-2): 2 x ResnetBlock2D(\n            (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (nonlinearity): SiLU()\n          )\n        )\n        (upsamplers): ModuleList(\n          (0): Upsample2D(\n            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          )\n        )\n      )\n      (3): UpDecoderBlock2D(\n        (resnets): ModuleList(\n          (0): ResnetBlock2D(\n            (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n            (conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (nonlinearity): SiLU()\n            (conv_shortcut): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n          )\n          (1-2): 2 x ResnetBlock2D(\n            (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (nonlinearity): SiLU()\n          )\n        )\n      )\n    )\n    (mid_block): UNetMidBlock2D(\n      (attentions): ModuleList(\n        (0): Attention(\n          (group_norm): GroupNorm(32, 512, eps=1e-06, affine=True)\n          (to_q): Linear(in_features=512, out_features=512, bias=True)\n          (to_k): Linear(in_features=512, out_features=512, bias=True)\n          (to_v): Linear(in_features=512, out_features=512, bias=True)\n          (to_out): ModuleList(\n            (0): Linear(in_features=512, out_features=512, bias=True)\n            (1): Dropout(p=0.0, inplace=False)\n          )\n        )\n      )\n      (resnets): ModuleList(\n        (0-1): 2 x ResnetBlock2D(\n          (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (nonlinearity): SiLU()\n        )\n      )\n    )\n    (conv_norm_out): GroupNorm(32, 128, eps=1e-06, affine=True)\n    (conv_act): SiLU()\n    (conv_out): Conv2d(128, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  )\n  (quant_conv): Conv2d(8, 8, kernel_size=(1, 1), stride=(1, 1))\n  (post_quant_conv): Conv2d(4, 4, kernel_size=(1, 1), stride=(1, 1))\n)\nModel size (number of parameters): 83.65M\n\n\n\n\nAnimal faces dataset\n\n# Load the animal faces dataset\nfrom datasets import load_dataset\nfrom matplotlib import pyplot as plt\nimport numpy as np\n\ndataset = load_dataset(\"zzsi/afhq64_16k\", split=\"train\")\n\n# visualize the first example\nimg = dataset[0][\"image\"]\nimg = np.array(img).astype(np.float32) / 255.0\nplt.figure(figsize=(3, 3))\nplt.imshow(img)\nplt.title(\"Original Image\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\nEncoding the Image into the Latent Space\nTake the first image in the dataset and encode it into the latent space.\n\nimport numpy as np\n\n# Fix the random seed for reproducibility\ntorch.manual_seed(0)\n\nimg = dataset[0][\"image\"]\nimg = np.array(img).astype(np.float32) / 255.0\nimg = torch.from_numpy(img).unsqueeze(0).permute(0, 3, 1, 2)\n\nprint(\"Encoding the image into latent space...\")\nprint(\"Shape of the image:\", img.shape)\nwith torch.no_grad():\n    latents = vae.encode(img).latent_dist.sample()\n\nprint(\"Shape of the latents:\", latents.shape)\nprint(\"avg value of the latents:\", latents.mean())\n\nEncoding the image into latent space...\nShape of the image: torch.Size([1, 3, 64, 64])\nShape of the latents: torch.Size([1, 4, 8, 8])\navg value of the latents: tensor(0.6707)\n\n\nThe latents are 4 x 8 x 8, which means there are 4 channels of 2D “latent images”, each of size 8 x 8. Since the original image is 64 x 64. The spatial compression ratio is 8. The compressed latent images are much smaller! Our hope is that it makes training the diffusion model easier in the smaller latent space.\nNow let’s visualize the 4 channels of the latents.\n\n# Visualize the latents\nfrom matplotlib import pyplot as plt\n\n# Use subplot to visualize the 4 channels of the latents\nfig, axs = plt.subplots(2, 2, figsize=(5, 5))\nlatents_np = latents.cpu().numpy()\naxs[0, 0].imshow(latents_np[0, 0])\naxs[0, 1].imshow(latents_np[0, 1])\naxs[1, 0].imshow(latents_np[0, 2])\naxs[1, 1].imshow(latents_np[0, 3])\n# add a title to each subplot\naxs[0, 0].set_title(\"Channel 0\")\naxs[0, 1].set_title(\"Channel 1\")\naxs[1, 0].set_title(\"Channel 2\")\naxs[1, 1].set_title(\"Channel 3\")\n# Add a title to the figure\nfig.suptitle(\"Visualization of the 4 channels of the latents\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\nDecoding the Latent Vectors back to the Image Space\nNow let’s decode the latents back to the image space, and see if we can reconstruct the original image.\n\nwith torch.no_grad():\n    decoder_output = vae.decode(latents)\n    decoded_images = decoder_output.sample.cpu().numpy().transpose(0, 2, 3, 1)\n\nprint(\"Shape of the decoded image:\", decoded_images.shape[1:])\n\n# Visualize the decoded image\nplt.figure(figsize=(3, 3))\nplt.imshow(decoded_images[0])\nplt.title(\"Decoded Image\")\nplt.show()\n\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n\n\nShape of the decoded image: (64, 64, 3)\n\n\n\n\n\n\n\n\n\nThe reconstruction is not perfect, and we can see some artifacts.\nLet’s make the image larger.\n\n\nimg = dataset[0][\"image\"].resize((256, 256))\nimg = np.array(img).astype(np.float32) / 255.0\n\n\nimg = torch.from_numpy(img).unsqueeze(0).permute(0, 3, 1, 2)\nwith torch.no_grad():\n    latents = vae.encode(img).latent_dist.sample()\n    decoded_images = vae.decode(latents).sample.cpu().numpy().transpose(0, 2, 3, 1)\n\nplt.figure(figsize=(6, 6))\nplt.subplot(1, 2, 1)\nplt.imshow(img.cpu().numpy().transpose(0, 2, 3, 1)[0])\nplt.title(\"Original Image\")\nplt.subplot(1, 2, 2)\nplt.imshow(decoded_images[0])\nplt.title(\"Decoded Image\")\nplt.show()\n\n\n\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n\n\n\n\n\n\n\n\n\nNow with the 256 x 256 image, the reconstruction feels much better, even though the encoding and decoding steps are the same as before. If we look closely at small details, there are still artifacts. With this example, we now have intuitive understanding of the benefits and limitations of VAE models.",
    "crumbs": [
      "Home",
      "Scaling up: generate bigger and more diverse images",
      "Generating in the Latent Space"
    ]
  },
  {
    "objectID": "5_1_vae_and_latent_space.html#using-vae-in-the-diffusion-training-and-generation",
    "href": "5_1_vae_and_latent_space.html#using-vae-in-the-diffusion-training-and-generation",
    "title": "Generating in the Latent Space",
    "section": "Using VAE in the Diffusion Training and Generation",
    "text": "Using VAE in the Diffusion Training and Generation\nTo use a VAE in the diffusion training, we just add an encoding step in the training loop:\nfor images in dataloader:\n    # Compress images into latent vectors\n    latents = vae.encode(images)\n    # Train the diffusion model on the latent vectors instead of the images\n    ...\nDuring generation, we can use the VAE to compress the generated latent vectors back to the image space:\n# Generate latent vectors\nlatents = # yourgeneration code ...\n\n# Decode latent vectors back to images to visualize\nimages = vae.decode(latents)\nHowever, this is not quite efficient, because the same training images are encoded again and again. Instead, we can pre-compute the latent vectors for all the training images and store them in a separate dataset.\nIn the next tutorial, we will apply this recipe to a larger dataset, MJ 600k images.",
    "crumbs": [
      "Home",
      "Scaling up: generate bigger and more diverse images",
      "Generating in the Latent Space"
    ]
  },
  {
    "objectID": "4_2_text_conditioning_cfm.html",
    "href": "4_2_text_conditioning_cfm.html",
    "title": "Text conditioning for Flow Matching",
    "section": "",
    "text": "Similar to diffusion, we can add text conditioning to the flow matching algorithm. The modification to the training loop is small. Most of the code change is already done inside the denoising model.",
    "crumbs": [
      "Home",
      "Text Conditioning",
      "Text conditioning for Flow Matching"
    ]
  },
  {
    "objectID": "4_2_text_conditioning_cfm.html#set-up-the-text-encoder",
    "href": "4_2_text_conditioning_cfm.html#set-up-the-text-encoder",
    "title": "Text conditioning for Flow Matching",
    "section": "Set Up the Text Encoder",
    "text": "Set Up the Text Encoder\n\n%load_ext autoreload\n%autoreload 2\n\n\nimport numpy as np\nnp.int = np.int32\nimport torch\nimport torch.nn as nn\nfrom transformers import CLIPTextModel, CLIPTokenizer\n\n\nclass TextEncoder(nn.Module):\n    def __init__(self, model_name: str, device: str):\n        super().__init__()\n        self.model_name = model_name\n        self.model = CLIPTextModel.from_pretrained(model_name).to(device)\n        self.tokenizer = CLIPTokenizer.from_pretrained(model_name)\n        self.device = device\n        # Get the text embedding dimension from the config\n        self.text_embed_dim = self.model.config.hidden_size\n\n    def forward(self, text: str) -&gt; torch.Tensor:\n        tokens = self.tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\").to(self.device)\n        return self.model(**tokens).pooler_output\n\n2025-01-09 03:50:19.415450: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n2025-01-09 03:50:19.428194: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1736394619.442354 3747064 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1736394619.446563 3747064 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2025-01-09 03:50:19.462277: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI, in other operations, rebuild TensorFlow with the appropriate compiler flags.",
    "crumbs": [
      "Home",
      "Text Conditioning",
      "Text conditioning for Flow Matching"
    ]
  },
  {
    "objectID": "4_2_text_conditioning_cfm.html#training",
    "href": "4_2_text_conditioning_cfm.html#training",
    "title": "Text conditioning for Flow Matching",
    "section": "Training",
    "text": "Training\n\nClassifier-Free Guidance in Training: Text Dropout\n\nimport itertools\nimport torch\nimport torch.nn as nn\nfrom torch.nn import MSELoss\nfrom torch.utils.data import DataLoader\nfrom tqdm.auto import tqdm\nimport sys\n\nsys.path.append(\"..\")\nsys.path.append(\"../src\")\nfrom lib_4_1.config import TrainingConfig\nfrom nanodiffusion.train_cfm import ExactOptimalTransportConditionalFlowMatcher, ConditionalFlowMatcher\n\ndef train(\n    config: TrainingConfig,\n    model: nn.Module,\n    text_encoder: TextEncoder,\n    train_dataloader: DataLoader,\n    val_dataloader: DataLoader,\n    optimizer: torch.optim.Optimizer,\n    steps: int=100,\n    silent: bool=False,\n) -&gt; float:\n  device = config.device\n  # FM = ExactOptimalTransportConditionalFlowMatcher(sigma=0)\n  FM = ConditionalFlowMatcher(sigma=0)\n  \n  model.train()\n  if not silent:\n    print(\"Training on device:\", device)\n  max_train_steps = steps\n\n  loss = None\n  progress_bar = tqdm(itertools.cycle(train_dataloader), total=max_train_steps, disable=silent)\n  step = 0\n  criterion = MSELoss()\n  for batch in progress_bar:\n    x_1 = batch[0]  # x_0 is the clean image to teach the model to generate\n    text = batch[1][\"text\"]  # text is the caption of the image\n    assert len(text) == x_1.shape[0]\n    # assert the type of text is a list of strings\n    \n    optimizer.zero_grad()\n\n    # Implement classifier-free guidance training\n    # Randomly drop out text conditioning with 10% probability\n    # The dropout is applied to the batch as a whole.\n    # Alternatively, we could apply it to each image in the batch.\n    text_drop_prob = 0.2\n    x_1 = x_1.to(device)\n    x_0 = torch.randn_like(x_1).to(device)\n\n    with torch.no_grad():\n        text_embeddings = text_encoder(text)\n\n    t, x_t, u_t = FM.sample_location_and_conditional_flow(x0=x_0, x1=x_1)\n    # t, x_t, u_t, _, text_embeddings_t = FM.guided_sample_location_and_conditional_flow(x0=x_0, x1=x_1, y0=None, y1=text_embeddings)\n    \n    # A dropout is applied to the ``text_embeddings`` input:\n    #   This means `predicted_noise` will be computed with 20% probability of the text embeddings being dropped out.\n    #   The model learns to predict the noise both with and without the text embeddings.\n    v_t = model(t=t, x=x_t, text_embeddings=text_embeddings, p_uncond=text_drop_prob)\n\n    loss = criterion(u_t, v_t)\n    loss.backward()\n    torch.nn.utils.clip_grad_norm_(model.parameters(), 1)  # try commenting it out\n    optimizer.step()\n\n    step += 1\n\n    if not silent:\n      progress_bar.set_postfix({\"loss\": loss.cpu().item()})\n\n    if step &gt;= max_train_steps:\n      break\n\n  return loss\n\n\n\nCaptioned Image Dataset, Text Encoder and the Main Denoising Model\n\nfrom torch import optim\nfrom lib_4_1.data import load_data\nfrom lib_4_1.model import create_unet_model\n\nconfig = TrainingConfig(dataset=\"reese-green/afhq64_captions_64k\", caption_column=\"caption_blip2-opt-2.7b\", batch_size=16, resolution=32)\ntext_encoder = TextEncoder(\"openai/clip-vit-large-patch14\", \"cuda:0\")\ntext_encoder.eval()\ntrain_ds, val_ds = load_data(config)\ndenoising_model = create_unet_model(config, config.device)\noptimizer = optim.AdamW(denoising_model.parameters(), lr=config.learning_rate, weight_decay=config.weight_decay)\n\nmodel params: 14.68 M\n\n\n\ntrain_dataloader = DataLoader(train_ds, batch_size=config.batch_size, shuffle=True)\nval_dataloader = DataLoader(val_ds, batch_size=config.batch_size, shuffle=False)\n\n\n\nTrain for 8000 Steps\n\ntrain(\n    config=config,\n    model=denoising_model,\n    text_encoder=text_encoder,\n    train_dataloader=train_dataloader,\n    val_dataloader=val_dataloader,\n    optimizer=optimizer,\n    steps=8000,\n    silent=False\n)\n\nTraining on device: cuda\n\n\n\n\n\ntensor(0.1608, device='cuda:0', grad_fn=&lt;MseLossBackward0&gt;)\n\n\n\n\nSave the Model\n\ntorch.save(denoising_model.state_dict(), \"denoising_model_4_2.pth\")\n\nIn the next tutorial, we will use the trained flow matching model to generate images with text conditioning.",
    "crumbs": [
      "Home",
      "Text Conditioning",
      "Text conditioning for Flow Matching"
    ]
  },
  {
    "objectID": "4_1_text_conditioning_ddpm.html",
    "href": "4_1_text_conditioning_ddpm.html",
    "title": "Text Conditioning (text2image)",
    "section": "",
    "text": "Conditioning means prompting. So far, we have been doing unconditional image generation from pure noise, or generation without prompting as any context information:\n\\[\n\\textrm{noise} \\rightarrow \\textrm{image}\n\\]\nThough it is cool to see the model generate interesting images like animal faces, it is not very useful yet as it lacks user control.\nIn this tutorial, we will start to add text conditioning to the model:\n\\[\n\\textrm{text} + \\textrm{noise} \\rightarrow \\textrm{image}\n\\]\nso that a prompt like “a black cat with lots of fur” can actually trigger the model to follow the instruction and generate such an image.\nA popular approach to do text-to-Image generation is “classifier-free guidance”. Historically, the first major technique along these lines was “classifier guidance”, which relies on a separate classifier to steer image synthesis.\nLet \\(x\\) be the image, and \\(y\\) be the text prompt. There are 2 distributions we could sample from:\nApplying the Bayes rule, we have:\n\\[\np_{\\theta}(x \\mid y) \\propto p_{\\theta}(x) \\cdot p_{\\theta}(y \\mid x)\n\\]\nWhen we look at the gradient of the log-probability,\n\\[\n\\nabla_{x} \\log p_{\\theta}(x \\mid y) = \\nabla_{x} \\log p_{\\theta}(x) + \\nabla_{x} \\log p_{\\theta}(y \\mid x)\n\\]\nNote that \\(\\nabla_{x_t} \\log p_{\\theta}(x_t)\\) is approximately the output of the denoising model (the noise prediction), up to a constant factor.\nTherefore, we can think of text-conditioned generation as taking the unconditional model and adjusting (“tweaking”) it with \\(p_{\\theta}(y \\mid x)\\), which is a “classifier” that predicts the label or text description given an image. When we add these two terms, we steer the generation toward images that are both likely under the unconditional model and consistent with the text \\(y\\). This is the core idea behind classifier guidance.",
    "crumbs": [
      "Home",
      "Text Conditioning",
      "Text Conditioning (text2image)"
    ]
  },
  {
    "objectID": "4_1_text_conditioning_ddpm.html#model-architecture-design-choices-for-text-conditioning",
    "href": "4_1_text_conditioning_ddpm.html#model-architecture-design-choices-for-text-conditioning",
    "title": "Text Conditioning (text2image)",
    "section": "Model Architecture Design choices for Text Conditioning",
    "text": "Model Architecture Design choices for Text Conditioning\nSimilar to the 2D point clouds, we are interested to experiment with different model architectures. Specifically for text conditioning (combining information from text and image), 3 approaches are in consideration:\n\nLinear modulation\n\nScale and shift other embeddings or intermediate representations based on the text embedding\nCan apply to time embedding, intermediate representations, or normalization layers.\n\nConcatenation\n\nConcatenate the text embedding with other embeddings.\n\nCross attention\n\nUse the intermediate representations as queries to attend to the text embedding.\n\n\nThe implementation also depends on whether we use the UNet or Transformer backbone. Let’s focus from UNet and linear modulation. The modulation can be done by simply adding the text embedding to the time embedding:\n# In UNetModel.__init__():\n+ self.text_proj = nn.Sequential(\n+                 linear(text_embed_dim, time_embed_dim),\n+                 nn.SiLU(),\n+                 linear(time_embed_dim, time_embed_dim),\n+             )\n...\n\n# In forward():\nemb = self.time_embed(timestep_embedding(t, self.model_channels))\n+ emb = emb + self.text_proj(text_embeddings)  # add the text embedding to the time embedding\nThen both the time+text embedding emb and the intermediate hidden representations h passes through the regular UNet blocks (no changes to following UNet blocks):\nfor module in self.input_blocks:\n    h = module(h, emb)\n    hs.append(h)\nh = self.middle_block(h, emb)\nfor module in self.output_blocks:\n    h = th.cat([h, hs.pop()], dim=1)\n    h = module(h, emb)\n...\n\nText embedding “dropout”\nThe recipe for classifier-free guidance requires us to randomly drop the text embedding some fraction of the time during training. This way we train a single model that can operates in both unconditional and conditional modes.\nWhen computing the training loss in the forward pass, we need to randomly drop the text embedding some fraction of the time:\n# In forward() of the denoising model (UNetModel), do the dropout randomly based on the `p_uncond` parameter:\n# If p_uncond is 0.2, then 20% of the time we drop out the text embedding.\n+ unconditional_mask = (th.rand(text_embeddings.shape[0]) &lt; p_uncond)  # this gets the indices of the texts that we want to drop out\n+ text_embeddings[unconditional_mask] = self.null_text_embed  # this sets the text embeddings to the null text embedding\nAnd the null_text_embed is a learnable embedding for the null text prompt:\n# In UNetModel.__init__():\n+ self.text_embed_dim = text_embed_dim\n+ self.null_text_embed = nn.Parameter(th.randn(1, text_embed_dim) * 0.02)\nThis way the model learns what the null text embedding means in the latent space. An alternative is simply setting null_text_embed to all zeros.",
    "crumbs": [
      "Home",
      "Text Conditioning",
      "Text Conditioning (text2image)"
    ]
  },
  {
    "objectID": "4_1_text_conditioning_ddpm.html#text-encoder",
    "href": "4_1_text_conditioning_ddpm.html#text-encoder",
    "title": "Text Conditioning (text2image)",
    "section": "Text Encoder",
    "text": "Text Encoder\nTo produce the text embedding, we can use a pre-trained CLIP model.\n\nimport torch\nimport torch.nn as nn\nfrom transformers import CLIPTextModel, CLIPTokenizer\n\n\nclass TextEncoder(nn.Module):\n    def __init__(self, model_name: str, device: str):\n        super().__init__()\n        self.model_name = model_name\n        self.model = CLIPTextModel.from_pretrained(model_name).to(device)\n        self.tokenizer = CLIPTokenizer.from_pretrained(model_name)\n        self.device = device\n        # Get the text embedding dimension from the config\n        self.text_embed_dim = self.model.config.hidden_size\n\n    def forward(self, text: str) -&gt; torch.Tensor:\n        tokens = self.tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\").to(self.device)\n        return self.model(**tokens).pooler_output\n\n2024-12-30 22:02:07.609824: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n2024-12-30 22:02:07.622932: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-12-30 22:02:07.637863: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-12-30 22:02:07.642424: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2024-12-30 22:02:07.653395: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2024-12-30 22:02:08.358540: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT",
    "crumbs": [
      "Home",
      "Text Conditioning",
      "Text Conditioning (text2image)"
    ]
  },
  {
    "objectID": "4_1_text_conditioning_ddpm.html#training",
    "href": "4_1_text_conditioning_ddpm.html#training",
    "title": "Text Conditioning (text2image)",
    "section": "Training",
    "text": "Training\nNow that the denoising model can handle text embeddings as an additional input, we also need to modify the training step to pass in the text embeddings. The main logic remains the same.\n\nClassifier-Free Guidance in Training: Text Dropout\nWith 20% of chance, the text embedding will be set to an empty embedding. The actual logic of dropout happens inside the denoising model.\n\nimport itertools\nimport torch\nimport torch.nn as nn\nfrom torch.nn import MSELoss\nfrom torch.utils.data import DataLoader\nfrom tqdm.auto import tqdm\nfrom typing import Dict\n\nfrom lib_4_1.diffusion import forward_diffusion\nfrom lib_4_1.bookkeeping import Bookkeeping\nfrom lib_4_1.config import TrainingConfig\n\ndef train(\n    config: TrainingConfig,\n    model: nn.Module,\n    text_encoder: TextEncoder,\n    train_dataloader: DataLoader,\n    val_dataloader: DataLoader,\n    noise_schedule: Dict,\n    optimizer: torch.optim.Optimizer,\n    steps: int=100,\n    silent: bool=False,\n    bookkeeping: Bookkeeping=None\n) -&gt; float:\n  device = config.device\n  num_denoising_steps = config.num_denoising_steps\n  \n  model.train()\n  if not silent:\n    print(\"Training on device:\", device)\n  max_train_steps = steps\n\n  loss = None\n  progress_bar = tqdm(itertools.cycle(train_dataloader), total=max_train_steps, disable=silent)\n  step = 0\n  criterion = MSELoss()\n  for batch in progress_bar:\n    x_0 = batch[0]  # x_0 is the clean image to teach the model to generate\n    text = batch[1][\"text\"]  # text is the caption of the image\n    assert len(text) == x_0.shape[0]\n    # assert the type of text is a list of strings\n    x_0 = x_0.float().to(device)  # x_0 is the clean data to teach the model to generate\n    optimizer.zero_grad()\n\n    # Implement classifier-free guidance training\n    # Randomly drop out text conditioning with 10% probability\n    # The dropout is applied to the batch as a whole.\n    # Alternatively, we could apply it to each image in the batch.\n    text_drop_prob = 0.2\n    true_noise = common_noise = torch.randn(x_0.shape).to(device)\n    t = torch.randint(0, num_denoising_steps, (x_0.shape[0],), device=device).long()\n    x_t, _ = forward_diffusion(x_0, t, noise_schedule, noise=common_noise)\n\n    with torch.no_grad():\n        text_embeddings = text_encoder(text)\n\n    # A dropout is applied to the ``text_embeddings`` input:\n    #   This means `predicted_noise` will be computed with 20% probability of the text embeddings being dropped out.\n    #   The model learns to predict the noise both with and without the text embeddings.\n    predicted_noise = model(t=t, x=x_t, text_embeddings=text_embeddings, p_uncond=text_drop_prob)\n\n    loss = criterion(predicted_noise, true_noise)\n    loss.backward()\n    torch.nn.utils.clip_grad_norm_(model.parameters(), 1)  # try commenting it out\n    optimizer.step()\n\n    step += 1\n\n    if not silent:\n      progress_bar.set_postfix({\"loss\": loss.cpu().item()})\n\n    if bookkeeping:\n      bookkeeping.run_callbacks(config=config, step=step, loss=loss, optimizer=optimizer, val_dataloader=val_dataloader)\n\n    if step &gt;= max_train_steps:\n      break\n\n  return loss\n\n\n\nCaptioned Image Dataset and Text Encoder\nLet’s create the main components of the model. Its has under 15M parameters, rather small.\nFor text encoder, we are using CLIP.\nFor the dataset of image-text pairs, we will use the dataset reese-green/afhq64_captions_64k generated by running the blip2-opt-2.7b model on the animal face images to extract the text description of the image. Here, we use a resolution of 32x32 pixels to make training faster.\nA good dataset is important for the performance of the model. To learn more about how to create a captioned image dataset, please refer to this tutorial.\n\nfrom torch import optim\nfrom lib_4_1.data import load_data\nfrom lib_4_1.model import create_unet_model\nfrom lib_4_1.diffusion import create_noise_schedule\n\nconfig = TrainingConfig(dataset=\"reese-green/afhq64_captions_64k\", caption_column=\"caption_blip2-opt-2.7b\", batch_size=16, resolution=32)\ntext_encoder = TextEncoder(\"openai/clip-vit-large-patch14\", \"cuda:0\")\ntext_encoder.eval()\ntrain_ds, val_ds = load_data(config)\nnoise_schedule = create_noise_schedule(n_T=config.num_denoising_steps, device=config.device)\ndenoising_model = create_unet_model(config, config.device)\noptimizer = optim.AdamW(denoising_model.parameters(), lr=config.learning_rate, weight_decay=config.weight_decay)\n\nmodel params: 14.68 M\n\n\n\n\nA Quick Data Check\nA quick check that the inputs and targets of a training example look good.\n\nfor x in train_ds:\n    print(x[0].shape)\n    print(x[1])\n    break\n\ntorch.Size([3, 32, 32])\n{'label': 0, 'text': 'a large white dog with brown eyes sitting on the grass'}\n\n\nSimilarly, check to see if the mini-batch look good.\n\ntrain_dataloader = DataLoader(train_ds, batch_size=config.batch_size, shuffle=True)\nval_dataloader = DataLoader(val_ds, batch_size=config.batch_size, shuffle=False)\nfor x in train_dataloader:\n    print(x[0].shape)\n    print(x[1][\"text\"])\n    text_embeddings = text_encoder(x[1][\"text\"])\n    print(text_embeddings.shape)\n    break\n\ntorch.Size([16, 3, 32, 32])\n['a gray cat with green eyes sitting on a table', 'a close up of a cat with a sad expression', 'a black cat with green eyes standing in front of a fence', 'a gray and white dog with an orange collar', 'a gray cat is sitting on a red blanket', 'a leopard walking through the grass in the wild', 'a white dog with its tongue out and its tongue hanging out', 'a cheetah with its tongue out in the grass', 'a gray and white cat laying on a couch', 'a black french bulldog sitting down with his ears up', 'a brown dog running on the ground', 'a cat is sitting on a branch with green leaves', 'a small black and white dog wearing a purple harness', 'a white dog with a collar on sitting on a bed', 'a lion cub is sitting in a zoo enclosure', 'a golden retriever sitting on a bench with its owner']\ntorch.Size([16, 768])\n\n\n\n\nWe Train\nWe train 20,000 steps. This can take 20~30 minutes on a A10 instance on Lambda Labs.\n\n%%time\n\ntrain(\n    config=config,\n    model=denoising_model,\n    text_encoder=text_encoder,\n    train_dataloader=train_dataloader,\n    val_dataloader=val_dataloader,\n    noise_schedule=noise_schedule,\n    optimizer=optimizer,\n    steps=20000,\n    silent=False\n)\n\nTraining on device: cuda\n\n\n\n\n\ntensor(0.0426, device='cuda:0', grad_fn=&lt;MseLossBackward0&gt;)\n\n\n\n\nSave the Model\nThe checkpoint will be useful in the next tutorial, where we see our model it in action.\n\n# save the model\ntorch.save(denoising_model.state_dict(), \"denoising_model_4_1.pth\")",
    "crumbs": [
      "Home",
      "Text Conditioning",
      "Text Conditioning (text2image)"
    ]
  },
  {
    "objectID": "4_1_text_conditioning_ddpm.html#generate-images-with-text-conditioning",
    "href": "4_1_text_conditioning_ddpm.html#generate-images-with-text-conditioning",
    "title": "Text Conditioning (text2image)",
    "section": "Generate images with text conditioning",
    "text": "Generate images with text conditioning\nIn the next tutorial, we will use the updated sampling code to generate images with text conditioning. By varying the guidance_scale parameter, we can see how the text conditioning affects the generated images.",
    "crumbs": [
      "Home",
      "Text Conditioning",
      "Text Conditioning (text2image)"
    ]
  },
  {
    "objectID": "2_2_fid.html",
    "href": "2_2_fid.html",
    "title": "Quality evaluation of an image generation model",
    "section": "",
    "text": "The Chamfer distance that we used in the previous notebook is a distance between point clouds. It is based on Euclidean distance, which works for low-dimensional data such as 2D points. For high-dimensional data such as images, the Euclidean distance is not a good measure, because the distance between two images can be high even if they are very similar.\nThe Fréchet Inception Distance (FID) is a metric that measures the distance between two distributions of images. A lower score means better quality. It is based on the Fréchet distance, which is a distance between Gaussian distributions. “Inception” here refers to a pre-trained Inception-v3 model, which is a convolutional neural network that was trained to classify images into 1000 categories. We can use the intermediate features of this model to measure the distance between two distributions of images.\nThe limitations of FID include:\n\nIt assumes Gaussian distribution of features.\nIt is sensitive to sample size and requires large sample sizes for stable estimates, which means it is computationally expensive.\n\nOther metrics include:\n\nInception Score (IS): measures both the diversity and quality of generated images. It may give misleading scores for specialized datasets (e.g., medical images, abstract art).\nLPIPS (Learned Perceptual Image Patch Similarity). It is good at measureing individual image quality, and more suitable for tasks like style transfer, image reconstruction.\nPSNR (Peak Signal-to-Noise Ratio). It measures pixel-level reconstruction quality,and does not measure perceptual quality.\nSSIM (Structural Similarity Index). It measures image quality based on luminance and contrast, and does not measure perceptual quality.\nArena-style human evaluation: compare generated images from different models side by side, and ask human experts to choose the better one.\n\nWe are going to use the packageclean-fid. As a santity check for the metric, let’s take a pre-trained model from huggingface with known good quality, and see what the FID score looks like.\n\n\n\n!pip install clean-fid\n\nDefaulting to user installation because normal site-packages is not writeable\nWARNING: Ignoring invalid distribution ~vidia-cusparse-cu12 (/home/ubuntu/.local/lib/python3.11/site-packages)\nWARNING: Ignoring invalid distribution ~vidia-cusparse-cu12 (/home/ubuntu/.local/lib/python3.11/site-packages)\nRequirement already satisfied: clean-fid in /home/ubuntu/.local/lib/python3.11/site-packages (0.1.35)\nRequirement already satisfied: torch in /home/ubuntu/.local/lib/python3.11/site-packages (from clean-fid) (2.5.1)\nRequirement already satisfied: torchvision in /home/ubuntu/.local/lib/python3.11/site-packages (from clean-fid) (0.20.1)\nRequirement already satisfied: numpy&gt;=1.14.3 in /home/ubuntu/.local/lib/python3.11/site-packages (from clean-fid) (1.26.4)\nRequirement already satisfied: scipy&gt;=1.0.1 in /home/ubuntu/.local/lib/python3.11/site-packages (from clean-fid) (1.14.0)\nRequirement already satisfied: tqdm&gt;=4.28.1 in /home/ubuntu/.local/lib/python3.11/site-packages (from clean-fid) (4.66.5)\nRequirement already satisfied: pillow&gt;=8.1 in /home/ubuntu/.local/lib/python3.11/site-packages (from clean-fid) (10.2.0)\nRequirement already satisfied: requests in /home/ubuntu/.local/lib/python3.11/site-packages (from clean-fid) (2.32.3)\nRequirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /opt/miniconda/lib/python3.11/site-packages (from requests-&gt;clean-fid) (2.0.4)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in /opt/miniconda/lib/python3.11/site-packages (from requests-&gt;clean-fid) (3.4)\nRequirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in /opt/miniconda/lib/python3.11/site-packages (from requests-&gt;clean-fid) (1.26.18)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /opt/miniconda/lib/python3.11/site-packages (from requests-&gt;clean-fid) (2023.7.22)\nRequirement already satisfied: filelock in /home/ubuntu/.local/lib/python3.11/site-packages (from torch-&gt;clean-fid) (3.13.3)\nRequirement already satisfied: typing-extensions&gt;=4.8.0 in /home/ubuntu/.local/lib/python3.11/site-packages (from torch-&gt;clean-fid) (4.10.0)\nRequirement already satisfied: networkx in /home/ubuntu/.local/lib/python3.11/site-packages (from torch-&gt;clean-fid) (3.2.1)\nRequirement already satisfied: jinja2 in /home/ubuntu/.local/lib/python3.11/site-packages (from torch-&gt;clean-fid) (3.1.4)\nRequirement already satisfied: fsspec in /home/ubuntu/.local/lib/python3.11/site-packages (from torch-&gt;clean-fid) (2024.3.1)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/ubuntu/.local/lib/python3.11/site-packages (from torch-&gt;clean-fid) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/ubuntu/.local/lib/python3.11/site-packages (from torch-&gt;clean-fid) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/ubuntu/.local/lib/python3.11/site-packages (from torch-&gt;clean-fid) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/ubuntu/.local/lib/python3.11/site-packages (from torch-&gt;clean-fid) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/ubuntu/.local/lib/python3.11/site-packages (from torch-&gt;clean-fid) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/ubuntu/.local/lib/python3.11/site-packages (from torch-&gt;clean-fid) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/ubuntu/.local/lib/python3.11/site-packages (from torch-&gt;clean-fid) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/ubuntu/.local/lib/python3.11/site-packages (from torch-&gt;clean-fid) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/ubuntu/.local/lib/python3.11/site-packages (from torch-&gt;clean-fid) (12.3.1.170)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/ubuntu/.local/lib/python3.11/site-packages (from torch-&gt;clean-fid) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/ubuntu/.local/lib/python3.11/site-packages (from torch-&gt;clean-fid) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/ubuntu/.local/lib/python3.11/site-packages (from torch-&gt;clean-fid) (12.4.127)\nRequirement already satisfied: triton==3.1.0 in /home/ubuntu/.local/lib/python3.11/site-packages (from torch-&gt;clean-fid) (3.1.0)\nRequirement already satisfied: sympy==1.13.1 in /home/ubuntu/.local/lib/python3.11/site-packages (from torch-&gt;clean-fid) (1.13.1)\nRequirement already satisfied: mpmath&lt;1.4,&gt;=1.1.0 in /home/ubuntu/.local/lib/python3.11/site-packages (from sympy==1.13.1-&gt;torch-&gt;clean-fid) (1.3.0)\nRequirement already satisfied: MarkupSafe&gt;=2.0 in /home/ubuntu/.local/lib/python3.11/site-packages (from jinja2-&gt;torch-&gt;clean-fid) (2.1.5)\nWARNING: Ignoring invalid distribution ~vidia-cusparse-cu12 (/home/ubuntu/.local/lib/python3.11/site-packages)\nWARNING: Ignoring invalid distribution ~vidia-cusparse-cu12 (/home/ubuntu/.local/lib/python3.11/site-packages)\nWARNING: Ignoring invalid distribution ~vidia-cusparse-cu12 (/home/ubuntu/.local/lib/python3.11/site-packages)\nWARNING: Ignoring invalid distribution ~vidia-cusparse-cu12 (/home/ubuntu/.local/lib/python3.11/site-packages)\n\n\n\n\n\nWe are using the model released by Google, which is trained on CIFAR-10 dataset.\n\nimport torch, transformers, diffusers, xformers, torchvision\nprint(\"torch version:\", torch.__version__)\nprint(\"transformers version:\", transformers.__version__)\nprint(\"diffusers version:\", diffusers.__version__)\nprint(\"xformers version:\", xformers.__version__)\nprint(\"torchvision version:\", torchvision.__version__)\n\ntorch version: 2.5.1+cu124\ntransformers version: 4.47.0\ndiffusers version: 0.31.0\nxformers version: 0.0.28.post3\ntorchvision version: 0.20.1+cu124\n\n\n\n%%time\nfrom diffusers import DDPMPipeline, DDIMPipeline, PNDMPipeline\n\nmodel_id = \"google/ddpm-cifar10-32\"\n\n# load model and scheduler\nddpm = DDPMPipeline.from_pretrained(model_id)  # you can replace DDPMPipeline with DDIMPipeline or PNDMPipeline for faster inference\nddpm.set_progress_bar_config(disable=True)\n\n2024-12-16 22:00:47.499784: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n2024-12-16 22:00:47.512904: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-12-16 22:00:47.527894: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-12-16 22:00:47.532473: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2024-12-16 22:00:47.543787: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2024-12-16 22:00:48.384524: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n\n\n\n\n\nAn error occurred while trying to fetch /home/ubuntu/.cache/huggingface/hub/models--google--ddpm-cifar10-32/snapshots/267b167dc01f0e4e61923ea244e8b988f84deb80: Error no file named diffusion_pytorch_model.safetensors found in directory /home/ubuntu/.cache/huggingface/hub/models--google--ddpm-cifar10-32/snapshots/267b167dc01f0e4e61923ea244e8b988f84deb80.\nDefaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n\n\nCPU times: user 2.57 s, sys: 639 ms, total: 3.21 s\nWall time: 3.51 s\n\n\n\n\n\n\n%%time\n# run pipeline in inference (sample random noise and denoise)\nfrom tqdm.notebook import tqdm\nddpm.to(\"cuda\")\n\nimages = []\nfor _ in tqdm(range(9)):\n    images += [i for i in ddpm(batch_size=100).images]\n\n\n\n\nCPU times: user 13min 48s, sys: 272 ms, total: 13min 49s\nWall time: 13min 48s\n\n\nVisualize the first image:\n\nimages[0]\n\n\n\n\n\n\n\n\n\n\n\n%%time\n# Save images\n\nfrom pathlib import Path\nimport numpy as np\nfrom cleanfid import fid\n\ngen_path = Path(\"results/sampled_img_100\")\ngen_path.mkdir(exist_ok=True)\n\nfor i, img in enumerate(images[:100]):\n    img_np = np.array(img)\n    np.save(gen_path / f\"{i}.npy\", img_np)\n\nscore = fid.compute_fid(\n            str(gen_path),\n            dataset_name=\"cifar10\",\n            dataset_res=32,\n            device=\"cuda\",\n            mode=\"clean\",\n            batch_size=2,\n        )\n\nprint(\"FID score:\", score)  # 156.41708381872482\n\ncompute FID of a folder with cifar10 statistics\nFound 100 images in the folder results/sampled_img_100\n\n\n\nFID sampled_img_100 :   0%|                                                                     | 0/50 [00:00&lt;?, ?it/s]\nFID sampled_img_100 :   2%|█▏                                                           | 1/50 [00:00&lt;00:24,  1.96it/s]\nFID sampled_img_100 :   4%|██▍                                                          | 2/50 [00:02&lt;01:19,  1.66s/it]\nFID sampled_img_100 :  40%|████████████████████████                                    | 20/50 [00:03&lt;00:03,  9.26it/s]\nFID sampled_img_100 :  78%|██████████████████████████████████████████████▊             | 39/50 [00:03&lt;00:00, 20.85it/s]\nFID sampled_img_100 : 100%|████████████████████████████████████████████████████████████| 50/50 [00:03&lt;00:00, 15.31it/s]\n\n\nFID score: 155.1173252056065\nCPU times: user 57 s, sys: 7.68 s, total: 1min 4s\nWall time: 6.18 s\n\n\n\n\n\n\n\n%%time\n# Save images\n\ngen_path = Path(\"results/sampled_img_1000\")\ngen_path.mkdir(exist_ok=True)\n\nfor i, img in enumerate(images[:1000]):\n    img_np = np.array(img)\n    np.save(gen_path / f\"{i}.npy\", img_np)\n\nCPU times: user 341 ms, sys: 905 ms, total: 1.25 s\nWall time: 166 ms\n\n\n\n%%time\nfrom cleanfid import fid\n\nscore = fid.compute_fid(\n            str(gen_path),\n            dataset_name=\"cifar10\",\n            dataset_res=32,\n            device=\"cuda\",\n            mode=\"clean\",\n            batch_size=2,\n            verbose=False,\n        )\n\nprint(\"FID score:\", score)  # 44.02850054643022\n\nFID score: 47.59607117774948\nCPU times: user 50.7 s, sys: 7.36 s, total: 58.1 s\nWall time: 7.92 s\n\n\n\n\n\n\n%%time\nfor _ in tqdm(range(490)):\n    images += [i for i in ddpm(batch_size=100).images]\n\n\n\n\nCPU times: user 12h 36min 39s, sys: 6.73 s, total: 12h 36min 45s\nWall time: 12h 36min 32s\n\n\n\ngen_path = Path(\"results/sampled_img_50k\")\ngen_path.mkdir(exist_ok=True)\n\nfor i, img in enumerate(images[:50000]):\n    img_np = np.array(img)\n    np.save(gen_path / f\"{i}.npy\", img_np)\n\n\n%%time\nscore = fid.compute_fid(\n            str(gen_path),\n            dataset_name=\"cifar10\",\n            dataset_res=32,\n            device=\"cuda\",\n            mode=\"clean\",\n            batch_size=2,\n            verbose=False,\n        )\n\nCPU times: user 2min 53s, sys: 18.6 s, total: 3min 11s\nWall time: 2min 26s\n\n\n\nprint(\"FID score:\", score)\n\nFID score: 14.702197781209179\n\n\nAs you can see, FID score is sensitive to the sample size. From 100 to 1000 and then to 50k sampled images, we see the FID score decreases from 150 to about 50 and then to roughly 10. The recommended sample size is 50k, however, it takes a long time to sample.",
    "crumbs": [
      "Home",
      "Generating Animal Face Images",
      "Quality evaluation of an image generation model"
    ]
  },
  {
    "objectID": "2_2_fid.html#fid",
    "href": "2_2_fid.html#fid",
    "title": "Quality evaluation of an image generation model",
    "section": "",
    "text": "The Chamfer distance that we used in the previous notebook is a distance between point clouds. It is based on Euclidean distance, which works for low-dimensional data such as 2D points. For high-dimensional data such as images, the Euclidean distance is not a good measure, because the distance between two images can be high even if they are very similar.\nThe Fréchet Inception Distance (FID) is a metric that measures the distance between two distributions of images. A lower score means better quality. It is based on the Fréchet distance, which is a distance between Gaussian distributions. “Inception” here refers to a pre-trained Inception-v3 model, which is a convolutional neural network that was trained to classify images into 1000 categories. We can use the intermediate features of this model to measure the distance between two distributions of images.\nThe limitations of FID include:\n\nIt assumes Gaussian distribution of features.\nIt is sensitive to sample size and requires large sample sizes for stable estimates, which means it is computationally expensive.\n\nOther metrics include:\n\nInception Score (IS): measures both the diversity and quality of generated images. It may give misleading scores for specialized datasets (e.g., medical images, abstract art).\nLPIPS (Learned Perceptual Image Patch Similarity). It is good at measureing individual image quality, and more suitable for tasks like style transfer, image reconstruction.\nPSNR (Peak Signal-to-Noise Ratio). It measures pixel-level reconstruction quality,and does not measure perceptual quality.\nSSIM (Structural Similarity Index). It measures image quality based on luminance and contrast, and does not measure perceptual quality.\nArena-style human evaluation: compare generated images from different models side by side, and ask human experts to choose the better one.\n\nWe are going to use the packageclean-fid. As a santity check for the metric, let’s take a pre-trained model from huggingface with known good quality, and see what the FID score looks like.\n\n\n\n!pip install clean-fid\n\nDefaulting to user installation because normal site-packages is not writeable\nWARNING: Ignoring invalid distribution ~vidia-cusparse-cu12 (/home/ubuntu/.local/lib/python3.11/site-packages)\nWARNING: Ignoring invalid distribution ~vidia-cusparse-cu12 (/home/ubuntu/.local/lib/python3.11/site-packages)\nRequirement already satisfied: clean-fid in /home/ubuntu/.local/lib/python3.11/site-packages (0.1.35)\nRequirement already satisfied: torch in /home/ubuntu/.local/lib/python3.11/site-packages (from clean-fid) (2.5.1)\nRequirement already satisfied: torchvision in /home/ubuntu/.local/lib/python3.11/site-packages (from clean-fid) (0.20.1)\nRequirement already satisfied: numpy&gt;=1.14.3 in /home/ubuntu/.local/lib/python3.11/site-packages (from clean-fid) (1.26.4)\nRequirement already satisfied: scipy&gt;=1.0.1 in /home/ubuntu/.local/lib/python3.11/site-packages (from clean-fid) (1.14.0)\nRequirement already satisfied: tqdm&gt;=4.28.1 in /home/ubuntu/.local/lib/python3.11/site-packages (from clean-fid) (4.66.5)\nRequirement already satisfied: pillow&gt;=8.1 in /home/ubuntu/.local/lib/python3.11/site-packages (from clean-fid) (10.2.0)\nRequirement already satisfied: requests in /home/ubuntu/.local/lib/python3.11/site-packages (from clean-fid) (2.32.3)\nRequirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /opt/miniconda/lib/python3.11/site-packages (from requests-&gt;clean-fid) (2.0.4)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in /opt/miniconda/lib/python3.11/site-packages (from requests-&gt;clean-fid) (3.4)\nRequirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in /opt/miniconda/lib/python3.11/site-packages (from requests-&gt;clean-fid) (1.26.18)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /opt/miniconda/lib/python3.11/site-packages (from requests-&gt;clean-fid) (2023.7.22)\nRequirement already satisfied: filelock in /home/ubuntu/.local/lib/python3.11/site-packages (from torch-&gt;clean-fid) (3.13.3)\nRequirement already satisfied: typing-extensions&gt;=4.8.0 in /home/ubuntu/.local/lib/python3.11/site-packages (from torch-&gt;clean-fid) (4.10.0)\nRequirement already satisfied: networkx in /home/ubuntu/.local/lib/python3.11/site-packages (from torch-&gt;clean-fid) (3.2.1)\nRequirement already satisfied: jinja2 in /home/ubuntu/.local/lib/python3.11/site-packages (from torch-&gt;clean-fid) (3.1.4)\nRequirement already satisfied: fsspec in /home/ubuntu/.local/lib/python3.11/site-packages (from torch-&gt;clean-fid) (2024.3.1)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/ubuntu/.local/lib/python3.11/site-packages (from torch-&gt;clean-fid) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/ubuntu/.local/lib/python3.11/site-packages (from torch-&gt;clean-fid) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/ubuntu/.local/lib/python3.11/site-packages (from torch-&gt;clean-fid) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/ubuntu/.local/lib/python3.11/site-packages (from torch-&gt;clean-fid) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/ubuntu/.local/lib/python3.11/site-packages (from torch-&gt;clean-fid) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/ubuntu/.local/lib/python3.11/site-packages (from torch-&gt;clean-fid) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/ubuntu/.local/lib/python3.11/site-packages (from torch-&gt;clean-fid) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/ubuntu/.local/lib/python3.11/site-packages (from torch-&gt;clean-fid) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/ubuntu/.local/lib/python3.11/site-packages (from torch-&gt;clean-fid) (12.3.1.170)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/ubuntu/.local/lib/python3.11/site-packages (from torch-&gt;clean-fid) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/ubuntu/.local/lib/python3.11/site-packages (from torch-&gt;clean-fid) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/ubuntu/.local/lib/python3.11/site-packages (from torch-&gt;clean-fid) (12.4.127)\nRequirement already satisfied: triton==3.1.0 in /home/ubuntu/.local/lib/python3.11/site-packages (from torch-&gt;clean-fid) (3.1.0)\nRequirement already satisfied: sympy==1.13.1 in /home/ubuntu/.local/lib/python3.11/site-packages (from torch-&gt;clean-fid) (1.13.1)\nRequirement already satisfied: mpmath&lt;1.4,&gt;=1.1.0 in /home/ubuntu/.local/lib/python3.11/site-packages (from sympy==1.13.1-&gt;torch-&gt;clean-fid) (1.3.0)\nRequirement already satisfied: MarkupSafe&gt;=2.0 in /home/ubuntu/.local/lib/python3.11/site-packages (from jinja2-&gt;torch-&gt;clean-fid) (2.1.5)\nWARNING: Ignoring invalid distribution ~vidia-cusparse-cu12 (/home/ubuntu/.local/lib/python3.11/site-packages)\nWARNING: Ignoring invalid distribution ~vidia-cusparse-cu12 (/home/ubuntu/.local/lib/python3.11/site-packages)\nWARNING: Ignoring invalid distribution ~vidia-cusparse-cu12 (/home/ubuntu/.local/lib/python3.11/site-packages)\nWARNING: Ignoring invalid distribution ~vidia-cusparse-cu12 (/home/ubuntu/.local/lib/python3.11/site-packages)\n\n\n\n\n\nWe are using the model released by Google, which is trained on CIFAR-10 dataset.\n\nimport torch, transformers, diffusers, xformers, torchvision\nprint(\"torch version:\", torch.__version__)\nprint(\"transformers version:\", transformers.__version__)\nprint(\"diffusers version:\", diffusers.__version__)\nprint(\"xformers version:\", xformers.__version__)\nprint(\"torchvision version:\", torchvision.__version__)\n\ntorch version: 2.5.1+cu124\ntransformers version: 4.47.0\ndiffusers version: 0.31.0\nxformers version: 0.0.28.post3\ntorchvision version: 0.20.1+cu124\n\n\n\n%%time\nfrom diffusers import DDPMPipeline, DDIMPipeline, PNDMPipeline\n\nmodel_id = \"google/ddpm-cifar10-32\"\n\n# load model and scheduler\nddpm = DDPMPipeline.from_pretrained(model_id)  # you can replace DDPMPipeline with DDIMPipeline or PNDMPipeline for faster inference\nddpm.set_progress_bar_config(disable=True)\n\n2024-12-16 22:00:47.499784: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n2024-12-16 22:00:47.512904: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-12-16 22:00:47.527894: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-12-16 22:00:47.532473: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2024-12-16 22:00:47.543787: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2024-12-16 22:00:48.384524: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n\n\n\n\n\nAn error occurred while trying to fetch /home/ubuntu/.cache/huggingface/hub/models--google--ddpm-cifar10-32/snapshots/267b167dc01f0e4e61923ea244e8b988f84deb80: Error no file named diffusion_pytorch_model.safetensors found in directory /home/ubuntu/.cache/huggingface/hub/models--google--ddpm-cifar10-32/snapshots/267b167dc01f0e4e61923ea244e8b988f84deb80.\nDefaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n\n\nCPU times: user 2.57 s, sys: 639 ms, total: 3.21 s\nWall time: 3.51 s\n\n\n\n\n\n\n%%time\n# run pipeline in inference (sample random noise and denoise)\nfrom tqdm.notebook import tqdm\nddpm.to(\"cuda\")\n\nimages = []\nfor _ in tqdm(range(9)):\n    images += [i for i in ddpm(batch_size=100).images]\n\n\n\n\nCPU times: user 13min 48s, sys: 272 ms, total: 13min 49s\nWall time: 13min 48s\n\n\nVisualize the first image:\n\nimages[0]\n\n\n\n\n\n\n\n\n\n\n\n%%time\n# Save images\n\nfrom pathlib import Path\nimport numpy as np\nfrom cleanfid import fid\n\ngen_path = Path(\"results/sampled_img_100\")\ngen_path.mkdir(exist_ok=True)\n\nfor i, img in enumerate(images[:100]):\n    img_np = np.array(img)\n    np.save(gen_path / f\"{i}.npy\", img_np)\n\nscore = fid.compute_fid(\n            str(gen_path),\n            dataset_name=\"cifar10\",\n            dataset_res=32,\n            device=\"cuda\",\n            mode=\"clean\",\n            batch_size=2,\n        )\n\nprint(\"FID score:\", score)  # 156.41708381872482\n\ncompute FID of a folder with cifar10 statistics\nFound 100 images in the folder results/sampled_img_100\n\n\n\nFID sampled_img_100 :   0%|                                                                     | 0/50 [00:00&lt;?, ?it/s]\nFID sampled_img_100 :   2%|█▏                                                           | 1/50 [00:00&lt;00:24,  1.96it/s]\nFID sampled_img_100 :   4%|██▍                                                          | 2/50 [00:02&lt;01:19,  1.66s/it]\nFID sampled_img_100 :  40%|████████████████████████                                    | 20/50 [00:03&lt;00:03,  9.26it/s]\nFID sampled_img_100 :  78%|██████████████████████████████████████████████▊             | 39/50 [00:03&lt;00:00, 20.85it/s]\nFID sampled_img_100 : 100%|████████████████████████████████████████████████████████████| 50/50 [00:03&lt;00:00, 15.31it/s]\n\n\nFID score: 155.1173252056065\nCPU times: user 57 s, sys: 7.68 s, total: 1min 4s\nWall time: 6.18 s\n\n\n\n\n\n\n\n%%time\n# Save images\n\ngen_path = Path(\"results/sampled_img_1000\")\ngen_path.mkdir(exist_ok=True)\n\nfor i, img in enumerate(images[:1000]):\n    img_np = np.array(img)\n    np.save(gen_path / f\"{i}.npy\", img_np)\n\nCPU times: user 341 ms, sys: 905 ms, total: 1.25 s\nWall time: 166 ms\n\n\n\n%%time\nfrom cleanfid import fid\n\nscore = fid.compute_fid(\n            str(gen_path),\n            dataset_name=\"cifar10\",\n            dataset_res=32,\n            device=\"cuda\",\n            mode=\"clean\",\n            batch_size=2,\n            verbose=False,\n        )\n\nprint(\"FID score:\", score)  # 44.02850054643022\n\nFID score: 47.59607117774948\nCPU times: user 50.7 s, sys: 7.36 s, total: 58.1 s\nWall time: 7.92 s\n\n\n\n\n\n\n%%time\nfor _ in tqdm(range(490)):\n    images += [i for i in ddpm(batch_size=100).images]\n\n\n\n\nCPU times: user 12h 36min 39s, sys: 6.73 s, total: 12h 36min 45s\nWall time: 12h 36min 32s\n\n\n\ngen_path = Path(\"results/sampled_img_50k\")\ngen_path.mkdir(exist_ok=True)\n\nfor i, img in enumerate(images[:50000]):\n    img_np = np.array(img)\n    np.save(gen_path / f\"{i}.npy\", img_np)\n\n\n%%time\nscore = fid.compute_fid(\n            str(gen_path),\n            dataset_name=\"cifar10\",\n            dataset_res=32,\n            device=\"cuda\",\n            mode=\"clean\",\n            batch_size=2,\n            verbose=False,\n        )\n\nCPU times: user 2min 53s, sys: 18.6 s, total: 3min 11s\nWall time: 2min 26s\n\n\n\nprint(\"FID score:\", score)\n\nFID score: 14.702197781209179\n\n\nAs you can see, FID score is sensitive to the sample size. From 100 to 1000 and then to 50k sampled images, we see the FID score decreases from 150 to about 50 and then to roughly 10. The recommended sample size is 50k, however, it takes a long time to sample.",
    "crumbs": [
      "Home",
      "Generating Animal Face Images",
      "Quality evaluation of an image generation model"
    ]
  },
  {
    "objectID": "1_2_Flow Matching 2D Toy.html",
    "href": "1_2_Flow Matching 2D Toy.html",
    "title": "Training a Flow Matching Model for 2D Point Generation",
    "section": "",
    "text": "open in colab\nIn this tutorial, you’ll learn how to train a flow matching model to generate spirals of 2-D points.\nFlow matching models provide an alternative approach to generative modeling by learning a continuous transformation that maps noise to structured data. This method is closely related to diffusion models but differs in how clean data transitions to noisy data and vice versa.\nThis tutorial is similar to the diffusion tutorial. We’ll explore the different components of flow matching models, as well as compare the quality of generated results using different model architectures.",
    "crumbs": [
      "Home",
      "Generating a 2D Point Cloud",
      "Flow Matching",
      "Training a Flow Matching Model for 2D Point Generation"
    ]
  },
  {
    "objectID": "1_2_Flow Matching 2D Toy.html#what-youll-learn",
    "href": "1_2_Flow Matching 2D Toy.html#what-youll-learn",
    "title": "Training a Flow Matching Model for 2D Point Generation",
    "section": "What you’ll learn",
    "text": "What you’ll learn\n\nHow flow matching works conceptually\nThree different model architectures for implementing flow matching\nHow to visualize and evaluate the generated results\nBest practices for training flow matching models",
    "crumbs": [
      "Home",
      "Generating a 2D Point Cloud",
      "Flow Matching",
      "Training a Flow Matching Model for 2D Point Generation"
    ]
  },
  {
    "objectID": "1_2_Flow Matching 2D Toy.html#prerequisites",
    "href": "1_2_Flow Matching 2D Toy.html#prerequisites",
    "title": "Training a Flow Matching Model for 2D Point Generation",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nBasic understanding of PyTorch\nFamiliarity with neural networks and optimization\n\n\nfrom dataclasses import dataclass\nimport math\nfrom typing import Dict, Tuple\n\nfrom matplotlib import pyplot as plt\nfrom sklearn.datasets import make_swiss_roll\nimport torch\nfrom torch import nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom torch.nn import MSELoss\nfrom torch.utils.data import DataLoader\nfrom tqdm.auto import tqdm\nfrom torchdyn.core import NeuralODE",
    "crumbs": [
      "Home",
      "Generating a 2D Point Cloud",
      "Flow Matching",
      "Training a Flow Matching Model for 2D Point Generation"
    ]
  },
  {
    "objectID": "1_2_Flow Matching 2D Toy.html#configuration-and-dataloaders",
    "href": "1_2_Flow Matching 2D Toy.html#configuration-and-dataloaders",
    "title": "Training a Flow Matching Model for 2D Point Generation",
    "section": "Configuration and Dataloaders",
    "text": "Configuration and Dataloaders\nBefore training the model, we need to define some hyperparameters and create our train and validation dataloaders.\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n@dataclass\nclass TrainingConfig:\n    batch_size: int = 256 # batch size\n    learning_rate: float = 5e-4 # initial learning rate\n    weight_decay: float = 1e-6 # weight decay\n    num_denoising_steps: int = 1000 # number of timesteps\n\n\ndef load_data(config: TrainingConfig) -&gt; Tuple[DataLoader, DataLoader]:\n    \"\"\"\n    Load the data and return the train and validation dataloaders.\n\n    Args:\n      config: TrainingConfig object.\n    Returns:\n      train_dataloader: DataLoader for the training data.\n      val_dataloader: DataLoader for the validation data.\n    \"\"\"\n    n = int(1e+6)\n    x, _ = make_swiss_roll(n_samples=n, noise=0)\n    x = x[:, [0, 2]]\n    scaling = 2\n    x = (x - x.mean()) / x.std() * scaling\n    x_train = x[:int(n * 0.8), :]\n    x_val = x[int(n * 0.8):, :]\n\n    class SimpleDataset:\n      def __init__(self, data):\n        self.data = data\n\n      def __len__(self):\n        return len(self.data)\n\n      def __getitem__(self, i):\n        return self.data[i]\n\n    train_dataset = SimpleDataset(x_train)\n    val_dataset = SimpleDataset(x_val)\n    train_dataloader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True, num_workers=0)\n    val_dataloader = DataLoader(val_dataset, batch_size=config.batch_size, shuffle=False, num_workers=0)\n\n    return train_dataloader, val_dataloader\n\n\nconfig = TrainingConfig()\ntrain_dataloader, val_dataloader = load_data(config)\nfirst_batch = next(iter(train_dataloader))\nprint(\"batch shape:\", first_batch.shape)\n\nbatch shape: torch.Size([256, 2])",
    "crumbs": [
      "Home",
      "Generating a 2D Point Cloud",
      "Flow Matching",
      "Training a Flow Matching Model for 2D Point Generation"
    ]
  },
  {
    "objectID": "1_2_Flow Matching 2D Toy.html#a-simple-flow-plan",
    "href": "1_2_Flow Matching 2D Toy.html#a-simple-flow-plan",
    "title": "Training a Flow Matching Model for 2D Point Generation",
    "section": "A Simple Flow Plan",
    "text": "A Simple Flow Plan\nGiven a clean data point \\(x_1\\) and random noise \\(x_0\\) with the same shape, we can simply use a straight line to move \\(x_1\\) to \\(x_0\\) or vice versa. If we allow for \\(T=100\\) time steps between \\(t=0\\) and \\(t=1\\), the following sequence can be produced:\n\\[x_t = t \\cdot x_1 + (1 - t) \\cdot x_0\\]\nA ConditionalFlowMatcher class is responsible for drawing samples from the flow plan, and computing the conditional flow: a vector/velocity field that moves the data points from \\(x_0\\) to \\(x_1\\):\n\\[u_t(x|x_0, x_1) = x_1 - x_0\\]\nThis is the desired velocity at time \\(t\\) that we want the denoising model to learn. Since the velocity stays constant with respect to \\(x\\) and \\(t\\), the path is a straight line, as we will visualize in a later section.\nBelow is a simple example of a conditional flow matcher, where each data point is paired with a random noise data point.\n\nConditional Flow Matcher\n\nfrom typing import Union\n\n\ndef pad_t_like_x(t, x):\n    \"\"\"Function to reshape the time vector t by the number of dimensions of x.\n\n    Parameters\n    ----------\n    x : Tensor, shape (bs, *dim)\n        represents the source minibatch\n    t : FloatTensor, shape (bs)\n\n    Returns\n    -------\n    t : Tensor, shape (bs, number of x dimensions)\n\n    Example\n    -------\n    x: Tensor (bs, C, W, H)\n    t: Vector (bs)\n    pad_t_like_x(t, x): Tensor (bs, 1, 1, 1)\n    \"\"\"\n    if isinstance(t, (float, int)):\n        return t\n    return t.reshape(-1, *([1] * (x.dim() - 1)))\n\n\nclass ConditionalFlowMatcher:\n    \"\"\"Base class for conditional flow matching methods. This class implements the independent\n    conditional flow matching methods from [1] and serves as a parent class for all other flow\n    matching methods.\n\n    It implements:\n    - Drawing data from gaussian probability path N(t * x1 + (1 - t) * x0, sigma) function\n    - conditional flow matching ut(x1|x0) = x1 - x0\n    - score function $\\nabla log p_t(x|x0, x1)$\n    \"\"\"\n\n    def __init__(self, sigma: Union[float, int] = 0.0):\n        r\"\"\"Initialize the ConditionalFlowMatcher class. It requires the hyper-parameter $\\sigma$.\n\n        Parameters\n        ----------\n        sigma : Union[float, int]\n        \"\"\"\n        self.sigma = sigma\n\n    def compute_mu_t(self, x0, x1, t):\n        \"\"\"\n        Compute the mean of the probability path N(t * x1 + (1 - t) * x0, sigma), see (Eq.14) [1].\n\n        Parameters\n        ----------\n        x0 : Tensor, shape (bs, *dim)\n            represents the source minibatch\n        x1 : Tensor, shape (bs, *dim)\n            represents the target minibatch\n        t : FloatTensor, shape (bs)\n\n        Returns\n        -------\n        mean mu_t: t * x1 + (1 - t) * x0\n\n        References\n        ----------\n        [1] Improving and Generalizing Flow-Based Generative Models with minibatch optimal transport, Preprint, Tong et al.\n        \"\"\"\n        t = pad_t_like_x(t, x0)\n        return t * x1 + (1 - t) * x0\n\n    def compute_sigma_t(self, t):\n        \"\"\"\n        Compute the standard deviation of the probability path N(t * x1 + (1 - t) * x0, sigma), see (Eq.14) [1].\n\n        Parameters\n        ----------\n        t : FloatTensor, shape (bs)\n\n        Returns\n        -------\n        standard deviation sigma\n\n        References\n        ----------\n        [1] Improving and Generalizing Flow-Based Generative Models with minibatch optimal transport, Preprint, Tong et al.\n        \"\"\"\n        del t\n        return self.sigma\n\n    def sample_xt(self, x0, x1, t, epsilon):\n        \"\"\"\n        Draw a sample from the probability path N(t * x1 + (1 - t) * x0, sigma), see (Eq.14) [1].\n\n        Parameters\n        ----------\n        x0 : Tensor, shape (bs, *dim)\n            represents the source minibatch\n        x1 : Tensor, shape (bs, *dim)\n            represents the target minibatch\n        t : FloatTensor, shape (bs)\n        epsilon : Tensor, shape (bs, *dim)\n            noise sample from N(0, 1)\n\n        Returns\n        -------\n        xt : Tensor, shape (bs, *dim)\n\n        References\n        ----------\n        [1] Improving and Generalizing Flow-Based Generative Models with minibatch optimal transport, Preprint, Tong et al.\n        \"\"\"\n        mu_t = self.compute_mu_t(x0, x1, t)\n        sigma_t = self.compute_sigma_t(t)\n        sigma_t = pad_t_like_x(sigma_t, x0)\n        return mu_t + sigma_t * epsilon\n\n    def compute_conditional_flow(self, x0, x1, t, xt):\n        \"\"\"\n        Compute the conditional vector field ut(x1|x0) = x1 - x0, see Eq.(15) [1].\n\n        Parameters\n        ----------\n        x0 : Tensor, shape (bs, *dim)\n            represents the source minibatch\n        x1 : Tensor, shape (bs, *dim)\n            represents the target minibatch\n        t : FloatTensor, shape (bs)\n        xt : Tensor, shape (bs, *dim)\n            represents the samples drawn from probability path pt\n\n        Returns\n        -------\n        ut : conditional vector field ut(x1|x0) = x1 - x0\n\n        References\n        ----------\n        [1] Improving and Generalizing Flow-Based Generative Models with minibatch optimal transport, Preprint, Tong et al.\n        \"\"\"\n        del t, xt\n        return x1 - x0\n\n    def sample_noise_like(self, x):\n        return torch.randn_like(x)\n\n    def sample_location_and_conditional_flow(self, x0, x1, t=None, return_noise=False):\n        \"\"\"\n        Compute the sample xt (drawn from N(t * x1 + (1 - t) * x0, sigma))\n        and the conditional vector field ut(x1|x0) = x1 - x0, see Eq.(15) [1].\n\n        Parameters\n        ----------\n        x0 : Tensor, shape (bs, *dim)\n            represents the source minibatch\n        x1 : Tensor, shape (bs, *dim)\n            represents the target minibatch\n        (optionally) t : Tensor, shape (bs)\n            represents the time levels\n            if None, drawn from uniform [0,1]\n        return_noise : bool\n            return the noise sample epsilon\n\n\n        Returns\n        -------\n        t : FloatTensor, shape (bs)\n        xt : Tensor, shape (bs, *dim)\n            represents the samples drawn from probability path pt\n        ut : conditional vector field ut(x1|x0) = x1 - x0\n        (optionally) eps: Tensor, shape (bs, *dim) such that xt = mu_t + sigma_t * epsilon\n\n        References\n        ----------\n        [1] Improving and Generalizing Flow-Based Generative Models with minibatch optimal transport, Preprint, Tong et al.\n        \"\"\"\n        if t is None:\n            t = torch.rand(x0.shape[0]).type_as(x0)\n        assert len(t) == x0.shape[0], f\"t has to have batch size dimension, got {len(t)}\"\n\n        eps = self.sample_noise_like(x0)\n        xt = self.sample_xt(x0, x1, t, eps)\n        ut = self.compute_conditional_flow(x0, x1, t, xt)\n        if return_noise:\n            return t, xt, ut, eps\n        else:\n            return t, xt, ut\n\n    def compute_lambda(self, t):\n        \"\"\"Compute the lambda function, see Eq.(23) [3].\n\n        Parameters\n        ----------\n        t : FloatTensor, shape (bs)\n\n        Returns\n        -------\n        lambda : score weighting function\n\n        References\n        ----------\n        [4] Simulation-free Schrodinger bridges via score and flow matching, Preprint, Tong et al.\n        \"\"\"\n        sigma_t = self.compute_sigma_t(t)\n        return 2 * sigma_t / (self.sigma**2 + 1e-8)",
    "crumbs": [
      "Home",
      "Generating a 2D Point Cloud",
      "Flow Matching",
      "Training a Flow Matching Model for 2D Point Generation"
    ]
  },
  {
    "objectID": "1_2_Flow Matching 2D Toy.html#optimal-transport-flow-plan",
    "href": "1_2_Flow Matching 2D Toy.html#optimal-transport-flow-plan",
    "title": "Training a Flow Matching Model for 2D Point Generation",
    "section": "Optimal Transport Flow Plan",
    "text": "Optimal Transport Flow Plan\nA simple flow matcher assigns noisy data apoints to clean data points arbitrarily. We can do better than that. One option is to use optimal transport (OT). Intuitively, it pairs up clean data points to close-by noisy data points, to minimize travel. This helps define better flow trajectories, improving training stability and sample quality.\nThe ExactOptimalTransportConditionalFlowMatcher class implements this idea by computing an OT plan for mapping noisy samples to clean samples.\nFor context, optimal transport is a technique that finds the best way to transport mass from one distribution to another. In the context of flow matching, it is used to plan the route between the source and target distributions.\nGiven two point clouds: the clean data points \\(\\{x_1^i\\}_{i=1}^n\\) and the noisy data points \\(\\{x_0^j\\}_{j=1}^m\\), the OT plan is a matrix \\(P\\) that describes the best way to transport mass from \\(x_0\\) to \\(x_1\\). The matrix \\(P\\) is defined as:\n\\[P = \\arg \\min_{P} \\sum_{i,j} c(x_1^i, x_0^j) P_{i,j} \\quad \\text{s.t.} \\quad \\sum_{j=1}^m P_{i,j} = 1, \\quad \\sum_{i=1}^n P_{i,j} = 1\\]\nwhere \\(c(x_1^i, x_0^j)\\) is the cost of transporting mass from \\(x_1^i\\) to \\(x_0^j\\). In this case, we use the squared Euclidean distance, so \\(c(x_1^i, x_0^j) = \\|x_1^i - x_0^j\\|^2\\).\nOnce the OT plan is computed, the OTPlanSampler class is then used to draw samples from the flow plan \\(P\\). The samples are drawn from all the possible pairings between the source (noisy data points) and the target (clean data points).\n\nimport numpy as np\nimport ot as pot\nimport warnings\nfrom functools import partial\n\nclass OTPlanSampler:\n    \"\"\"OTPlanSampler implements sampling coordinates according to an OT plan (wrt squared Euclidean\n    cost) with different implementations of the plan calculation.\"\"\"\n\n    def __init__(\n        self,\n        method: str,\n        reg: float = 0.05,\n        reg_m: float = 1.0,\n        normalize_cost: bool = False,\n        num_threads: Union[int, str] = 1,\n        warn: bool = True,\n    ) -&gt; None:\n        \"\"\"Initialize the OTPlanSampler class.\n\n        Parameters\n        ----------\n        method: str\n            choose which optimal transport solver you would like to use.\n            Currently supported are [\"exact\", \"sinkhorn\", \"unbalanced\",\n            \"partial\"] OT solvers.\n        reg: float, optional\n            regularization parameter to use for Sinkhorn-based iterative solvers.\n        reg_m: float, optional\n            regularization weight for unbalanced Sinkhorn-knopp solver.\n        normalize_cost: bool, optional\n            normalizes the cost matrix so that the maximum cost is 1. Helps\n            stabilize Sinkhorn-based solvers. Should not be used in the vast\n            majority of cases.\n        num_threads: int or str, optional\n            number of threads to use for the \"exact\" OT solver. If \"max\", uses\n            the maximum number of threads.\n        warn: bool, optional\n            if True, raises a warning if the algorithm does not converge\n        \"\"\"\n        # ot_fn should take (a, b, M) as arguments where a, b are marginals and\n        # M is a cost matrix\n        if method == \"exact\":\n            self.ot_fn = partial(pot.emd, numThreads=num_threads)\n        elif method == \"sinkhorn\":\n            self.ot_fn = partial(pot.sinkhorn, reg=reg)\n        elif method == \"unbalanced\":\n            self.ot_fn = partial(pot.unbalanced.sinkhorn_knopp_unbalanced, reg=reg, reg_m=reg_m)\n        elif method == \"partial\":\n            self.ot_fn = partial(pot.partial.entropic_partial_wasserstein, reg=reg)\n        else:\n            raise ValueError(f\"Unknown method: {method}\")\n        self.reg = reg\n        self.reg_m = reg_m\n        self.normalize_cost = normalize_cost\n        self.warn = warn\n\n    def get_map(self, x0, x1):\n        \"\"\"Compute the OT plan (wrt squared Euclidean cost) between a source and a target\n        minibatch.\n\n        Parameters\n        ----------\n        x0 : Tensor, shape (bs, *dim)\n            represents the source minibatch\n        x1 : Tensor, shape (bs, *dim)\n            represents the source minibatch\n\n        Returns\n        -------\n        p : numpy array, shape (bs, bs)\n            represents the OT plan between minibatches\n        \"\"\"\n        a, b = pot.unif(x0.shape[0]), pot.unif(x1.shape[0])\n        if x0.dim() &gt; 2:\n            x0 = x0.reshape(x0.shape[0], -1)\n        if x1.dim() &gt; 2:\n            x1 = x1.reshape(x1.shape[0], -1)\n        x1 = x1.reshape(x1.shape[0], -1)\n        M = torch.cdist(x0, x1) ** 2\n        if self.normalize_cost:\n            M = M / M.max()  # should not be normalized when using minibatches\n        p = self.ot_fn(a, b, M.detach().cpu().numpy())\n        if not np.all(np.isfinite(p)):\n            print(\"ERROR: p is not finite\")\n            print(p)\n            print(\"Cost mean, max\", M.mean(), M.max())\n            print(x0, x1)\n        if np.abs(p.sum()) &lt; 1e-8:\n            if self.warn:\n                warnings.warn(\"Numerical errors in OT plan, reverting to uniform plan.\")\n            p = np.ones_like(p) / p.size\n        return p\n\n    def sample_map(self, pi, batch_size, replace=True):\n        r\"\"\"Draw source and target samples from pi  $(x,z) \\sim \\pi$\n\n        Parameters\n        ----------\n        pi : numpy array, shape (bs, bs)\n            represents the source minibatch\n        batch_size : int\n            represents the OT plan between minibatches\n        replace : bool\n            represents sampling or without replacement from the OT plan\n\n        Returns\n        -------\n        (i_s, i_j) : tuple of numpy arrays, shape (bs, bs)\n            represents the indices of source and target data samples from $\\pi$\n        \"\"\"\n        p = pi.flatten()\n        p = p / p.sum()\n        choices = np.random.choice(\n            pi.shape[0] * pi.shape[1], p=p, size=batch_size, replace=replace\n        )\n        return np.divmod(choices, pi.shape[1])\n\n    def sample_plan(self, x0, x1, replace=True):\n        r\"\"\"Compute the OT plan $\\pi$ (wrt squared Euclidean cost) between a source and a target\n        minibatch and draw source and target samples from pi $(x,z) \\sim \\pi$\n\n        Parameters\n        ----------\n        x0 : Tensor, shape (bs, *dim)\n            represents the source minibatch\n        x1 : Tensor, shape (bs, *dim)\n            represents the source minibatch\n        replace : bool\n            represents sampling or without replacement from the OT plan\n\n        Returns\n        -------\n        x0[i] : Tensor, shape (bs, *dim)\n            represents the source minibatch drawn from $\\pi$\n        x1[j] : Tensor, shape (bs, *dim)\n            represents the source minibatch drawn from $\\pi$\n        \"\"\"\n        pi = self.get_map(x0, x1)\n        i, j = self.sample_map(pi, x0.shape[0], replace=replace)\n        return x0[i], x1[j]\n\n    def sample_plan_with_labels(self, x0, x1, y0=None, y1=None, replace=True):\n        r\"\"\"Compute the OT plan $\\pi$ (wrt squared Euclidean cost) between a source and a target\n        minibatch and draw source and target labeled samples from pi $(x,z) \\sim \\pi$\n\n        Parameters\n        ----------\n        x0 : Tensor, shape (bs, *dim)\n            represents the source minibatch\n        x1 : Tensor, shape (bs, *dim)\n            represents the target minibatch\n        y0 : Tensor, shape (bs)\n            represents the source label minibatch\n        y1 : Tensor, shape (bs)\n            represents the target label minibatch\n        replace : bool\n            represents sampling or without replacement from the OT plan\n\n        Returns\n        -------\n        x0[i] : Tensor, shape (bs, *dim)\n            represents the source minibatch drawn from $\\pi$\n        x1[j] : Tensor, shape (bs, *dim)\n            represents the target minibatch drawn from $\\pi$\n        y0[i] : Tensor, shape (bs, *dim)\n            represents the source label minibatch drawn from $\\pi$\n        y1[j] : Tensor, shape (bs, *dim)\n            represents the target label minibatch drawn from $\\pi$\n        \"\"\"\n        pi = self.get_map(x0, x1)\n        i, j = self.sample_map(pi, x0.shape[0], replace=replace)\n        return (\n            x0[i],\n            x1[j],\n            y0[i] if y0 is not None else None,\n            y1[j] if y1 is not None else None,\n        )\n\n    def sample_trajectory(self, X):\n        \"\"\"Compute the OT trajectories between different sample populations moving from the source\n        to the target distribution.\n\n        Parameters\n        ----------\n        X : Tensor, (bs, times, *dim)\n            different populations of samples moving from the source to the target distribution.\n\n        Returns\n        -------\n        to_return : Tensor, (bs, times, *dim)\n            represents the OT sampled trajectories over time.\n        \"\"\"\n        times = X.shape[1]\n        pis = []\n        for t in range(times - 1):\n            pis.append(self.get_map(X[:, t], X[:, t + 1]))\n\n        indices = [np.arange(X.shape[0])]\n        for pi in pis:\n            j = []\n            for i in indices[-1]:\n                j.append(np.random.choice(pi.shape[1], p=pi[i] / pi[i].sum()))\n            indices.append(np.array(j))\n\n        to_return = []\n        for t in range(times):\n            to_return.append(X[:, t][indices[t]])\n        to_return = np.stack(to_return, axis=1)\n        return to_return\n\n\nclass ExactOptimalTransportConditionalFlowMatcher(ConditionalFlowMatcher):\n    \"\"\"Child class for optimal transport conditional flow matching method. This class implements\n    the OT-CFM methods from [1] and inherits the ConditionalFlowMatcher parent class.\n\n    It overrides the sample_location_and_conditional_flow.\n    \"\"\"\n\n    def __init__(self, sigma: Union[float, int] = 0.0):\n        r\"\"\"Initialize the ConditionalFlowMatcher class. It requires the hyper-parameter $\\sigma$.\n\n        Parameters\n        ----------\n        sigma : Union[float, int]\n        ot_sampler: exact OT method to draw couplings (x0, x1) (see Eq.(17) [1]).\n        \"\"\"\n        super().__init__(sigma)\n        self.ot_sampler = OTPlanSampler(method=\"exact\")\n\n    def sample_location_and_conditional_flow(self, x0, x1, t=None, return_noise=False):\n        r\"\"\"\n        Compute the sample xt (drawn from N(t * x1 + (1 - t) * x0, sigma))\n        and the conditional vector field ut(x1|x0) = x1 - x0, see Eq.(15) [1]\n        with respect to the minibatch OT plan $\\Pi$.\n\n        Parameters\n        ----------\n        x0 : Tensor, shape (bs, *dim)\n            represents the source minibatch\n        x1 : Tensor, shape (bs, *dim)\n            represents the target minibatch\n        (optionally) t : Tensor, shape (bs)\n            represents the time levels\n            if None, drawn from uniform [0,1]\n        return_noise : bool\n            return the noise sample epsilon\n\n        Returns\n        -------\n        t : FloatTensor, shape (bs)\n        xt : Tensor, shape (bs, *dim)\n            represents the samples drawn from probability path pt\n        ut : conditional vector field ut(x1|x0) = x1 - x0\n        (optionally) epsilon : Tensor, shape (bs, *dim) such that xt = mu_t + sigma_t * epsilon\n\n        References\n        ----------\n        [1] Improving and Generalizing Flow-Based Generative Models with minibatch optimal transport, Preprint, Tong et al.\n        \"\"\"\n        x0, x1 = self.ot_sampler.sample_plan(x0, x1)\n        return super().sample_location_and_conditional_flow(x0, x1, t, return_noise)\n\n    def guided_sample_location_and_conditional_flow(\n        self, x0, x1, y0=None, y1=None, t=None, return_noise=False\n    ):\n        r\"\"\"\n        Compute the sample xt (drawn from N(t * x1 + (1 - t) * x0, sigma))\n        and the conditional vector field ut(x1|x0) = x1 - x0, see Eq.(15) [1]\n        with respect to the minibatch OT plan $\\Pi$.\n\n        Parameters\n        ----------\n        x0 : Tensor, shape (bs, *dim)\n            represents the source minibatch\n        x1 : Tensor, shape (bs, *dim)\n            represents the target minibatch\n        y0 : Tensor, shape (bs) (default: None)\n            represents the source label minibatch\n        y1 : Tensor, shape (bs) (default: None)\n            represents the target label minibatch\n        (optionally) t : Tensor, shape (bs)\n            represents the time levels\n            if None, drawn from uniform [0,1]\n        return_noise : bool\n            return the noise sample epsilon\n\n        Returns\n        -------\n        t : FloatTensor, shape (bs)\n        xt : Tensor, shape (bs, *dim)\n            represents the samples drawn from probability path pt\n        ut : conditional vector field ut(x1|x0) = x1 - x0\n        (optionally) epsilon : Tensor, shape (bs, *dim) such that xt = mu_t + sigma_t * epsilon\n\n        References\n        ----------\n        [1] Improving and Generalizing Flow-Based Generative Models with minibatch optimal transport, Preprint, Tong et al.\n        \"\"\"\n        x0, x1, y0, y1 = self.ot_sampler.sample_plan_with_labels(x0, x1, y0, y1)\n        if return_noise:\n            t, xt, ut, eps = super().sample_location_and_conditional_flow(x0, x1, t, return_noise)\n            return t, xt, ut, y0, y1, eps\n        else:\n            t, xt, ut = super().sample_location_and_conditional_flow(x0, x1, t, return_noise)\n            return t, xt, ut, y0, y1\n\n2025-03-11 18:48:24.221049: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2025-03-11 18:48:24.221086: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2025-03-11 18:48:24.222585: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2025-03-11 18:48:25.048985: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT",
    "crumbs": [
      "Home",
      "Generating a 2D Point Cloud",
      "Flow Matching",
      "Training a Flow Matching Model for 2D Point Generation"
    ]
  },
  {
    "objectID": "1_2_Flow Matching 2D Toy.html#visualizing-the-flow-matching-plan",
    "href": "1_2_Flow Matching 2D Toy.html#visualizing-the-flow-matching-plan",
    "title": "Training a Flow Matching Model for 2D Point Generation",
    "section": "Visualizing the flow matching plan",
    "text": "Visualizing the flow matching plan\nHere we visualize the paths from \\(x_0\\) to \\(x_1\\) according to the simple flow plan, and compare them with the optimal transport plan.\n\ndef plot_flow_matching_trajectories(FM, n_examples=30):\n    # set a commmon xlim and ylim\n    xlim = [-3.7, 3.7]\n    ylim = [-3.7, 3.7]\n    # clean data (spiral)\n    x_clean = next(iter(val_dataloader))[:n_examples]\n\n    # noisy data\n    x_noisy = torch.randn_like(x_clean)\n\n    # Sample multiple timesteps to visualize flow progression\n    num_timesteps = 10\n    t_grid = torch.linspace(0, 1, num_timesteps)\n    x_ts = []\n    u_ts = []\n\n    for t in t_grid:\n        t_batch = t.repeat(x_clean.shape[0])\n        np.random.seed(0)\n        t, x_t, u_t = FM.sample_location_and_conditional_flow(x0=x_noisy, x1=x_clean, t=t_batch)\n        x_ts.append(x_t.detach().cpu().numpy())\n        u_ts.append(u_t.detach().cpu().numpy())\n\n    # Create visualization\n    fig, axs = plt.subplots(2, 2, figsize=(12, 12))\n\n    # Plot initial and final distributions\n    axs[0, 0].scatter(x_noisy[:, 0], x_noisy[:, 1], color='blue', alpha=0.5, label='Initial')\n    axs[0, 0].scatter(x_clean[:, 0], x_clean[:, 1], color='red', alpha=0.5, label='Target') \n    axs[0, 0].set_title(\"Initial and Target Distributions\")\n    axs[0, 0].legend()\n    axs[0, 0].set_xlim(xlim)\n    axs[0, 0].set_ylim(ylim)\n\n    # ======================================\n    ## Now show the full trajectories\n    # ======================================\n    axs[0, 1].scatter(x_ts[-1][:, 0], x_ts[-1][:, 1], color='red', alpha=0.5, label='Target (t=1)')\n    axs[0, 1].scatter(x_ts[0][:, 0], x_ts[0][:, 1], color='blue', alpha=0.5, label='Initial (t=0)')\n    \n    # Sample a few timesteps to show progression\n    sample_ts = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n    for t in sample_ts:\n        current_points = np.array(x_ts[t])\n        prev_points = np.array(x_ts[t-1])\n        # Calculate color interpolation factor (0 to 1)\n        factor = t / (len(x_ts)-1)\n        # Interpolate between blue (0,0,1) and red (1,0,0)\n        color = (factor, 0, 1-factor)\n        axs[0, 1].scatter(current_points[:, 0], current_points[:, 1], color=color, alpha=0.3, label=f't=0.{t}')\n        # connect the dots: previous point to current point\n        assert prev_points.shape == (n_examples, 2), f\"prev_traj.shape={prev_points.shape}\"\n        \n        for i_point in range(len(current_points)):\n            # Plot the line segment from previous point to current point\n            axs[0, 1].plot(\n                [prev_points[i_point, 0], current_points[i_point, 0]],\n                [prev_points[i_point, 1], current_points[i_point, 1]],\n                'k-', alpha=0.1\n            )\n    \n    axs[0, 1].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n    axs[0, 1].set_title(\"Flow Matching Full Trajectories\")\n    axs[0, 1].set_xlim(xlim)\n    axs[0, 1].set_ylim(ylim)\n\n    # Plot vector field at t=0\n    x_0 = x_ts[0]\n    u_0 = u_ts[0]\n    axs[1, 0].quiver(x_0[:, 0], x_0[:, 1], u_0[:, 0], u_0[:, 1], \n                alpha=0.5, scale=30, width=0.003)\n    axs[1, 0].scatter(x_0[:, 0], x_0[:, 1], c='k', alpha=0.2, s=1)\n    axs[1, 0].set_title(\"Vector Field (t=0)\")\n    axs[1, 0].set_xlim(xlim)\n    axs[1, 0].set_ylim(ylim)\n\n    # Plot vector field at t=0.5\n    mid_idx = len(x_ts)//2\n    x_mid = x_ts[mid_idx]\n    u_mid = u_ts[mid_idx]\n    axs[1, 1].quiver(x_mid[:, 0], x_mid[:, 1], u_mid[:, 0], u_mid[:, 1], \n                alpha=0.5, scale=30, width=0.003)\n    axs[1, 1].scatter(x_mid[:, 0], x_mid[:, 1], c='k', alpha=0.2, s=1)\n    axs[1, 1].set_title(\"Vector Field (t=0.5)\")\n    axs[1, 1].set_xlim(xlim)\n    axs[1, 1].set_ylim(ylim)\n\n    plt.tight_layout()\n\n\n\nSimple Flow Plan\nThe figure below shows:\n\nThe initial and target distributions (top left)\nThe full trajectories (top right)\nThe vector field at t=0 (bottom left)\nThe vector field at t=0.5 (bottom right)\n\n\n# This is the \"teaching\".\nFM = ConditionalFlowMatcher(sigma=0)\n# FM = ExactOptimalTransportConditionalFlowMatcher(sigma=0)\nplot_flow_matching_trajectories(FM, n_examples=30)\n\n\n\n\n\n\n\n\nWe can plot it with more samples to make the spiral more visible:\n\n# Drawing more samples\nplot_flow_matching_trajectories(FM, n_examples=100)\n\n\n\n\n\n\n\n\n\n\nOptimal Transport\nWith Optimal Transport, the flow paths overlap much less. Intuitively, this can make the denoising model less confused: at each position of the 2D space, the direction is more consistent.\n\n# This is the \"teaching\".\nplot_flow_matching_trajectories(ExactOptimalTransportConditionalFlowMatcher(), n_examples=100)",
    "crumbs": [
      "Home",
      "Generating a 2D Point Cloud",
      "Flow Matching",
      "Training a Flow Matching Model for 2D Point Generation"
    ]
  },
  {
    "objectID": "1_2_Flow Matching 2D Toy.html#visualizing-the-denoising-flow",
    "href": "1_2_Flow Matching 2D Toy.html#visualizing-the-denoising-flow",
    "title": "Training a Flow Matching Model for 2D Point Generation",
    "section": "Visualizing the denoising flow",
    "text": "Visualizing the denoising flow\nThe following code generates and visualizes the denoising flow from a denoising model.\nBefore learning, the denoising flow is random and the velocity field does not move the points much.\n\nfrom typing import Tuple\n\n\nclass TrajectorySet:\n    def __init__(self, embeddings):\n        \"\"\"\n        Managing a set of trajectories, each of which is a sequence of embeddings.\n\n        Parameters\n        ----------\n        embeddings: (n_timesteps, n_samples, *embedding_dims). This assumes\n            the first dimension is time. And it is ordered from t=0 to t=n_timesteps-1.\n            With t=0 representing the clean data and t=n_timesteps-1 representing the noise.\n\n        \"\"\"\n        self.embeddings = embeddings\n        self.embeddings_2d = None\n    \n    def run_tsne(self, n_components: int = 2, seed: int = 0, **kwargs):\n        \"\"\"Run t-SNE on the embeddings.\n        \"\"\"\n        print(f\"Running t-SNE on {self.embeddings.shape} embeddings...\")\n        from sklearn.manifold import TSNE\n        tsne = TSNE(n_components=n_components, random_state=seed, **kwargs)\n        flattened_embeddings = self.embeddings.reshape(-1, self.embeddings.shape[-1])\n        flattened_embeddings_2d = tsne.fit_transform(flattened_embeddings)\n        self.embeddings_2d = flattened_embeddings_2d.reshape(self.embeddings.shape[0], self.embeddings.shape[1], -1)\n        print(f\"t-SNE done. Shape of 2D embeddings: {self.embeddings_2d.shape}\")\n        return self.embeddings_2d\n    \n    def plot_trajectories(\n            self,\n            n: int = 10,\n            show_figure: bool = False,\n            noise_color: Tuple[float, float, float] = (0, 0, 1),  # blue\n            data_color: Tuple[float, float, float] = (1, 0, 0),  # red\n            figsize: tuple = (6, 6),\n            xlim: Tuple[float, float] = None,\n            ylim: Tuple[float, float] = None,\n            with_ticks: bool = False,\n            title: str = None,\n            tsne_seed: int = 0,\n            **kwargs):\n        \"\"\"Plot trajectories of some selected samples.\n\n        This assumes the first dimension is time. And it is ordered from t=0 to t=n_timesteps-1.\n        With t=0 representing the clean data and t=n_timesteps-1 representing the noise.\n\n        Parameters\n        ----------\n        n: int\n            number of samples to plot\n        figsize: tuple\n            figure size\n        kwargs:\n            other keyword arguments for matplotlib.pyplot.scatter\n        \"\"\"\n        import numpy as np\n        import matplotlib.pyplot as plt\n\n        colors = []\n        for t in range(self.embeddings.shape[0]):\n            # interpolate between noise_color and data_color\n            factor = t / (self.embeddings.shape[0] - 1)\n            colors.append(np.array(noise_color) * factor + np.array(data_color) * (1 - factor))\n        colors = np.array(colors)\n        \n        if self.embeddings_2d is None:\n            if self.embeddings.shape[2] == 2:\n                self.embeddings_2d = self.embeddings\n            else:\n                self.embeddings_2d = self.run_tsne(seed=tsne_seed)\n\n        traj = self.embeddings_2d[:, :n, :]\n        g = plt.figure(figsize=figsize)\n        plt.scatter(traj[0, :n, 0], traj[0, :n, 1], s=10, alpha=0.8, c=\"red\")  # real\n        plt.scatter(traj[-1, :n, 0], traj[-1, :n, 1], s=4, alpha=1, c=\"blue\")  # noise\n        plt.scatter(traj[:, :n, 0], traj[:, :n, 1], s=0.5, alpha=0.7, c=colors.repeat(n, axis=0))  # \"olive\"\n        plt.plot(traj[:, :n, 0], traj[:, :n, 1], c=\"olive\", alpha=0.3)\n        if xlim is not None:\n            plt.xlim(xlim)\n        if ylim is not None:\n            plt.ylim(ylim)\n        plt.legend([\"Data\", \"Noise\", \"Intermediate Samples (color coded)\", \"Flow trajectory\"], loc=\"upper right\")\n        if not with_ticks:\n            plt.xticks([])\n            plt.yticks([])\n        elif xlim is not None and ylim is not None:\n            plt.xticks(xlim)\n            plt.yticks(ylim)\n        if title is not None:\n            plt.title(title)\n        if show_figure:\n            plt.show()\n        \n        plt.tight_layout()\n        # save to bytes (png)\n        import io\n\n        bytes_io = io.BytesIO()\n        g.savefig(bytes_io, format=\"png\")\n        return bytes_io.getvalue()\n        \n        # # return the figure\n        # return plt.gcf()\n\ndef generate_trajectories_from_denoising_model(model, num_samples=100, num_timesteps=100, data_dims=[2,], seed=0, device=device):\n    # Use generate_samples_with_flow_matching to generate the trajectories\n    traj = generate_samples_with_flow_matching(\n        denoising_model=model,\n        device=device,\n        num_samples=num_samples,\n        num_timesteps=num_timesteps,\n        data_dims=data_dims,\n        seed=seed,\n        clip_min=-100,\n        clip_max=100,\n        return_full_trajectory=True,\n    ).cpu().numpy()\n    traj_set = TrajectorySet(traj)\n    return traj_set\n\n\ndef visualize_denoising_flow(model, n=100, seed=0):\n    traj_set = generate_trajectories_from_denoising_model(model, num_samples=n, seed=seed)\n    traj_set.plot_trajectories(n=n, show_figure=True, figsize=(8, 8))\n\n\n# This is the \"learning\".\nvisualize_denoising_flow(model3, n=100)\n\n\n\n\n\n\n\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\nThe above figure shows the denoising flow from the denoising model. Since the points barely moved, the flow paths are not visible.\nWe will save the animation.\n\nUtility code to generate animations\n\n# A shared state to store the frames for the vector field animation.\n_state = {\n    \"frames\": {},\n}\n\ndef visualize_denoising_vector_field(model, t=0, num_samples=32, num_timesteps=100, seed=0):\n    # visualize the vector field of the denoising model\n    # The denoising model is a velocity field.\n    # Given a grid of points, we can visualize the vector field at these points.\n    x_grid = torch.linspace(-3.6, 3.6, 20)\n    y_grid = torch.linspace(-3.6, 3.6, 20)\n    x_grid, y_grid = torch.meshgrid(x_grid, y_grid)\n    points = torch.stack([x_grid, y_grid], dim=-1)\n    points = points.view(-1, 2).to(device)\n    t_int = int(t * num_timesteps)\n    t = t * torch.ones(points.shape[0]).to(device)\n    with torch.no_grad():\n        v = model(t=t, x=points)\n        v = v.cpu().numpy()\n    # plot the vector field\n    points = points.cpu().numpy()\n    x_grid = x_grid.cpu().numpy()\n    y_grid = y_grid.cpu().numpy()\n\n    if \"traj\" not in _state:\n        node = NeuralODE(model, solver=\"euler\", sensitivity=\"adjoint\")\n        data_dims = [2,]\n        torch.manual_seed(seed)\n        with torch.no_grad():\n            _state[\"traj\"] = node.trajectory(\n                torch.randn(num_samples, *data_dims, device=device),\n                t_span=torch.linspace(0, 1, num_timesteps, device=device),\n            ).cpu().numpy()\n    \n    traj = _state[\"traj\"]\n    g = plt.figure(figsize=(6, 6))\n    plt.quiver(x_grid, y_grid, v[:, 0], v[:, 1], alpha=0.5, scale=30, width=0.003)\n    plt.scatter(points[:, 0], points[:, 1], c='k', alpha=0.2, s=1)\n    # overlay with the points at the current timestep\n    plt.scatter(traj[t_int, :, 0], traj[t_int, :, 1], c='blue', edgecolor='gray', alpha=0.5, s=25)\n    plt.title(f\"t={t_int}\")\n    plt.show()\n    \n    # save the figure into a numpy array\n    # g = plt.gcf()\n    # save figure as an image\n    import io\n    bytes_io = io.BytesIO()\n    g.savefig(fname=bytes_io, format=\"png\")\n    _state[\"frames\"][t_int] = bytes_io.getvalue()\n    # return g\n\n\n# create an interactive widget to visualize the vector field at different timesteps\nfrom ipywidgets import interact, fixed\n\ninteract(visualize_denoising_vector_field, model=fixed(model3), t=(0.0, 0.99, 0.01), num_samples=fixed(100), num_timesteps=fixed(100), seed=fixed(0))\n\n\n\n\n\n&lt;function __main__.visualize_denoising_vector_field(model, t=0, num_samples=32, num_timesteps=100, seed=0)&gt;\n\n\n\n# Generate a GIF from the frames\nfrom PIL import Image\nimport io\n# _state[\"frames\"] has bytes objects (png images)\nframes = list(_state[\"frames\"].values())\nframes = [Image.open(io.BytesIO(frame)).resize((360, 360)) for frame in frames]\nframes[0].save(\"initial_denoising_vector_field.gif\", save_all=True, append_images=frames[1:], duration=500, loop=0)\n\nNow let’s train the model to get back to the clean data.",
    "crumbs": [
      "Home",
      "Generating a 2D Point Cloud",
      "Flow Matching",
      "Training a Flow Matching Model for 2D Point Generation"
    ]
  },
  {
    "objectID": "1_2_Flow Matching 2D Toy.html#training-loop",
    "href": "1_2_Flow Matching 2D Toy.html#training-loop",
    "title": "Training a Flow Matching Model for 2D Point Generation",
    "section": "Training loop",
    "text": "Training loop\nThe training loop is very simple. We sample a batch of data points \\(x_1\\) and noise \\(x_0\\) from the dataset. We then compute the desired conditional flow \\(u_t\\) using the flow matching planner. We then compute the loss as the mean squared error between the predicted velocity field \\(v_\\theta(x_t, t)\\) and the conditional flow \\(u_t\\).\n\ndef train(model: nn.Module, optimizer: torch.optim.Optimizer, steps: int=100, quiet: bool=False) -&gt; float:\n  model.train()\n  if not quiet:\n    print(\"Training on device:\", device)\n  max_train_steps = steps\n\n  step = 0\n  loss = None\n  while step &lt; max_train_steps:\n\n    if quiet:\n      progress_bar = train_dataloader\n    else:\n      progress_bar = tqdm(train_dataloader, total=min(max_train_steps - step, len(train_dataloader)))\n    for x_1 in progress_bar:\n\n      ###########################################\n      # Start of the core training step\n      ###########################################\n      optimizer.zero_grad()\n      x_1 = x_1.float().to(device)\n\n      x_0 = torch.randn_like(x_1).to(device)\n      t, x_t, u_t = FM.sample_location_and_conditional_flow(x_0, x_1)\n\n      v_t = model(t=t, x=x_t)\n      v_t = v_t.sample if hasattr(v_t, \"sample\") else v_t\n      loss = torch.mean((v_t - u_t) ** 2)  # MSE loss\n      loss.backward()\n      optimizer.step()\n      ###########################################\n      # End of the core training step\n      ###########################################\n\n      if not quiet:\n        progress_bar.set_postfix({\"loss\": loss.cpu().item()})\n\n      step += 1\n\n      if step &gt;= max_train_steps:\n        if not quiet:\n          print(f\"Reached the max training steps:\", max_train_steps)\n        break\n\n  return loss\n\n\nTrain for 100 steps\n\n# Define the model and optimizer\nmodel3 = Model3(\n    dim_in=2,\n    dim_out=2,\n    dim_hids=[128, 128, 128],  # [128, 128, 256],\n    num_timesteps=config.num_denoising_steps,\n).to(device)\nprint(f\"model params: {sum(p.numel() for p in model3.parameters())}\")\nmodel3_optimizer = optim.AdamW(model3.parameters(), lr=config.learning_rate, weight_decay=config.weight_decay)\n\n# Train the model\nmodel3_loss = train(model3, model3_optimizer, steps=100)\n\nmodel params: 182410\nTraining on device: cuda\n\n\n\n\n\nReached the max training steps: 100\n\n\nDid the model learn anything? Let’s check.\n\n\nVisualize the sampled data\n\n# visualize the sampled images\ndef visualize_sampled_data(model):\n  # print(\"Loss of the denoising model:\", loss.item())\n  x_sampled = generate_samples_with_flow_matching(\n    denoising_model=model,\n    device=device,\n    num_samples=128,\n    parallel=False,\n    seed=0,\n    clip_min=-3.6,\n    clip_max=3.6,\n  )\n\n  # plt.scatter(x_sampled[:, 0], x_sampled[:, 1])\n  fig, axs = plt.subplots(1, 1, figsize=(5, 5))\n  x_sampled = x_sampled.cpu().numpy()\n  axs.scatter(x_sampled[:,0], x_sampled[:,1], color='white', edgecolor='gray', s=5)\n  # axs.set_axis_off()\n  # plt.xlim(-3.6, 3.6)\n  # plt.ylim(-3.6, 3.6)\n  return fig\n\nmodel3.eval()\n_ = visualize_sampled_data(model3)\n\n\n\n\n\n\n\n\nWe can already see the shape of the data changed, but the spiral is not very visible. What if we increase the length of training?",
    "crumbs": [
      "Home",
      "Generating a 2D Point Cloud",
      "Flow Matching",
      "Training a Flow Matching Model for 2D Point Generation"
    ]
  },
  {
    "objectID": "1_2_Flow Matching 2D Toy.html#train-more-steps-and-generate-again",
    "href": "1_2_Flow Matching 2D Toy.html#train-more-steps-and-generate-again",
    "title": "Training a Flow Matching Model for 2D Point Generation",
    "section": "Train more steps and generate again",
    "text": "Train more steps and generate again\n\n# Train some more\nmodel3_loss = train(model3, model3_optimizer, steps=1000)\nprint(\"loss:\", model3_loss.item())\n_ = visualize_sampled_data(model3)\n\nTraining on device: cuda\n\n\n\n\n\nReached the max training steps: 1000\nloss: 2.776731014251709\n\n\n\n\n\n\n\n\n\nThere we go. In just 1000 training steps, we were able to go from complete noise to a fairly representative spiral.",
    "crumbs": [
      "Home",
      "Generating a 2D Point Cloud",
      "Flow Matching",
      "Training a Flow Matching Model for 2D Point Generation"
    ]
  },
  {
    "objectID": "1_2_Flow Matching 2D Toy.html#visualize-denoising-flows",
    "href": "1_2_Flow Matching 2D Toy.html#visualize-denoising-flows",
    "title": "Training a Flow Matching Model for 2D Point Generation",
    "section": "Visualize denoising flows",
    "text": "Visualize denoising flows\nWe can visualize the denoising flow at different timesteps, connecting the sequence of points from noise (\\(x_0\\), blue) to clean data (\\(x_1\\), red), with intermediate samples colored in between.\nThe trajectories roughly follow the flow plans that we use to teach the model, but not exactly.\n\n\nvisualize_denoising_flow(model3)\n\n\n\n\n\n\n\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\nAnimate the denoising process\nLet’s animate the denoising process over generation steps.\n\n# A shared state to store the frames for the vector field animation.\n_state = {\n    \"frames\": {},\n}\n\ninteract(visualize_denoising_vector_field, model=fixed(model3), t=(0.0, 0.99, 0.01), num_samples=fixed(128), num_timesteps=fixed(100), seed=fixed(0))\n\n\n\n\n&lt;function __main__.visualize_denoising_vector_field(model, t=0, num_samples=32, num_timesteps=100, seed=0)&gt;\n\n\nUse the slider above to move through the vector field at different timesteps. Try to move slowly and cover many timesteps. Once done, you should run the next cell to save the frames as a GIF.\n\n# save the frames as a GIF\nif len(_state[\"frames\"]) &gt; 1:\n    frames = [_state[\"frames\"][k] for k in sorted(_state[\"frames\"].keys(), key=float)]\n    frames = [Image.open(io.BytesIO(frame)).resize((360, 360)) for frame in frames]\n    frames[0].save(\n        \"learned_denoising_vector_field.gif\", save_all=True,\n        append_images=frames[1:],\n        duration=100 * 5,  # Increased duration (milliseconds per frame) for slower motion\n        loop=0\n    )\n\nAnd here we display the GIF animation:\n\n# show the generated GIF animation\nfrom IPython.display import HTML\n\nHTML(\"\"\"\n&lt;img src=\"learned_denoising_vector_field.gif\"&gt;\n\"\"\")\n\n\n\n\n\nLet’s double check that the final generated data is close to the target spiral.\n\n_ = visualize_sampled_data(model3)",
    "crumbs": [
      "Home",
      "Generating a 2D Point Cloud",
      "Flow Matching",
      "Training a Flow Matching Model for 2D Point Generation"
    ]
  },
  {
    "objectID": "1_2_Flow Matching 2D Toy.html#visualize-the-learned-vector-field-over-time",
    "href": "1_2_Flow Matching 2D Toy.html#visualize-the-learned-vector-field-over-time",
    "title": "Training a Flow Matching Model for 2D Point Generation",
    "section": "Visualize the learned vector field over time",
    "text": "Visualize the learned vector field over time\nWe can also visualize the progress of learning over the training steps.\n\nmodel3 = Model3(\n    dim_in=2,\n    dim_out=2,\n    dim_hids=[128, 128, 256],\n    num_timesteps=config.num_denoising_steps,\n).to(device)\nmodel3_optimizer = optim.AdamW(model3.parameters(), lr=config.learning_rate, weight_decay=config.weight_decay)\n\nsamples = []\nnum_samples = 10\nall_trajs = []\nsteps_to_checkpoint = [10, 15, 20, 25, 30, 35, 40, 45, 50, 60, 70, 80, 90, 100, 120, 140, 160, 180, 200, 300, 400, 500]\nfor i, checkpoint_steps in tqdm(enumerate(steps_to_checkpoint), total=len(steps_to_checkpoint)):\n    steps = checkpoint_steps - steps_to_checkpoint[i-1] if i &gt; 0 else checkpoint_steps\n    model3_loss = train(model3, model3_optimizer, steps=steps, quiet=True)\n    node = NeuralODE(model3, solver=\"euler\", sensitivity=\"adjoint\")\n    torch.manual_seed(0)\n    num_samples = 100\n    num_timesteps = 100\n    data_dims = [2,]\n    # clip_min = -3.6\n    # clip_max = 3.6\n    with torch.no_grad():\n        torch.manual_seed(0)\n        trajs = node.trajectory(\n            torch.randn(num_samples, *data_dims, device=device),\n            t_span=torch.linspace(0, 1, num_timesteps, device=device),\n        )\n        trajs = trajs.cpu().numpy()\n        # invert the time dimension to make it start from clean data to noise\n        trajs = trajs[::-1, ...]\n        # all_trajs.append(trajs)\n        # plot the trajectory\n        traj_set = TrajectorySet(trajs)\n        all_trajs.append(traj_set)\n    # samples.append(generate_samples_with_flow_matching(model3, device, num_samples=128, parallel=False, seed=0, clip_min=-3.6, clip_max=3.6))\n\n\n\n\n\n# A shared state to store the frames for the vector field animation.\n_state = {\n    \"frames\": {},\n}\n\ndef show_traj_at_learning_step(i=0):\n    bytes_value = all_trajs[i].plot_trajectories(\n        n=100, show_figure=True, figsize=(6, 6),\n        with_ticks=True, title=f\"train steps = {steps_to_checkpoint[i]}\",\n        xlim=(-5, 5), ylim=(-5, 5),\n    )\n    _state[\"frames\"][steps_to_checkpoint[i]] = bytes_value\n\ninteract(show_traj_at_learning_step, i=(0, len(all_trajs)-1, 1))\n\n\n\n\n&lt;function __main__.show_traj_at_learning_step(i=0)&gt;\n\n\nAgain, drag the slider to move through different timesteps, and observe how the trajectories evolve. Try to move slowly and cover many timesteps. Once done, you should run the next cell to save the frames as a GIF.\n\n# save the frames as a GIF\nif _state[\"frames\"] and len(_state[\"frames\"]) &gt; 5:\n    frames = list(_state[\"frames\"].values())\n    frames = [Image.open(io.BytesIO(frame)) for frame in frames]\n    # Add copies of the last frame to make it pause at the end\n    frames_with_pause = frames[:-1] + [frames[-1]] * 5\n    frames[0].save(\n        \"denoising_trajectories_over_training_steps.gif\", save_all=True,\n        append_images=frames_with_pause[1:],\n        duration=100 * 5,  # Increased duration (milliseconds per frame) for slower motion\n        loop=0\n    )\n    # plt.imshow(frames[0])  # double check that the first frame looks ok\n\n\n# load the generated GIF\nfrom IPython.display import HTML\nHTML(\"\"\"\n&lt;img src=\"denoising_trajectories_over_training_steps.gif\"&gt;\n\"\"\")\n\n\n\n\n\nInterestingly, the learned trajectories are not quite the same as the flow plans. They seem to first go “inward” before traveling to the target positions. Did the “student” (the model) find a better route?",
    "crumbs": [
      "Home",
      "Generating a 2D Point Cloud",
      "Flow Matching",
      "Training a Flow Matching Model for 2D Point Generation"
    ]
  },
  {
    "objectID": "1_2_Flow Matching 2D Toy.html#model-1",
    "href": "1_2_Flow Matching 2D Toy.html#model-1",
    "title": "Training a Flow Matching Model for 2D Point Generation",
    "section": "Model 1",
    "text": "Model 1\n\nmodel1 = Model1().to(device)\nprint(f\"model params: {sum(p.numel() for p in model1.parameters())}\")\n\nmodel1_optimizer = optim.AdamW(model1.parameters(), lr=config.learning_rate, weight_decay=config.weight_decay)\nmodel1_loss = train(model1, model1_optimizer, steps=10000)\nprint(\"model1_loss:\", model1_loss.item())\n_ = visualize_sampled_data(model1)\n\nmodel params: 265730\nTraining on device: cuda\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReached the max training steps: 10000\nmodel1_loss: 4.001157760620117",
    "crumbs": [
      "Home",
      "Generating a 2D Point Cloud",
      "Flow Matching",
      "Training a Flow Matching Model for 2D Point Generation"
    ]
  },
  {
    "objectID": "1_2_Flow Matching 2D Toy.html#model-2",
    "href": "1_2_Flow Matching 2D Toy.html#model-2",
    "title": "Training a Flow Matching Model for 2D Point Generation",
    "section": "Model 2",
    "text": "Model 2\n\nmodel2 = Model2(features=2, hidden_features=[256, 256, 512]).to(device)\nprint(f\"model params: {sum(p.numel() for p in model2.parameters())}\")\n\nmodel2_optimizer = optim.AdamW(model2.parameters(), lr=config.learning_rate, weight_decay=config.weight_decay)\nmodel2_loss = train(model2, model2_optimizer, steps=10000)\nprint(\"model2_loss:\", model2_loss.item())\n_ = visualize_sampled_data(model2)\n\nmodel params: 207362\nTraining on device: cuda\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReached the max training steps: 10000\nmodel2_loss: 2.452648162841797",
    "crumbs": [
      "Home",
      "Generating a 2D Point Cloud",
      "Flow Matching",
      "Training a Flow Matching Model for 2D Point Generation"
    ]
  },
  {
    "objectID": "1_2_Flow Matching 2D Toy.html#model-3",
    "href": "1_2_Flow Matching 2D Toy.html#model-3",
    "title": "Training a Flow Matching Model for 2D Point Generation",
    "section": "Model 3",
    "text": "Model 3\n\nmodel3 = Model3(\n    dim_in=2,\n    dim_out=2,\n    dim_hids=[128, 128, 128],\n    num_timesteps=config.num_denoising_steps,\n).to(device)\nprint(f\"model params: {sum(p.numel() for p in model3.parameters())}\")\n\nmodel3_optimizer = optim.AdamW(model3.parameters(), lr=config.learning_rate, weight_decay=config.weight_decay)\nmodel3_loss = train(model3, model3_optimizer, steps=10000)\nprint(\"model3_loss:\", model3_loss.item())\n_ = visualize_sampled_data(model3)\n\nmodel params: 182410\nTraining on device: cuda\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReached the max training steps: 10000\nmodel3_loss: 2.957486152648926\n\n\n\n\n\n\n\n\n\n\nimport torch\n\n# Sample points from the model\ndef generate_points(model):\n    with torch.no_grad():\n        x_T = torch.randn(128, 2)\n        x_sampled = generate_samples_with_flow_matching(model, device, num_samples=128, parallel=False, seed=0, clip_min=-3.6, clip_max=3.6)\n    return x_sampled.cpu().numpy()\n\n\n# The target spiral points for comparison\ntarget_spiral = next(iter(train_dataloader))\nchamfer_dist = {}\n\ngenerated_points_1 = generate_points(model1)\ngenerated_points_1 = np.clip(generated_points_1, -3, 3)\nchamfer_dist[1] = chamfer_distance(generated_points_1, target_spiral, direction='bi')\nprint(\"Model 1 Chamfer Distance:\", chamfer_dist[1])\n\n\ngenerated_points_2 = generate_points(model2)\nchamfer_dist[2] = chamfer_distance(generated_points_2, target_spiral, direction='bi')\nprint(\"Model 2 Chamfer Distance:\", chamfer_dist[2])\n\n# Calculate Chamfer distance\ngenerated_points_3 = generate_points(model3)\nchamfer_dist[3] = chamfer_distance(generated_points_3, target_spiral, direction='bi')\nprint(\"Model 3 Chamfer Distance:\", chamfer_dist[3])\n\n# # visualize the sampled images side by side\ndef visualize_sampled_data_side_by_side(models, generated_points):\n    fig, axs = plt.subplots(1, len(models) + 1, figsize=(5 * len(models), 5))\n    # Add ground truth\n    axs[0].scatter(target_spiral[:, 0], target_spiral[:, 1], color='white', edgecolor='gray', s=5)\n    axs[0].set_title(\"Ground truth\")\n\n    for i, (model, points) in enumerate(zip(models, generated_points)):\n        axs[i+1].scatter(points[:,0], points[:,1], color='white', edgecolor='gray', s=5)\n        axs[i+1].set_title(f\"{model.__class__.__name__}, Chamfer: {chamfer_dist[i+1]:.2f}\")\n\nvisualize_sampled_data_side_by_side([model1, model2, model3], [generated_points_1, generated_points_2, generated_points_3])\n\nModel 1 Chamfer Distance: 0.9307475872814921\nModel 2 Chamfer Distance: 0.29258441181503825\nModel 3 Chamfer Distance: 0.19064741917066574\n\n\n\n\n\n\n\n\n\nVisually and quantitatively, Model 3 and Model 2 outpeform Model 1 a large margin. Model 3 is also clearly better than Model 2. The conclusion is similar to our experiments in diffusion.",
    "crumbs": [
      "Home",
      "Generating a 2D Point Cloud",
      "Flow Matching",
      "Training a Flow Matching Model for 2D Point Generation"
    ]
  },
  {
    "objectID": "1_2_Flow Matching 2D Toy.html#questions-for-future-exploration",
    "href": "1_2_Flow Matching 2D Toy.html#questions-for-future-exploration",
    "title": "Training a Flow Matching Model for 2D Point Generation",
    "section": "Questions for future exploration",
    "text": "Questions for future exploration\n\nHow does the model size matter (number of parameters, controlled by the number of layers, and number of hidden units), within the same class of arch?\nHow does the number of timesteps matter?\nWhat other designs of flow plans are there in recent literature? Can you bring them to this toy example?\nThe learned denoising flow is not exactly the same as the flow plans. They seem to first go “inward” before traveling to the target positions. Why is this happening?\nIf you change the scale factor of the data (look for scaling = 2) to 1 or 3, how does the model’s behavior change?\nIf you change the target dataset to a different shape (e.g. make_moons), how does the model’s behavior change?",
    "crumbs": [
      "Home",
      "Generating a 2D Point Cloud",
      "Flow Matching",
      "Training a Flow Matching Model for 2D Point Generation"
    ]
  },
  {
    "objectID": "1_1_a_refactor.html",
    "href": "1_1_a_refactor.html",
    "title": "A refactoring exercise",
    "section": "",
    "text": "Let’s pause for a moment and think about what we’ve done. We’ve built a diffusion model that can generate 2D spiral data.\nThere are still many questions that we haven’t answered:\nTo answer these questions, we need to do experiments that require numerous code changes. However, the notebook is quite long even for this simple task. Let’s refactor it to make it more modular and easier to understand.",
    "crumbs": [
      "Home",
      "Generating a 2D Point Cloud",
      "Diffusion",
      "A refactoring exercise"
    ]
  },
  {
    "objectID": "1_1_a_refactor.html#animate-the-learning-process",
    "href": "1_1_a_refactor.html#animate-the-learning-process",
    "title": "A refactoring exercise",
    "section": "Animate the learning process",
    "text": "Animate the learning process\n\nfrom lib_1_1.diffusion import generate_samples_by_denoising\nfrom tqdm import tqdm\n\nmodel3 = model.Model3(\n    hidden_features=[128, 128, 256],\n    num_timesteps=training_config.num_denoising_steps,\n).to(device)\nmodel3_optimizer = torch.optim.Adam(model3.parameters(), lr=training_config.learning_rate)\n# model3_optimizer = torch.optim.AdamW(model3.parameters(), lr=training_config.learning_rate, weight_decay=training_config.weight_decay)\n\nsamples = []\nsteps_to_checkpoint = [0, 100, 200, 300, 400, 500, 600, 700, 800, 900]\nall_trajs = []\n\npbar = tqdm(range(len(steps_to_checkpoint)))\nfor i in pbar:\n    if i &gt; 0:\n        steps_to_train = steps_to_checkpoint[i] - steps_to_checkpoint[i-1]\n    else:\n        steps_to_train = i\n    model3_loss = train(\n        model3, train_dataloader,\n        noise_schedule, model3_optimizer,\n        steps=steps_to_train, silent=True,\n        device=device, num_denoising_steps=training_config.num_denoising_steps\n    )\n    if i &gt; 0:\n        pbar.set_description(f\"loss: {model3_loss:.4f}\")\n    torch.manual_seed(0)\n    traj = generate_samples_by_denoising(\n        model3, torch.randn(128, 2), noise_schedule, n_T=1000, device=device, return_full_trajectory=True, silent=True)\n    traj = traj.detach().cpu().numpy()\n    samples.append(traj[0])\n    traj = traj[::20, :, :]\n    traj_set = TrajectorySet(traj)\n    all_trajs.append(traj_set)\n\n  0%|          | 0/10 [00:00&lt;?, ?it/s]loss: 0.2669: 100%|██████████| 10/10 [00:16&lt;00:00,  1.66s/it]\n\n\n\nvisualize_sampled_data(model3, noise_schedule, num_samples=128, device=device)\n\n  0%|          | 0/1000 [00:00&lt;?, ?it/s, std=0.933]100%|██████████| 1000/1000 [00:01&lt;00:00, 571.12it/s, std=2.01]\n\n\n\n\n\n\n\n\n\n\nimport matplotlib.animation as animation\nfrom IPython.display import HTML\n\nfig, ax = plt.subplots()\nsc = ax.scatter(samples[i][:, 0], samples[i][:, 1], color='white', edgecolor='gray', s=5)\nax.set_xlim(-5, 5)\nax.set_ylim(-5, 5)\nfig.suptitle(\"Samples\")\n\ndef update(i):\n    sc.set_offsets(samples[i])\n    return sc,\n\nani = animation.FuncAnimation(fig, update, frames=range(len(steps_to_checkpoint)), interval=1000, blit=True, repeat=False)\nHTML(ani.to_jshtml())\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# A shared state to store the frames for the vector field animation.\n_state = {\n    \"frames\": {},\n}\n\nfrom ipywidgets import interact\n\ndef show_traj_at_learning_step(i=0):\n    train_steps = steps_to_checkpoint[i]\n    bytes_value = all_trajs[i].plot_trajectories(\n        n=12, show_figure=True, figsize=(6, 6),\n        with_ticks=True, title=f\"train steps = {train_steps}\",\n        xlim=(-5, 5), ylim=(-5, 7),\n        with_lines=True,\n    )\n    _state[\"frames\"][train_steps] = bytes_value\n\ninteract(show_traj_at_learning_step, i=(0, len(all_trajs)-1, 1))\n\n\n\n\n&lt;function __main__.show_traj_at_learning_step(i=0)&gt;",
    "crumbs": [
      "Home",
      "Generating a 2D Point Cloud",
      "Diffusion",
      "A refactoring exercise"
    ]
  },
  {
    "objectID": "1_1_a_refactor.html#compare-model-performance",
    "href": "1_1_a_refactor.html#compare-model-performance",
    "title": "A refactoring exercise",
    "section": "Compare model performance",
    "text": "Compare model performance\n\nmodel1 = model.Model1(\n    hidden_features=[512, 256, 256],\n    num_timesteps=training_config.num_denoising_steps\n).to(device)\nmodel1_optimizer = torch.optim.Adam(model1.parameters(), lr=training_config.learning_rate)\nmodel1_loss = train(model1, train_dataloader, noise_schedule, model1_optimizer, steps=5000, device=device, num_denoising_steps=training_config.num_denoising_steps)\n\nmodel2 = model.Model2(\n    hidden_features=[512, 256, 256],\n    num_timesteps=training_config.num_denoising_steps\n).to(device)\nmodel2_optimizer = torch.optim.Adam(model2.parameters(), lr=training_config.learning_rate)\nmodel2_loss = train(model2, train_dataloader, noise_schedule, model2_optimizer, steps=5000, device=device, num_denoising_steps=training_config.num_denoising_steps)\n\nmodel3 = model.Model3(\n    hidden_features=[128, 128, 128],\n    num_timesteps=training_config.num_denoising_steps\n).to(device)\nmodel3_optimizer = torch.optim.Adam(model3.parameters(), lr=training_config.learning_rate)\nmodel3_loss = train(model3, train_dataloader, noise_schedule, model3_optimizer, steps=5000, device=device, num_denoising_steps=training_config.num_denoising_steps)\n\nTraining on device: cuda\n\n\n\n\n\nReached the max training steps: 5000\nTraining on device: cuda\n\n\n\n\n\nReached the max training steps: 5000\nTraining on device: cuda\n\n\n\n\n\nReached the max training steps: 5000\n\n\n\nimport torch\nimport numpy as np\nfrom lib_1_1.eval import chamfer_distance\n\n\n# The target spiral points for comparison\ntarget_spiral = next(iter(train_dataloader))\n\ngenerated_points_1 = generate_samples_by_denoising(model1, torch.randn(128, 2), noise_schedule, n_T=1000, device=device)\ngenerated_points_1 = np.clip(generated_points_1.cpu().numpy(), -3.9, 3.9)\nchamfer_dist = chamfer_distance(generated_points_1, target_spiral, direction='bi')\nprint(\"Model 1 Size:\", sum(p.numel() for p in model1.parameters()), \"Chamfer Distance:\", chamfer_dist)\n\ngenerated_points_2 = generate_samples_by_denoising(model2, torch.randn(128, 2), noise_schedule, n_T=1000, device=device)\ngenerated_points_2 = generated_points_2.cpu().numpy()\nchamfer_dist = chamfer_distance(generated_points_2, target_spiral, direction='bi')\nprint(\"Model 2 Size:\", sum(p.numel() for p in model2.parameters()), \"Chamfer Distance:\", chamfer_dist)\n\ngenerated_points_3 = generate_samples_by_denoising(model3, torch.randn(128, 2), noise_schedule, n_T=1000, device=device)\ngenerated_points_3 = generated_points_3.cpu().numpy()\nchamfer_dist = chamfer_distance(generated_points_3, target_spiral, direction='bi')\nprint(\"Model 3 Size:\", sum(p.numel() for p in model3.parameters()), \"Chamfer Distance:\", chamfer_dist)\n\n# # visualize the sampled images side by side\ndef visualize_sampled_data_side_by_side(models, generated_points):\n    fig, axs = plt.subplots(1, len(models) + 1, figsize=(5 * len(models), 5))\n    # Add ground truth\n    axs[0].scatter(target_spiral[:, 0], target_spiral[:, 1], color='white', edgecolor='gray', s=5)\n    axs[0].set_title(\"Ground truth\")\n\n    for i, (model, points) in enumerate(zip(models, generated_points)):\n        axs[i+1].scatter(points[:,0], points[:,1], color='white', edgecolor='gray', s=5)\n        axs[i+1].set_title(model.__class__.__name__)\n\nvisualize_sampled_data_side_by_side([model1, model2, model3], [generated_points_1, generated_points_2, generated_points_3])\n\n  0%|          | 0/1000 [00:00&lt;?, ?it/s, std=1.49]100%|██████████| 1000/1000 [00:00&lt;00:00, 1016.28it/s, std=62.6]\n\n\nModel 1 Size: 199682 Chamfer Distance: 1.546387922825363\n\n\n100%|██████████| 1000/1000 [00:01&lt;00:00, 970.83it/s, std=1.93]\n\n\nModel 2 Size: 215554 Chamfer Distance: 0.37767705435093313\n\n\n100%|██████████| 1000/1000 [00:01&lt;00:00, 556.81it/s, std=2.01]\n\n\nModel 3 Size: 182410 Chamfer Distance: 0.2109826764526811",
    "crumbs": [
      "Home",
      "Generating a 2D Point Cloud",
      "Diffusion",
      "A refactoring exercise"
    ]
  },
  {
    "objectID": "1_1_a_refactor.html#conclusion",
    "href": "1_1_a_refactor.html#conclusion",
    "title": "A refactoring exercise",
    "section": "Conclusion",
    "text": "Conclusion\nWith refactoring, we were able to replicate the functionality of the original notebook, while making it much shorter and easier to understand.\nNext, we are ready to do a series of experiments to understand the impact of hyperparameters in the training algorithm.",
    "crumbs": [
      "Home",
      "Generating a 2D Point Cloud",
      "Diffusion",
      "A refactoring exercise"
    ]
  },
  {
    "objectID": "1_1_Diffusion 2D Toy.html",
    "href": "1_1_Diffusion 2D Toy.html",
    "title": "Training a Diffusion Model on 2D Points",
    "section": "",
    "text": "open in colab\nDiffusion models have revolutionized generative AI, but they can be complex to understand. In this tutorial, we’ll build intuition by training a diffusion model on a simple 2D spiral dataset. This allows us to visualize the entire process and understand key concepts without the complexity of image generation.\nIn this notebook, you’ll learn how to train a diffusion model to generate spirals of 2-D points.",
    "crumbs": [
      "Home",
      "Generating a 2D Point Cloud",
      "Diffusion",
      "Training a Diffusion Model on 2D Points"
    ]
  },
  {
    "objectID": "1_1_Diffusion 2D Toy.html#what-well-cover",
    "href": "1_1_Diffusion 2D Toy.html#what-well-cover",
    "title": "Training a Diffusion Model on 2D Points",
    "section": "What We’ll Cover",
    "text": "What We’ll Cover\n\nForward diffusion: How to gradually add noise to data\nReverse diffusion: How to learn to denoise data\nThree different model architectures and their tradeoffs\nVisualization of the diffusion process",
    "crumbs": [
      "Home",
      "Generating a 2D Point Cloud",
      "Diffusion",
      "Training a Diffusion Model on 2D Points"
    ]
  },
  {
    "objectID": "1_1_Diffusion 2D Toy.html#why-2d-points",
    "href": "1_1_Diffusion 2D Toy.html#why-2d-points",
    "title": "Training a Diffusion Model on 2D Points",
    "section": "Why 2D Points?",
    "text": "Why 2D Points?\nWorking with 2D points offers several advantages:\n\nFast training on CPU\nEasy visualization of the entire process\nClear understanding of how diffusion models work\n\nWe’ll explore the different components of diffusion models and their functions, as well as compare the quality of generated results using different model architectures.\n\nImport dependencies\nFirst, let’s import a few libraries. You do not need to have many dependencies. torch is really the main library you need. matplotlib is used for visualization and sklearn is used to load the dataset.\n\nfrom dataclasses import dataclass\nimport math\nfrom typing import Dict, Tuple\n\nfrom matplotlib import pyplot as plt\nfrom sklearn.datasets import make_swiss_roll\nimport torch\nfrom torch import nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom torch.nn import MSELoss\nfrom torch.utils.data import DataLoader\nfrom tqdm.auto import tqdm",
    "crumbs": [
      "Home",
      "Generating a 2D Point Cloud",
      "Diffusion",
      "Training a Diffusion Model on 2D Points"
    ]
  },
  {
    "objectID": "1_1_Diffusion 2D Toy.html#forward-diffusion-the-teaching",
    "href": "1_1_Diffusion 2D Toy.html#forward-diffusion-the-teaching",
    "title": "Training a Diffusion Model on 2D Points",
    "section": "Forward Diffusion: the Teaching",
    "text": "Forward Diffusion: the Teaching\nInstead of making the model “imagine” a spiral from scratch, we define a more structured and easier task:\n\nWe add noise to the data in multiple steps.\nThe more steps we apply, the more the structure is lost.\nThis process gives the model examples of partially corrupted data at different noise levels.\n\nThe above process is called forward diffusion. We gradually add noise to our spiral data according to a carefully designed schedule. This creates a sequence:\n\nClean data → Slightly noisy → More noisy → Pure noise\n\nForward diffusion can be considered as “data augmentation” during training. We challenge the model tell apart signal from noise, rather than predicting the signal from nothing (or pure noise). This makes the task more structured and easier to learn.\nWith forward diffusion, we produce a sequence of data points (each \\(x_t\\) is a 2 dimensional vector):\n\\[x_0, x_1, x_2, ..., x_T\\]\nwhere T is the number of diffusion steps (e.g. 1000).\nSimilar to an algebraic sequence with a common difference, the diffusion sequence has a “common noise” \\(\\mathbf\\epsilon\\) that is the same for all steps in a sequence. The recursive formula of the diffusion sequence is:\n\\[\nx_t = \\sqrt{\\alpha_t} x_{t-1} + \\sqrt{\\beta_t} \\mathbf\\epsilon\n\\]\nwhere \\(\\beta_t = 1 - \\alpha_t\\) is the variance of the noise at time \\(t\\). This relationship is primarily a design choice that simplifies the mathematics and makes the derivations more elegant and manageable.\nOne can then write the explicit formula for \\(x_t\\) expressed in terms of \\(x_0\\) and \\(\\mathbf\\epsilon\\):\n\\[\nx_t = \\sqrt{\\bar\\alpha_t} x_0 + \\sqrt{1 - \\bar\\alpha_t} \\mathbf\\epsilon\n\\]\nwhere \\(\\bar\\alpha_t = \\prod_{i=1}^t \\alpha_i\\) is the cumulative product of the variance of the noise at time \\(t\\).\nThe forward diffusion process is thus defined by this noise schedule: \\(\\beta_1, \\beta_2, ..., \\beta_T\\), which defines the “blending ratio” between the clean data and the noise.\nBelow is an implementation of forward diffusion.\n\ndef forward_diffusion(x_0, t, noise_schedule, noise=None):\n    \"\"\"\n    Applies forward diffusion to input data.\n    \n    Args:\n        x_0: Clean input data\n        t: Timestep\n        noise_schedule: Dictionary containing pre-computed noise parameters\n        noise: Optional pre-generated noise\n        \n    Returns:\n        x_t: Noised version of input\n        noise: The noise that was added\n    \"\"\"\n    t_shape = (-1,) + (1,) * (x_0.ndim - 1)\n    _ts = t.view(*t_shape)\n    if noise is None:\n        noise = torch.randn_like(x_0)\n    assert _ts.max() &lt; len(noise_schedule[\"alphas_cumprod\"]), f\"t={_ts.max()} is larger than the length of noise_schedule: {len(noise_schedule['alphas_cumprod'])}\"\n    alpha_prod_t = noise_schedule[\"alphas_cumprod\"][_ts]\n    x_t = (alpha_prod_t ** 0.5) * x_0 + ((1 - alpha_prod_t) ** 0.5) * noise\n    return x_t, noise\n\n\nNoise Schedule\nThe noise shedule controls how noise is added to the data across timesteps during forward difffusion. The schedule is parametized by \\(\\beta_{min}\\) and \\(\\beta_{max}\\) which define the range of noise intensity values. It is a design choice. We will use a standard schedule in this tutorial.\nThe range of \\(\\beta_t\\) is empirically chosen to be between 1e-4 and 0.02. We can plot the noise schedule curves to visualize the blending ratio between the clean data and the noise, and how it changes over “time” through the diffusion process.\n\nbeta_min, beta_max = 1e-4, 0.02\n\ndef create_noise_schedule(n_T: int, device: torch.device) -&gt; Dict[str, torch.Tensor]:\n    betas = torch.linspace(beta_min, beta_max, n_T).to(device)\n    alphas = 1. - betas\n    alphas_cumprod = torch.cumprod(alphas, axis=0).to(device)\n    alphas_cumprod_prev = torch.cat([torch.ones(1).to(device), alphas_cumprod[:-1].to(device)])\n    sqrt_recip_alphas = torch.sqrt(1.0 / alphas).to(device)\n    sqrt_alphas_cumprod = torch.sqrt(alphas_cumprod).to(device)\n    sqrt_one_minus_alphas_cumprod = torch.sqrt(1. - alphas_cumprod)\n    posterior_variance = betas * (1. - alphas_cumprod_prev) / (1. - alphas_cumprod)\n\n    return {\n        \"betas\": betas,\n        \"alphas_cumprod\": alphas_cumprod,\n        \"sqrt_recip_alphas\": sqrt_recip_alphas,\n        \"sqrt_alphas_cumprod\": sqrt_alphas_cumprod,\n        \"sqrt_one_minus_alphas_cumprod\": sqrt_one_minus_alphas_cumprod,\n        \"posterior_variance\": posterior_variance,\n    }\n\n\nnoise_schedule = create_noise_schedule(config.num_denoising_steps, device)\n\n# plot the schedule\nplt.figure(figsize=(7, 3))\n\nplt.subplot(1, 2, 1); plt.plot(range(1000), noise_schedule[\"sqrt_alphas_cumprod\"].cpu().numpy())\nplt.title(r\"$\\sqrt{\\bar\\alpha_t}$: signal level\")\n\nplt.subplot(1, 2, 2); plt.plot(range(1000), noise_schedule[\"sqrt_one_minus_alphas_cumprod\"].cpu().numpy())\n_ = plt.title(r\"$\\sqrt{1-\\bar\\alpha_t}$: noise level\")\n\n\n\n\n\n\n\n\nLet’s take a look at the process in action.\n\nx_0 = next(iter(val_dataloader))\nx_0 = x_0.to(device)\nx_t_list = []\ncommon_noise = torch.randn_like(x_0)\n\nfig, axs = plt.subplots(1, 6, figsize=(20, 3))\nfor i, t in enumerate([0, 50, 100, 200, 500, 999]):\n    t_ = torch.full((x_0.shape[0],), t, device=device)\n    x_t = forward_diffusion(x_0, t_, noise_schedule, noise=common_noise)[0]\n    x_t = x_t.cpu()\n    axs[i].scatter(x_t[:,0], x_t[:,1], color='white', edgecolor='gray', s=5)\n    axs[i].set_axis_off()\n    axs[i].set_title('$q(\\mathbf{x}_{'+str(t)+'})$')\n\n\n\n\n\n\n\n\nAs the noise increases, the orderly spiral quickly fades away as it becomes a disordered pile of points (Gaussian distribution with mean 0 and variance 1).\n\n\nVisualize forward diffusion trajectories\nLet’s plot the trajectories of the forward diffusion process. For each data point \\(\\mathbf{x}_0\\), we plot the sequence:\n\\[ \\mathbf{x}_0, \\mathbf{x}_{10}, \\mathbf{x}_{20}, \\ldots, \\mathbf{x}_{990} \\]\nWould they look like straight lines or curves?\n\nTools for Visualizing Trajectories\nHere are some utility functions to plot the trajectories of the diffusion process.\n\nfrom typing import Tuple\n\n\nclass TrajectorySet:\n    def __init__(self, embeddings):\n        \"\"\"\n        Managing a set of trajectories, each of which is a sequence of embeddings.\n\n        Parameters\n        ----------\n        embeddings: (n_timesteps, n_samples, *embedding_dims). This assumes\n            the first dimension is time. And it is ordered from t=0 to t=n_timesteps-1.\n            With t=0 representing the clean data and t=n_timesteps-1 representing the noise.\n\n        \"\"\"\n        self.embeddings = embeddings\n        self.embeddings_2d = None\n    \n    def run_tsne(self, n_components: int = 2, seed: int = 0, **kwargs):\n        \"\"\"Run t-SNE on the embeddings.\n        \"\"\"\n        print(f\"Running t-SNE on {self.embeddings.shape} embeddings...\")\n        from sklearn.manifold import TSNE\n        tsne = TSNE(n_components=n_components, random_state=seed, **kwargs)\n        flattened_embeddings = self.embeddings.reshape(-1, self.embeddings.shape[-1])\n        flattened_embeddings_2d = tsne.fit_transform(flattened_embeddings)\n        self.embeddings_2d = flattened_embeddings_2d.reshape(self.embeddings.shape[0], self.embeddings.shape[1], -1)\n        print(f\"t-SNE done. Shape of 2D embeddings: {self.embeddings_2d.shape}\")\n        return self.embeddings_2d\n    \n    def plot_trajectories(\n            self,\n            n: int = 10,\n            show_figure: bool = False,\n            noise_color: Tuple[float, float, float] = (0, 0, 1),  # blue\n            data_color: Tuple[float, float, float] = (1, 0, 0),  # red\n            figsize: tuple = (6, 6),\n            xlim: Tuple[float, float] = None,\n            ylim: Tuple[float, float] = None,\n            with_ticks: bool = False,\n            with_lines: bool = True,\n            title: str = None,\n            tsne_seed: int = 0,\n            **kwargs):\n        \"\"\"Plot trajectories of some selected samples.\n\n        This assumes the first dimension is time. And it is ordered from t=0 to t=n_timesteps-1.\n        With t=0 representing the clean data and t=n_timesteps-1 representing the noise.\n\n        Parameters\n        ----------\n        n: int\n            number of samples to plot\n        figsize: tuple\n            figure size\n        kwargs:\n            other keyword arguments for matplotlib.pyplot.scatter\n        \"\"\"\n        import numpy as np\n        import matplotlib.pyplot as plt\n\n        colors = []\n        for t in range(self.embeddings.shape[0]):\n            # interpolate between noise_color and data_color\n            factor = t / (self.embeddings.shape[0] - 1)\n            colors.append(np.array(noise_color) * factor + np.array(data_color) * (1 - factor))\n        colors = np.array(colors)\n        \n        if self.embeddings_2d is None:\n            if self.embeddings.shape[2] == 2:\n                self.embeddings_2d = self.embeddings\n            else:\n                self.embeddings_2d = self.run_tsne(seed=tsne_seed)\n\n        traj = self.embeddings_2d[:, :n, :]\n        g = plt.figure(figsize=figsize)\n        plt.scatter(traj[0, :n, 0], traj[0, :n, 1], s=10, alpha=0.8, c=\"red\")  # real\n        plt.scatter(traj[-1, :n, 0], traj[-1, :n, 1], s=4, alpha=1, c=\"blue\")  # noise\n        plt.scatter(traj[:, :n, 0], traj[:, :n, 1], s=0.5, alpha=0.7, c=colors.repeat(n, axis=0))  # \"olive\"\n        if with_lines:  \n            plt.plot(traj[:, :n, 0], traj[:, :n, 1], c=\"olive\", alpha=0.3)\n        if xlim is not None:\n            plt.xlim(xlim)\n        if ylim is not None:\n            plt.ylim(ylim)\n        if with_lines:\n            plt.legend([\"Data\", \"Noise\", \"Intermediate Samples (color coded)\", \"Trajectory\"], loc=\"upper right\")\n        else:\n            plt.legend([\"Data\", \"Noise\", \"Intermediate Samples (color coded)\"], loc=\"upper right\")\n        if not with_ticks:\n            plt.xticks([])\n            plt.yticks([])\n        elif xlim is not None and ylim is not None:\n            plt.xticks(xlim)\n            plt.yticks(ylim)\n        if title is not None:\n            plt.title(title)\n        if show_figure:\n            plt.show()\n        \n        plt.tight_layout()\n        # save to bytes (png)\n        import io\n\n        bytes_io = io.BytesIO()\n        g.savefig(bytes_io, format=\"png\")\n        return bytes_io.getvalue()\n\n\n\nPlotting the forward diffusion trajectories\n\n# Generate forward diffusion trajectories (would they look like straight lines or curves?)\n\nx_clean = next(iter(val_dataloader))\nx_clean = x_clean.to(device)\n\n# Run forward diffusion\nx_t_list = []\ntorch.manual_seed(0)\ncommon_noise = torch.randn_like(x_clean)\n\nfor t in range(0, 1000, 10):\n    t_ = torch.full((x_clean.shape[0],), t, device=device)\n    x_t = forward_diffusion(x_clean, t_, noise_schedule, noise=common_noise)[0]\n    x_t_list.append(x_t)\n\ntraj = torch.stack(x_t_list).cpu().numpy()\ntraj_set = TrajectorySet(traj)\n_ = traj_set.plot_trajectories(n=60, show_figure=True, figsize=(8, 8))\n\n\n\n\n\n\n\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\nThe dotted lines are the trajectories of the forward diffusion. Good data points on the spiral pattern (red) are gradually transformed into noise (blue), which is a Gaussian distribution. The color gradient represents the time step or the noise level. We can see that the trajectories are generally smooth curves.\n\n\n\nForward Diffusion is a Form of Data Augmentation\nWe can consider this as a data augmentation process. For each clean data point, one or more noisy data points are created by adding different amounts of noise to it. We then ask to the model predict the noise based on corrupted data points.\nOne could write a new dataset class to represent this data augmentation process:\nclass DiffusionAugmentedDataset:\n    def __getitem__(self, index):\n        x_0 = self.data[index]\n        t = torch.randint(0, self.num_timesteps, (1,))\n        x_t, noise = forward_diffusion(x_0, t, self.noise_schedule)\n        inputs = (t, x_t)\n        targets = noise\n        return inputs, targets\nBy doing so, we cast the generation problem as a supervised learning problem.\nIn this notebook, to keep the code simple, we implement the data augmentation inside the training step on mini-batches, as commonly done in practice.",
    "crumbs": [
      "Home",
      "Generating a 2D Point Cloud",
      "Diffusion",
      "Training a Diffusion Model on 2D Points"
    ]
  },
  {
    "objectID": "1_1_Diffusion 2D Toy.html#denoising-reverse-diffusion",
    "href": "1_1_Diffusion 2D Toy.html#denoising-reverse-diffusion",
    "title": "Training a Diffusion Model on 2D Points",
    "section": "Denoising: Reverse diffusion",
    "text": "Denoising: Reverse diffusion\nAs mentioned before, instead of learning to generate from scratch, we train a model to predict and remove noise at each step, so that we can get back to the clean data.\nThis model is called a denoising model. It predicts the noise at each time step \\(t\\) and the current state of the data \\(x_t\\). The model is parameterized as a neural network:\n\\[\n\\mathbf\\epsilon_{\\theta}(t, \\mathbf{x_t})\n\\]\nwhere \\(\\theta\\) is the learnable parameters, \\(t\\) is a scalar value that represents the current time step or progress in the diffusion process. \\(t\\) is also referred to as the noise level, with 0 indicating the clean data and \\(T\\) (e.g. \\(T=1000\\)) indicating the fully noisy data.\n\n\n\n\n\n\nNote\n\n\n\nSide note: There are other ways to parameterize the problem. For example, some train a model to predict the previous state \\(x_{t-1}\\) directly. In this tutorial, we focus on the standard noise prediction formulation.\n\n\nWhen generating data with the denoising model, we use this model to iteratively remove noise from the data point and move it back towards \\(x_0\\). This is the reverse diffusion process.\nWe’ll define the individual denoising step for this process later.\nFor now, a key challenge is designing a model architecture that can effectively understand both the noisy input \\(x_t\\) and the current time step \\(t\\).",
    "crumbs": [
      "Home",
      "Generating a 2D Point Cloud",
      "Diffusion",
      "Training a Diffusion Model on 2D Points"
    ]
  },
  {
    "objectID": "1_1_Diffusion 2D Toy.html#network-architecture-of-the-denoising-model",
    "href": "1_1_Diffusion 2D Toy.html#network-architecture-of-the-denoising-model",
    "title": "Training a Diffusion Model on 2D Points",
    "section": "Network Architecture of the Denoising Model",
    "text": "Network Architecture of the Denoising Model\nThe network architecture of the denoising model can be quite flexible, as long as it can take the time step \\(t\\) and the noisy data \\(x_t\\) as input, and output a tensor of the same shape as \\(x_t\\). We’ll implement three different model architectures to incorporate time information with the data:\nModel 1: Basic - Simplest approach: direct concatenation of time with noisy data.\nModel 2: Sinusoidal Time Embedding - Inspired by transformer positional encodings. - Better representation of the time step.\nModel 3: Time-Modulated Linear Layers - The output of each linear transformation is modulated by a learned time embedding through element-wise multiplication, allowing the network to adapt its representations based on the timestep. - Time information influences every layer.\nBelow is a table summarizing the design choices:\n\n\n\n\n\n\n\n\n\nArchitecture\nTimestep Handling\nPros\nCons\n\n\n\n\nModel 1: Basic\nDirect concatenation\nSimple, lightweight\nLimited temporal understanding\n\n\nModel 2: Sinusoidal\nTransformer-style embeddings\nBetter periodic patterns\nMore parameters\n\n\nModel 3: Modulation\nTime influences all layers\nMost expressive\nMost complex\n\n\n\n\nModel 1: Simple Concatenation\nOur first model is a simple three layer MLP. Note that in the foward method, we concatinate the time to the noisy data before each foward pass.\n\nclass Model1(nn.Module):\n    def __init__(self, hidden_features: list[int]):\n        super().__init__()\n        \n        layers = []\n        input_dim = 3  # The input dimension (t and x combined)\n        \n        for hidden_dim in hidden_features:\n            layers.append(nn.Linear(input_dim, hidden_dim))\n            layers.append(nn.ReLU())\n            input_dim = hidden_dim\n        \n        # Output layer\n        layers.append(nn.Linear(input_dim, 2))\n        layers.append(nn.Tanh())\n        \n        self.net = nn.Sequential(*layers)\n\n    def forward(self, t, x):\n        t = t / 1000.\n        t = t.reshape(-1, 1)\n        return self.net(torch.cat([t, x], 1))\n\n# Example usage\nhidden_dimensions = [512, 512]  # Example list of hidden dimensions\ndevice = \"cpu\"\nmodel1 = Model1(hidden_dimensions).to(device)\n\nprint(f\"Model parameters: {sum(p.numel() for p in model1.parameters())}\")\n\nModel parameters: 265730\n\n\n\n\nModel 2: Sinusoidal Time Embedding Concatination\nIn this model, we utilize a 3-layer MLP. Instead of concatenating the raw time embedding directly to the data embedding, we first transform the time embedding into a sinusoidal embedding and then concatenate it with the data embedding.\nBackground\nThis model builds upon the idea of incorporating temporal information into the embedding space, inspired by the effectiveness of sinusoidal embeddings in positional encoding for transformers. The sinusoidal representation allows the model to capture periodicity and smooth transitions in the time domain, potentially improving its ability to generalize across different temporal contexts.\n\nclass MLP(nn.Sequential):\n    def __init__(\n        self,\n        in_features: int,\n        out_features: int,\n        hidden_features: list[int],\n    ):\n        layers = []\n\n        for a, b in zip(\n            (in_features, *hidden_features),\n            (*hidden_features, out_features),\n        ):\n            layers.extend([nn.Linear(a, b), nn.ELU()])\n\n        super().__init__(*layers[:-1])\n\n\nclass Model2(nn.Module):\n    def __init__(self, features: int, freqs: int = 16, **kwargs):\n        super().__init__()\n\n        self.net = MLP(2 * freqs + features, features, **kwargs)\n\n        self.register_buffer('freqs', torch.arange(1, freqs + 1) * torch.pi)\n\n    def forward(self, t, x):\n        t = t / 1000.\n        t = self.freqs * t[..., None]\n        t = torch.cat((t.cos(), t.sin()), dim=-1)\n        t = t.expand(*x.shape[:-1], -1)\n\n        return self.net(torch.cat((t, x), dim=-1))\n\n\nmodel2 = Model2(features=2, hidden_features=[512, 512]).to(device)\nprint(f\"model params: {sum(p.numel() for p in model2.parameters())}\")\n\nmodel params: 281602\n\n\n\n\nModel 3: Time-Modulated Linear Layers\nModel 3 (reference) introduces a time-conditioned architecture where time is incorporated directly into the network through TimeLinear layers. Instead of directly concatenating time embeddings with the data embeddings, the output of each linear transformation is modulated by a learned time embedding through element-wise multiplication, allowing the network to adapt its representations based on the timestep.\n\nclass TimeEmbedding(nn.Module):\n    \"\"\"\n    Time embedding module that embeds the time step into a hidden representation.\n    Args:\n        hidden_size: the size of the hidden representation.\n        frequency_embedding_size: the size of the frequency embedding\n    \"\"\"\n    def __init__(self, hidden_size, frequency_embedding_size=256):\n        super().__init__()\n        self.mlp = nn.Sequential(\n            nn.Linear(frequency_embedding_size, hidden_size, bias=True),\n            nn.SiLU(),\n            nn.Linear(hidden_size, hidden_size, bias=True),\n        )\n        self.frequency_embedding_size = frequency_embedding_size\n\n    @staticmethod\n    def timestep_embedding(t, dim, max_period=10000):\n        \"\"\"\n        Create sinusoidal timestep embeddings.\n        :param t: a 1-D Tensor of N indices, one per batch element.\n                          These may be fractional.\n        :param dim: the dimension of the output.\n        :param max_period: controls the minimum frequency of the embeddings.\n        :return: an (N, D) Tensor of positional embeddings.\n        \"\"\"\n        # https://github.com/openai/glide-text2im/blob/main/glide_text2im/nn.py\n        half = dim // 2\n        freqs = torch.exp(\n            -math.log(max_period)\n            * torch.arange(start=0, end=half, dtype=torch.float32)\n            / half\n        ).to(device=t.device)\n        args = t[:, None].float() * freqs[None]\n        embedding = torch.cat([torch.cos(args), torch.sin(args)], dim=-1)\n        if dim % 2:\n            embedding = torch.cat(\n                [embedding, torch.zeros_like(embedding[:, :1])], dim=-1\n            )\n        return embedding\n\n    def forward(self, t: torch.Tensor):\n        \"\"\"Forward pass of the module.\"\"\"\n\n        if t.ndim == 0:\n            t = t.unsqueeze(-1)\n        t_freq = self.timestep_embedding(t, self.frequency_embedding_size)\n        t_emb = self.mlp(t_freq)\n        return t_emb\n\n\nclass TimeLinear(nn.Module):\n    \"\"\"\n    Time linear layer that applies a linear transformation with time-dependent weights.\n    \"\"\"\n\n    def __init__(self, dim_in: int, dim_out: int, num_timesteps: int):\n        \"\"\"\n        Args:\n            dim_in: the number of input features.\n            dim_out: the number of output features.\n            num_timesteps: the number of timesteps.\n        \"\"\"\n        super().__init__()\n        self.dim_in = dim_in\n        self.dim_out = dim_out\n        self.num_timesteps = num_timesteps\n\n        self.time_embedding = TimeEmbedding(dim_out)\n        self.fc = nn.Linear(dim_in, dim_out)\n\n    def forward(self, x: torch.Tensor, t: torch.Tensor):\n        x = self.fc(x)\n        alpha = self.time_embedding(t).view(-1, self.dim_out)\n        return alpha * x\n\n\nclass Model3(nn.Module):\n    def __init__(\n        self, hidden_features: list[int], num_timesteps: int, dim_in:int = 2, dim_out:int = 2,\n    ):\n        super().__init__()\n        \"\"\"\n        Build a noise estimating network.\n\n        Args:\n            hidden_features: dimensions of hidden features\n            num_timesteps: number of timesteps\n            dim_in: dimension of input\n            dim_out: dimension of output\n        \"\"\"\n\n        layers = []\n        dims = [dim_in] + hidden_features\n\n        # Build MLP layers with time-dependent linear layers\n        for i in range(len(dims)-1):\n            layers.append(TimeLinear(dims[i], dims[i+1], num_timesteps))\n            layers.append(nn.ReLU())\n\n        # Final layer to output noise prediction\n        layers.append(TimeLinear(dims[-1], dim_out, num_timesteps))\n\n        self.net = nn.Sequential(*layers)\n        self.num_timesteps = num_timesteps\n\n\n    def forward(self, x: torch.Tensor, t: torch.Tensor):\n        \"\"\"\n        Args:\n            x: the noisy data after t period diffusion\n            t: the time that the forward diffusion has been running\n        \"\"\"\n        for layer in self.net:\n            if isinstance(layer, TimeLinear):\n                x = layer(x, t)\n            else:\n                x = layer(x)\n        ######################\n        return x\n\n\nmodel3 = Model3(\n    dim_in=2,\n    dim_out=2,\n    hidden_features=[512, 512],\n    num_timesteps=config.num_denoising_steps,\n).to(device)\nprint(f\"model params: {sum(p.numel() for p in model3.parameters())}\")\n\nmodel params: 1054218",
    "crumbs": [
      "Home",
      "Generating a 2D Point Cloud",
      "Diffusion",
      "Training a Diffusion Model on 2D Points"
    ]
  },
  {
    "objectID": "1_1_Diffusion 2D Toy.html#training-loop",
    "href": "1_1_Diffusion 2D Toy.html#training-loop",
    "title": "Training a Diffusion Model on 2D Points",
    "section": "Training loop",
    "text": "Training loop\nWe teach the model to predict the “common noise” \\(\\mathbf\\epsilon\\) that produced the diffusion sequence. The model bases its prediction on \\(x_t\\) at any time (or noise level) \\(t\\). No matter where it is, the model needs to help the data point get back to \\(x_0\\).\nFor each clean training example \\(x_0\\), we sample a random time \\(t\\) and compute the forward diffusion of \\(x_0\\) to get the noisy data \\(x_t\\). We then compute the predicted noise \\(\\mathbf\\epsilon\\) using the model.\nA simple MSE loss is used to compare the predicted noise with the true noise. This feedback is propagated back to the model to update its parameters.\nThe core training step can be implemented in under 10 lines of code.\nWe repeat it a number of times in the training loop.\n\ndef train(model: nn.Module, optimizer: torch.optim.Optimizer, steps: int=100, silent: bool=False) -&gt; float:\n  criterion = MSELoss()\n  model.train()\n  if not silent:\n    print(\"Training on device:\", device)\n  max_train_steps = steps\n\n  loss = None\n  step = 0\n  while step &lt; max_train_steps:\n    progress_bar = tqdm(train_dataloader, disable=silent)\n    for x_0 in progress_bar:\n      x_0 = x_0.float().to(device)  # x_0 is the clean data\n\n      #######################################\n      # Start of the core training step\n      #######################################\n      optimizer.zero_grad()\n      true_noise = common_noise = torch.randn(x_0.shape).to(device)\n      t = torch.randint(0, config.num_denoising_steps, (x_0.shape[0],), device=device).long()\n      x_t, _ = forward_diffusion(x_0, t, noise_schedule, noise=common_noise)\n\n      predicted_noise = model(t=t, x=x_t)\n\n      loss = criterion(predicted_noise, true_noise)\n      loss.backward()\n      torch.nn.utils.clip_grad_norm_(model.parameters(), 1)  # try commenting it out\n      optimizer.step()\n      #######################################\n      # End of the core training step\n      #######################################\n\n      if not silent:\n        progress_bar.set_postfix({\"loss\": loss.cpu().item()})\n\n      step += 1\n\n      if step &gt;= max_train_steps:\n        if not silent:\n          print(f\"Reached the max training steps:\", max_train_steps)\n        break\n\n  return loss\n\n\nTrain for 100 steps\nFirst, train the model for a small number (100) of steps to make sure the training loop is working.\n\n# Train the model\nmodel3_optimizer = optim.AdamW(model3.parameters(), lr=config.learning_rate, weight_decay=config.weight_decay)\nmodel3_loss = train(model3, model3_optimizer, steps=100)\n\nTraining on device: cpu\n\n\n\n\n\nReached the max training steps: 100\n\n\nIt does not take long to train 100 steps. The loss goes down in the progress bar. Did the model learn anything?\n\nCheck the quality of the denoising model\nLet’s see what the output looks like when the model is trained for only 100 steps.\n\n# visualize the sampled data points\ndef visualize_sampled_data(model):\n  # print(\"Loss of the denoising model:\", loss.item())\n  x_T = torch.randn(128, 2)\n  x_sampled = generate_samples_by_denoising(model, x_T, noise_schedule, n_T=1000, device=device).cpu().detach().numpy()\n\n  # plt.scatter(x_sampled[:, 0], x_sampled[:, 1])\n  fig, axs = plt.subplots(1, 1, figsize=(5, 5))\n  axs.scatter(x_sampled[:,0], x_sampled[:,1], color='white', edgecolor='gray', s=5)\n  # axs.set_axis_off()\n  # plt.xlim(-3.6, 3.6)\n  # plt.ylim(-3.6, 3.6)\n\n\nvisualize_sampled_data(model3)\n\n\n\n\n\n\n\n\n\n\n\nStill looks like noise. What if we increase the length of training?\n\n# Train some more\nmodel3_loss = train(model3, model3_optimizer, steps=1000)\nprint(\"loss:\", model3_loss.item())\nvisualize_sampled_data(model3)\n\nTraining on device: cpu\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReached the max training steps: 1000\nloss: 0.3116355836391449\n\n\n\n\n\n\n\n\n\n\n\n\nThat’s more like it! In just 1000 training steps, we were able to go from complete noise to a fairly representative spiral.\n\n\nVisualize Learned Denoising Trajectories\nWhat do the denoising trajectories look like now, after 1000 training steps?\n\n# visualize denoising trajectories\nvisualize_denoising_trajectories(model3, n=160)\n\n\n\n\n\n\n\n\n\n\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\nWow, the blue dots (random starting points) took many turns and swirls to get back to the red dots (clean data)! It is surprising that they are able to even find the way back, let alone with such accuracy. Talk about the power of Math.\n\n\n\nAnimate the learning process\nTo get a better idea of what the model’s learning progress looked like, let’s checkpoint the model and do a sampling every 100 training steps.\nClick the play button to see the animation of generated samples as the model learns.\n\nmodel3 = Model3(\n    dim_in=2,\n    dim_out=2,\n    hidden_features=[128, 128, 256],\n    num_timesteps=config.num_denoising_steps,\n).to(device)\nmodel3_optimizer = optim.AdamW(model3.parameters(), lr=config.learning_rate, weight_decay=config.weight_decay)\n\nsamples = []\nsteps_to_checkpoint = [0, 100, 200, 300, 400, 500, 600, 700, 800, 900]\nall_trajs = []\n\nfor i in range(len(steps_to_checkpoint)):\n    if i &gt; 0:\n        steps_to_train = steps_to_checkpoint[i] - steps_to_checkpoint[i-1]\n    else:\n        steps_to_train = i\n    model3_loss = train(model3, model3_optimizer, steps=steps_to_train, silent=True)\n    torch.manual_seed(0)\n    traj = generate_samples_by_denoising(\n        model3, torch.randn(128, 2), noise_schedule, n_T=1000, device=device, return_full_trajectory=True, silent=True)\n    traj = traj.detach().cpu().numpy()\n    reversed_traj = traj[::-20, :, :]\n    traj_set = TrajectorySet(reversed_traj)\n    all_trajs.append(traj_set)\n    samples.append(traj[-1])\n\n\nimport matplotlib.animation as animation\nfrom IPython.display import HTML\n\nfig, ax = plt.subplots()\nsc = ax.scatter(samples[i][:, 0], samples[i][:, 1], color='white', edgecolor='gray', s=5)\nax.set_xlim(-5, 5)\nax.set_ylim(-5, 5)\nfig.suptitle(\"Samples\")\n\ndef update(i):\n    sc.set_offsets(samples[i])\n    return sc,\n\n\nani = animation.FuncAnimation(fig, update, frames=range(len(steps_to_checkpoint)), interval=1000, blit=True, repeat=False)\nHTML(ani.to_jshtml())\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\nDrag the slider to see the denoising trajectories as the model learns.\n\n# A shared state to store the frames for the vector field animation.\n_state = {\n    \"frames\": {},\n}\n\nfrom ipywidgets import interact\n\ndef show_traj_at_learning_step(i=0):\n    train_steps = steps_to_checkpoint[i]\n    bytes_value = all_trajs[i].plot_trajectories(\n        n=32, show_figure=True, figsize=(6, 6),\n        with_ticks=True, title=f\"train steps = {train_steps}\",\n        xlim=(-5, 5), ylim=(-5, 7),\n        with_lines=True,\n    )\n    _state[\"frames\"][train_steps] = bytes_value\n\n_ = interact(show_traj_at_learning_step, i=(0, len(all_trajs)-1, 1))",
    "crumbs": [
      "Home",
      "Generating a 2D Point Cloud",
      "Diffusion",
      "Training a Diffusion Model on 2D Points"
    ]
  },
  {
    "objectID": "1_1_Diffusion 2D Toy.html#model-1",
    "href": "1_1_Diffusion 2D Toy.html#model-1",
    "title": "Training a Diffusion Model on 2D Points",
    "section": "Model 1",
    "text": "Model 1\n\nmodel1 = Model1(hidden_features=[512, 256, 256]).to(device)\nprint(f\"model params: {sum(p.numel() for p in model1.parameters())}\")\n\nmodel1_optimizer = optim.AdamW(model1.parameters(), lr=config.learning_rate, weight_decay=config.weight_decay)\nmodel1_loss = train(model1, model1_optimizer, steps=5000)\nprint(\"model1_loss:\", model1_loss.item())\nvisualize_sampled_data(model1)\n\nmodel params: 199682\nTraining on device: cpu\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReached the max training steps: 5000\nmodel1_loss: 0.34836021065711975\n\n\n\n\n\n\n\n\n\n\n\n\nThe shape looks terrible. But it is actually not that bad. Some points stray too far due to error accumulation. Let’s clip the points to a reasonable range.\n\nvisualize_sampled_data(model1)\nplt.xlim(-3.9, 3.9)\nplt.ylim(-3.9, 3.9)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nWe excluded the points that strayed too far. If we include them, their values need to be clipped to a small range (e.g. -3.9 to 3.9) and they will lie on the boundary of a square. You can see this in the next section where we plot them 3 models side by side.",
    "crumbs": [
      "Home",
      "Generating a 2D Point Cloud",
      "Diffusion",
      "Training a Diffusion Model on 2D Points"
    ]
  },
  {
    "objectID": "1_1_Diffusion 2D Toy.html#model-2",
    "href": "1_1_Diffusion 2D Toy.html#model-2",
    "title": "Training a Diffusion Model on 2D Points",
    "section": "Model 2",
    "text": "Model 2\n\nmodel2 = Model2(features=2, hidden_features=[512, 256, 256]).to(device)\nprint(f\"model params: {sum(p.numel() for p in model2.parameters())}\")\n\nmodel2_optimizer = optim.AdamW(model2.parameters(), lr=config.learning_rate, weight_decay=config.weight_decay)\nmodel2_loss = train(model2, model2_optimizer, steps=5000)\nprint(\"model2_loss:\", model2_loss.item())\nvisualize_sampled_data(model2)\n\nmodel params: 215554\nTraining on device: cpu\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReached the max training steps: 5000\nmodel2_loss: 0.3272062838077545",
    "crumbs": [
      "Home",
      "Generating a 2D Point Cloud",
      "Diffusion",
      "Training a Diffusion Model on 2D Points"
    ]
  },
  {
    "objectID": "1_1_Diffusion 2D Toy.html#model-3",
    "href": "1_1_Diffusion 2D Toy.html#model-3",
    "title": "Training a Diffusion Model on 2D Points",
    "section": "Model 3",
    "text": "Model 3\n\nmodel3 = Model3(\n    dim_in=2,\n    dim_out=2,\n    hidden_features=[128, 128, 128],\n    num_timesteps=config.num_denoising_steps,\n).to(device)\nprint(f\"model params: {sum(p.numel() for p in model3.parameters())}\")\n\nmodel3_optimizer = optim.AdamW(model3.parameters(), lr=config.learning_rate, weight_decay=config.weight_decay)\nmodel3_loss = train(model3, model3_optimizer, steps=5000)\nprint(\"model3_loss:\", model3_loss.item())\nvisualize_sampled_data(model3)\n\nmodel params: 182410\nTraining on device: cpu\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReached the max training steps: 5000\nmodel3_loss: 0.2785179913043976\n\n\n\n\n\n\n\n\n\n\n\n\n\nimport torch\n\n# Sample points from the model\ndef generate_points(model):\n    with torch.no_grad():\n        x_T = torch.randn(128, 2)\n        x_sampled = generate_samples_by_denoising(model, x_T, noise_schedule, n_T=1000, device=device)\n    return x_sampled.cpu().numpy()\n\n\n# The target spiral points for comparison\ntarget_spiral = next(iter(train_dataloader))\n\ngenerated_points_1 = generate_points(model1)\ngenerated_points_1 = np.clip(generated_points_1, -3.9, 3.9)\nchamfer_dist = chamfer_distance(generated_points_1, target_spiral, direction='bi')\nprint(\"Model 1 Size:\", sum(p.numel() for p in model1.parameters()), \"Chamfer Distance:\", chamfer_dist)\n\n\ngenerated_points_2 = generate_points(model2)\nchamfer_dist = chamfer_distance(generated_points_2, target_spiral, direction='bi')\nprint(\"Model 2 Size:\", sum(p.numel() for p in model2.parameters()), \"Chamfer Distance:\", chamfer_dist)\n\n# Calculate Chamfer distance\ngenerated_points_3 = generate_points(model3)\nchamfer_dist = chamfer_distance(generated_points_3, target_spiral, direction='bi')\nprint(\"Model 3 Size:\", sum(p.numel() for p in model3.parameters()), \"Chamfer Distance:\", chamfer_dist)\n\n# # visualize the sampled data points side by side\ndef visualize_sampled_data_side_by_side(models, generated_points):\n    fig, axs = plt.subplots(1, len(models) + 1, figsize=(5 * len(models), 5))\n    # Add ground truth\n    axs[0].scatter(target_spiral[:, 0], target_spiral[:, 1], color='white', edgecolor='gray', s=5)\n    axs[0].set_title(\"Ground truth\")\n\n    for i, (model, points) in enumerate(zip(models, generated_points)):\n        axs[i+1].scatter(points[:,0], points[:,1], color='white', edgecolor='gray', s=5)\n        axs[i+1].set_title(model.__class__.__name__)\n\nvisualize_sampled_data_side_by_side([model1, model2, model3], [generated_points_1, generated_points_2, generated_points_3])\n\n\n\n\nModel 1 Size: 199682 Chamfer Distance: 1.1920804347906755\n\n\n\n\n\nModel 2 Size: 215554 Chamfer Distance: 0.3012108333136537\n\n\n\n\n\nModel 3 Size: 182410 Chamfer Distance: 0.18239878315537367\n\n\n\n\n\n\n\n\n\nLet’s compare how our three architectural approaches performed:\n\nModel 1 (Basic): Shows decent spiral structure but with significant noise\nModel 2 (Sinusoidal): Improved spiral with less noise\nModel 3 (Modulation): Cleanest results with least noise\n\nThe Chamfer distance also tells us the same story, with model 3 having the lowest distance to the target spiral.",
    "crumbs": [
      "Home",
      "Generating a 2D Point Cloud",
      "Diffusion",
      "Training a Diffusion Model on 2D Points"
    ]
  },
  {
    "objectID": "1_1_Diffusion 2D Toy.html#key-takeaways-and-discussion",
    "href": "1_1_Diffusion 2D Toy.html#key-takeaways-and-discussion",
    "title": "Training a Diffusion Model on 2D Points",
    "section": "Key Takeaways and Discussion",
    "text": "Key Takeaways and Discussion\nWe’ve seen how diffusion models can learn to generate complex patterns, even with a simple architecture. We have learned:\n\nForward diffusion provides a structured way to add noise to the clean data. It can be viewed as a data augmentation technique.\nReverse diffusion is a generative process that uses a trained denoising model to predict and remove noise in an iterative manner.\nThe choice of architecture significantly impacts generation quality. Time embedding strategy matters for model performance.\n\n\nDiscussion Questions\n\nWhat do you think is the role of forward diffusion for training?\nWhat is the role of time embeddings?\nDo you think the model architecture matters for the quality of the generated samples?\nHow many training examples are sufficient for the model to learn the data distribution?",
    "crumbs": [
      "Home",
      "Generating a 2D Point Cloud",
      "Diffusion",
      "Training a Diffusion Model on 2D Points"
    ]
  },
  {
    "objectID": "1_1_b_Diffusion_2D_hyperparams.html",
    "href": "1_1_b_Diffusion_2D_hyperparams.html",
    "title": "Our First Pareto Frontier Plot",
    "section": "",
    "text": "A defining characteristic of deep learning and the more recent LLM revolution is the existence of the scaling law, which states that the quality of AI improves predictively as a function of data, compute and model size. The power of this law is so strong that, across domains like vision and NLP, researchers learned the bitter lesson that: a smart algorithm design do not quite matter, unless it scales with data and compute.\nReality is more nuanced. First, scaling laws shift. Every once in a while (roughly a decade), a breakthrough in model architecture or training methods unlocks more efficient use of data and compute. In the 2020s, Transformers dominate architecture choices, while diffusion is a break-through in training algorithms. Secondly, optimal hyperparameters need to be found, through time, money and sometimes tears. Poorly tuned runs fall off the Pareto frontier and are absent from the clean scaling law curves often presented.\nWith this broader context in mind, let’s scale things down to a simple 2D toy example.\nWe’ll plot quality vs. cost, where each data point represents a training run with a specific hyperparameter setting:\n\\[\\left( q(h_1), c(h_1) \\right), \\left( q(h_2), c(h_2) \\right) \\cdots\\]\nHere \\(h_1, h_2, \\cdots\\) are different configurations of hyperparameters, \\(q\\) measures quality and \\(c\\) represents cost.\nAs we collect more data points, a Pareto frontier will emerge, highlighting the best trade-offs between quality and cost. This small-scale experiment illustrates a core research strategy in modern generative modeling: optimizing for efficiency along the Pareto frontier.\nIn practice, measuring the Pareto frontier for specific use cases is very valuable. It helps guide model and algorithm choices to achieve the highest possible quality within a given budget.",
    "crumbs": [
      "Home",
      "Generating a 2D Point Cloud",
      "Diffusion",
      "Our First Pareto Frontier Plot"
    ]
  },
  {
    "objectID": "1_1_b_Diffusion_2D_hyperparams.html#experiment-with-hyperparameters",
    "href": "1_1_b_Diffusion_2D_hyperparams.html#experiment-with-hyperparameters",
    "title": "Our First Pareto Frontier Plot",
    "section": "Experiment with hyperparameters",
    "text": "Experiment with hyperparameters\nTo generate our scaling law plot, we first need a series of training runs across different hyperparameter settings.\n\nDefine the grid of hyperparameters\n\nfrom lib_1_1.eval import chamfer_distance\nfrom lib_1_1.model import Model1, Model2, Model3\nfrom lib_1_1.training_loop  import train\nfrom lib_1_1.data import load_data\nfrom lib_1_1.config import TrainingConfig\nfrom lib_1_1.diffusion import generate_samples_by_denoising, create_noise_schedule\nfrom dataclasses import dataclass\nimport torch\nimport torch.optim as optim\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n@dataclass\nclass ModelConfig:\n    model: str\n    hidden_features: list[int]\n    train_steps: int\n\nmodels = [\"model1\", \"model2\", \"model3\"]\nlayers = [[128], [256, 256], [512, 512], [256, 256, 256, 256, 256]]\ntrain_steps = [1000, 5000, 10000]\n\nmodel_configs = [\n    ModelConfig(model=model, hidden_features=layer, train_steps=num_timestep)\n    for model in models for layer in layers for num_timestep in train_steps\n]\nprint(f\"Number of model configs: {len(model_configs)}\")\n\nNumber of model configs: 36\n\n\n\n\nRun model training experiments\nFor quality metric, we use the Chamfer distance: the lower the better. For cost, we can use model size.\nThis can take about 10 minutes to run. The output can be quite long. Feel free to skip to the next section using the side navigation.\n\ntrain_config = TrainingConfig()\ntrain_dataloader, _ = load_data(train_config)\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n\nnoise_schedule = create_noise_schedule(train_config.num_denoising_steps, device)\nlosses = []\nchamfer_distances = []\ntarget_spiral = next(iter(train_dataloader))\nfor i, m_config in enumerate(model_configs):\n    print(f\"Experiment run {i+1} of {len(model_configs)}\")\n    model = None\n    if m_config.model == \"model1\":\n        model = Model1(hidden_features=m_config.hidden_features, num_timesteps=m_config.train_steps).to(device)\n    elif m_config.model == \"model2\":\n        model = Model2(hidden_features=m_config.hidden_features, num_timesteps=m_config.train_steps).to(device)\n    elif m_config.model == \"model3\":\n        model = Model3(\n            hidden_features=m_config.hidden_features,\n            num_timesteps=m_config.train_steps,\n        ).to(device)\n    print(f\"{m_config.model}, Layers: {m_config.hidden_features}, Train Steps: {m_config.train_steps}\")\n    print(f\"model params: {sum(p.numel() for p in model.parameters())}\")\n    optimizer = optim.AdamW(model.parameters(), lr=train_config.learning_rate, weight_decay=train_config.weight_decay)\n    loss = train(model, train_dataloader=train_dataloader, optimizer=optimizer, steps=m_config.train_steps, noise_schedule=noise_schedule, device=device)\n    print(\"loss:\", loss.item())\n    generated_points = generate_samples_by_denoising(model, torch.randn(128, 2), noise_schedule, n_T=1000, device=device)\n    generated_points = generated_points.cpu().numpy()\n    if m_config.model == \"model1\":\n        # special treatment for model1\n        generated_points = np.clip(generated_points, -3.6, 3.6)\n    chamfer_dist = chamfer_distance(generated_points, target_spiral, direction='bi')\n    chamfer_distances.append(chamfer_dist)\n    print(\"Chamfer Distance:\", chamfer_dist)\n\nmodel1, Layers: [128], Train Steps: 1000\nmodel params: 770\nTraining on device: cuda\n\n\n\n\n\nReached the max training steps: 1000\nloss: 0.4509263038635254\n\n\n100%|██████████| 1000/1000 [00:00&lt;00:00, 1025.70it/s, std=50.1]\n\n\nChamfer Distance: 40.83210172000426\nmodel1, Layers: [128], Train Steps: 5000\nmodel params: 770\nTraining on device: cuda\n\n\n\n\n\nReached the max training steps: 5000\nloss: 0.46364229917526245\n\n\n100%|██████████| 1000/1000 [00:00&lt;00:00, 1068.78it/s, std=64.4]\n\n\nChamfer Distance: 47.438745148771524\nmodel1, Layers: [128], Train Steps: 10000\nmodel params: 770\nTraining on device: cuda\n\n\n\n\n\nReached the max training steps: 10000\nloss: 0.4620330333709717\n\n\n100%|██████████| 1000/1000 [00:00&lt;00:00, 1053.48it/s, std=58.7]\n\n\nChamfer Distance: 43.8795945442217\nmodel1, Layers: [256, 256], Train Steps: 1000\nmodel params: 67330\nTraining on device: cuda\n\n\n\n\n\nReached the max training steps: 1000\nloss: 0.433613121509552\n\n\n100%|██████████| 1000/1000 [00:00&lt;00:00, 1025.87it/s, std=55.3]\n\n\nChamfer Distance: 34.900078827080634\nmodel1, Layers: [256, 256], Train Steps: 5000\nmodel params: 67330\nTraining on device: cuda\n\n\n\n\n\nReached the max training steps: 5000\nloss: 0.41779962182044983\n\n\n100%|██████████| 1000/1000 [00:00&lt;00:00, 1030.43it/s, std=49.4]\n\n\nChamfer Distance: 37.8007218336046\nmodel1, Layers: [256, 256], Train Steps: 10000\nmodel params: 67330\nTraining on device: cuda\n\n\n\n\n\nReached the max training steps: 10000\nloss: 0.4159809947013855\n\n\n100%|██████████| 1000/1000 [00:00&lt;00:00, 1027.20it/s, std=56.5]\n\n\nChamfer Distance: 41.10801919781987\nmodel1, Layers: [512, 512], Train Steps: 1000\nmodel params: 265730\nTraining on device: cuda\n\n\n\n\n\nReached the max training steps: 1000\nloss: 0.4471423327922821\n\n\n100%|██████████| 1000/1000 [00:01&lt;00:00, 982.32it/s, std=64.2]\n\n\nChamfer Distance: 44.991998336324244\nmodel1, Layers: [512, 512], Train Steps: 5000\nmodel params: 265730\nTraining on device: cuda\n\n\n\n\n\nReached the max training steps: 5000\nloss: 0.3601759076118469\n\n\n100%|██████████| 1000/1000 [00:00&lt;00:00, 1003.78it/s, std=64] \n\n\nChamfer Distance: 47.84504672777196\nmodel1, Layers: [512, 512], Train Steps: 10000\nmodel params: 265730\nTraining on device: cuda\n\n\n\n\n\nReached the max training steps: 10000\nloss: 0.44915974140167236\n\n\n100%|██████████| 1000/1000 [00:01&lt;00:00, 993.84it/s, std=56.1]\n\n\nChamfer Distance: 40.11150077540098\nmodel1, Layers: [256, 256, 256, 256, 256], Train Steps: 1000\nmodel params: 264706\nTraining on device: cuda\n\n\n\n\n\nReached the max training steps: 1000\nloss: 0.42826491594314575\n\n\n100%|██████████| 1000/1000 [00:01&lt;00:00, 938.80it/s, std=64.7]\n\n\nChamfer Distance: 45.433996849178996\nmodel1, Layers: [256, 256, 256, 256, 256], Train Steps: 5000\nmodel params: 264706\nTraining on device: cuda\n\n\n\n\n\nReached the max training steps: 5000\nloss: 0.43620750308036804\n\n\n100%|██████████| 1000/1000 [00:01&lt;00:00, 956.80it/s, std=68.4]\n\n\nChamfer Distance: 47.47955309238842\nmodel1, Layers: [256, 256, 256, 256, 256], Train Steps: 10000\nmodel params: 264706\nTraining on device: cuda\n\n\n\n\n\nReached the max training steps: 10000\nloss: 0.4281949996948242\n\n\n100%|██████████| 1000/1000 [00:01&lt;00:00, 941.21it/s, std=56.2]\n\n\nChamfer Distance: 40.01103618165592\nmodel2, Layers: [128], Train Steps: 1000\nmodel params: 4738\nTraining on device: cuda\n\n\n\n\n\nReached the max training steps: 1000\nloss: 0.3815053701400757\n\n\n100%|██████████| 1000/1000 [00:00&lt;00:00, 1000.22it/s, std=1.9]\n\n\nChamfer Distance: 0.9952134982106191\nmodel2, Layers: [128], Train Steps: 5000\nmodel params: 4738\nTraining on device: cuda\n\n\n\n\n\nReached the max training steps: 5000\nloss: 0.342546284198761\n\n\n100%|██████████| 1000/1000 [00:00&lt;00:00, 1014.42it/s, std=2.02]\n\n\nChamfer Distance: 0.9870064848257294\nmodel2, Layers: [128], Train Steps: 10000\nmodel params: 4738\nTraining on device: cuda\n\n\n\n\n\nReached the max training steps: 10000\nloss: 0.40761643648147583\n\n\n100%|██████████| 1000/1000 [00:00&lt;00:00, 1019.49it/s, std=1.95]\n\n\nChamfer Distance: 0.826520714937363\nmodel2, Layers: [256, 256], Train Steps: 1000\nmodel params: 75266\nTraining on device: cuda\n\n\n\n\n\nReached the max training steps: 1000\nloss: 0.40317225456237793\n\n\n100%|██████████| 1000/1000 [00:01&lt;00:00, 990.18it/s, std=1.98]\n\n\nChamfer Distance: 0.9103175682988993\nmodel2, Layers: [256, 256], Train Steps: 5000\nmodel params: 75266\nTraining on device: cuda\n\n\n\n\n\nReached the max training steps: 5000\nloss: 0.4225660264492035\n\n\n100%|██████████| 1000/1000 [00:01&lt;00:00, 976.46it/s, std=1.98]\n\n\nChamfer Distance: 0.6969788011770026\nmodel2, Layers: [256, 256], Train Steps: 10000\nmodel params: 75266\nTraining on device: cuda\n\n\n\n\n\nReached the max training steps: 10000\nloss: 0.34160494804382324\n\n\n100%|██████████| 1000/1000 [00:01&lt;00:00, 971.04it/s, std=1.95] \n\n\nChamfer Distance: 0.5928464470174432\nmodel2, Layers: [512, 512], Train Steps: 1000\nmodel params: 281602\nTraining on device: cuda\n\n\n\n\n\nReached the max training steps: 1000\nloss: 0.3745132386684418\n\n\n100%|██████████| 1000/1000 [00:00&lt;00:00, 1001.80it/s, std=1.94]\n\n\nChamfer Distance: 0.8723187112974164\nmodel2, Layers: [512, 512], Train Steps: 5000\nmodel params: 281602\nTraining on device: cuda\n\n\n\n\n\nReached the max training steps: 5000\nloss: 0.4065595269203186\n\n\n100%|██████████| 1000/1000 [00:01&lt;00:00, 955.66it/s, std=2]   \n\n\nChamfer Distance: 0.6087286500392258\nmodel2, Layers: [512, 512], Train Steps: 10000\nmodel params: 281602\nTraining on device: cuda\n\n\n\n\n\nReached the max training steps: 10000\nloss: 0.32167112827301025\n\n\n100%|██████████| 1000/1000 [00:00&lt;00:00, 1011.62it/s, std=1.97]\n\n\nChamfer Distance: 0.46802006722301026\nmodel2, Layers: [256, 256, 256, 256, 256], Train Steps: 1000\nmodel params: 272642\nTraining on device: cuda\n\n\n\n\n\nReached the max training steps: 1000\nloss: 0.4069325625896454\n\n\n100%|██████████| 1000/1000 [00:01&lt;00:00, 934.27it/s, std=2]   \n\n\nChamfer Distance: 0.6562601955836662\nmodel2, Layers: [256, 256, 256, 256, 256], Train Steps: 5000\nmodel params: 272642\nTraining on device: cuda\n\n\n\n\n\nReached the max training steps: 5000\nloss: 0.3000778257846832\n\n\n100%|██████████| 1000/1000 [00:01&lt;00:00, 898.61it/s, std=1.94]\n\n\nChamfer Distance: 0.23615802336684183\nmodel2, Layers: [256, 256, 256, 256, 256], Train Steps: 10000\nmodel params: 272642\nTraining on device: cuda\n\n\n\n\n\nReached the max training steps: 10000\nloss: 0.3427009582519531\n\n\n100%|██████████| 1000/1000 [00:01&lt;00:00, 901.10it/s, std=1.93]\n\n\nChamfer Distance: 0.22002034624801564\nmodel3, Layers: [128], Train Steps: 1000\nmodel params: 50570\nTraining on device: cuda\n\n\n\n\n\nReached the max training steps: 1000\nloss: 0.34604138135910034\n\n\n100%|██████████| 1000/1000 [00:01&lt;00:00, 741.65it/s, std=1.93]\n\n\nChamfer Distance: 0.720878815943192\nmodel3, Layers: [128], Train Steps: 5000\nmodel params: 50570\nTraining on device: cuda\n\n\n\n\n\nReached the max training steps: 5000\nloss: 0.26689964532852173\n\n\n100%|██████████| 1000/1000 [00:01&lt;00:00, 738.06it/s, std=2.03]\n\n\nChamfer Distance: 0.281599986112402\nmodel3, Layers: [128], Train Steps: 10000\nmodel params: 50570\nTraining on device: cuda\n\n\n\n\n\nReached the max training steps: 10000\nloss: 0.3543072044849396\n\n\n100%|██████████| 1000/1000 [00:01&lt;00:00, 751.87it/s, std=1.97]\n\n\nChamfer Distance: 0.3179484760186321\nmodel3, Layers: [256, 256], Train Steps: 1000\nmodel params: 330762\nTraining on device: cuda\n\n\n\n\n\nReached the max training steps: 1000\nloss: 0.32084426283836365\n\n\n100%|██████████| 1000/1000 [00:01&lt;00:00, 619.71it/s, std=1.97]\n\n\nChamfer Distance: 0.3826757721628579\nmodel3, Layers: [256, 256], Train Steps: 5000\nmodel params: 330762\nTraining on device: cuda\n\n\n\n\n\nReached the max training steps: 5000\nloss: 0.33935433626174927\n\n\n100%|██████████| 1000/1000 [00:01&lt;00:00, 632.97it/s, std=1.93]\n\n\nChamfer Distance: 0.2178326634163048\nmodel3, Layers: [256, 256], Train Steps: 10000\nmodel params: 330762\nTraining on device: cuda\n\n\n\n\n\nReached the max training steps: 10000\nloss: 0.29220932722091675\n\n\n100%|██████████| 1000/1000 [00:01&lt;00:00, 638.39it/s, std=1.96]\n\n\nChamfer Distance: 0.20583606746893657\nmodel3, Layers: [512, 512], Train Steps: 1000\nmodel params: 1054218\nTraining on device: cuda\n\n\n\n\n\nReached the max training steps: 1000\nloss: 0.29506391286849976\n\n\n100%|██████████| 1000/1000 [00:01&lt;00:00, 641.70it/s, std=2.01]\n\n\nChamfer Distance: 0.35953832038652656\nmodel3, Layers: [512, 512], Train Steps: 5000\nmodel params: 1054218\nTraining on device: cuda\n\n\n\n\n\nReached the max training steps: 5000\nloss: 0.32820066809654236\n\n\n100%|██████████| 1000/1000 [00:01&lt;00:00, 631.76it/s, std=2.03]\n\n\nChamfer Distance: 0.2234631816484437\nmodel3, Layers: [512, 512], Train Steps: 10000\nmodel params: 1054218\nTraining on device: cuda\n\n\n\n\n\nReached the max training steps: 10000\nloss: 0.3356664776802063\n\n\n100%|██████████| 1000/1000 [00:01&lt;00:00, 634.93it/s, std=1.92]\n\n\nChamfer Distance: 0.2181650604105344\nmodel3, Layers: [256, 256, 256, 256, 256], Train Steps: 1000\nmodel params: 922890\nTraining on device: cuda\n\n\n\n\n\nReached the max training steps: 1000\nloss: 0.3741267919540405\n\n\n100%|██████████| 1000/1000 [00:02&lt;00:00, 432.72it/s, std=1.97]\n\n\nChamfer Distance: 0.25825727442566093\nmodel3, Layers: [256, 256, 256, 256, 256], Train Steps: 5000\nmodel params: 922890\nTraining on device: cuda\n\n\n\n\n\nReached the max training steps: 5000\nloss: 0.330197274684906\n\n\n100%|██████████| 1000/1000 [00:02&lt;00:00, 419.03it/s, std=2.02]\n\n\nChamfer Distance: 0.21255801422529738\nmodel3, Layers: [256, 256, 256, 256, 256], Train Steps: 10000\nmodel params: 922890\nTraining on device: cuda\n\n\n\n\n\nReached the max training steps: 10000\nloss: 0.2812938988208771\n\n\n100%|██████████| 1000/1000 [00:02&lt;00:00, 435.70it/s, std=1.94]\n\n\nChamfer Distance: 0.17926968569031748",
    "crumbs": [
      "Home",
      "Generating a 2D Point Cloud",
      "Diffusion",
      "Our First Pareto Frontier Plot"
    ]
  },
  {
    "objectID": "1_1_b_Diffusion_2D_hyperparams.html#analyze-the-results",
    "href": "1_1_b_Diffusion_2D_hyperparams.html#analyze-the-results",
    "title": "Our First Pareto Frontier Plot",
    "section": "Analyze the results",
    "text": "Analyze the results\nWe can find the model with highest quality. It is the “model3” architecture with time modulated linear layers, a comparatively narrow and deep model with 5 layers\n\n# Find the best model\nbest_model_idx = np.argmin(chamfer_distances)\n\nbest_model = model_configs[best_model_idx]\nprint(f\"Best model: {best_model}, Chamfer Distance: {chamfer_distances[best_model_idx]}\")\n\nBest model: ModelConfig(model='model3', hidden_features=[256, 256, 256, 256, 256], train_steps=10000), Chamfer Distance: 0.17926968569031748\n\n\n\n# Print a grid of the chamfer distances and the different configs\nimport pandas as pd\n\ndf = pd.DataFrame({\n    \"Model\": [m.model for m in model_configs],\n    \"Layers\": [m.hidden_features for m in model_configs],\n    \"Train Steps\": [m.train_steps for m in model_configs],\n    \"Chamfer Distance\": chamfer_distances,\n})\n\n# add num parameters to df\nnum_parameters = []\nfor m_config in model_configs:\n    if m_config.model == \"model1\":\n        model = Model1(hidden_features=m_config.hidden_features, num_timesteps=m_config.train_steps).to(device)\n    elif m_config.model == \"model2\":\n        model = Model2(hidden_features=m_config.hidden_features, num_timesteps=m_config.train_steps).to(device)\n    elif m_config.model == \"model3\":\n        model = Model3(\n            hidden_features=m_config.hidden_features,\n            num_timesteps=m_config.train_steps,\n        ).to(device)\n    num_parameters.append(sum(p.numel() for p in model.parameters()))\n\ndf[\"Num Parameters\"] = num_parameters\ndf['Quality (Negative Log Chamfer Distance)'] = np.log(1/df['Chamfer Distance'])\n\n\nPlot quality vs. model size\nWe can now plot quality against cost. Can you find the “Pareto frontier”?\nFrom the plot, most of model3 (green) points are on the frontier. There is also one orange (model2) point and one blue (model1) point on the frontier. Is model1 really on the frontier? Would the conclusion change if we vary the model size more widely and collect more data points?\n\n# uses seaborn scatterplot to plot the scaling law\n# only uses 10,000 timesteps for the scaling law\n\nimport seaborn as sns\n\n# We fix the number of timesteps to 10,000.\nselected_df = df[df[\"Train Steps\"] == 10000]\n# model2 or 3\n# df = df[df[\"Model\"] != \"model1\"]\n\n#color based on model\n# adds layers as very small text next to point\nsns.scatterplot(data=selected_df, x=\"Num Parameters\", y=\"Quality (Negative Log Chamfer Distance)\", hue=\"Model\")\nfor i in range(len(selected_df)):\n    plt.text(\n        selected_df[\"Num Parameters\"].iloc[i],\n        selected_df[\"Quality (Negative Log Chamfer Distance)\"].iloc[i],\n        str(selected_df[\"Layers\"].iloc[i]), fontsize=5)\n\nplt.title(\"Quality vs. Model Size\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\nPlot quality vs. training steps\nAnother measure of cost if the number of training steps. We can see more training steps generally helps, with a diminishing return. From the plot, do you think we have trained the model enough? Would you train the model some more time?\n\n\n# We only look at the 5 layer model\nselected_df = df[df[\"Layers\"].astype(str) == '[256, 256, 256, 256, 256]']\nsns.lineplot(data=selected_df, x=\"Train Steps\", y=\"Quality (Negative Log Chamfer Distance)\", hue=\"Model\", style=\"Model\", markers=True, dashes=False)\nplt.title(\"Quality vs. Training Steps\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\nDiscussion Questions\nAnalyzing the training runs can also help us with other design choices such as the shape of the model. For example, should the model be wider or deeper (e.g. [512, 512] vs. [256, 256, 256, 256, 256])?\nCollecting training runs can be costly if we always do a brute-force grid search of hyper-parameters. Are there strategies to priortize on certain sets of hyper-parameters?",
    "crumbs": [
      "Home",
      "Generating a 2D Point Cloud",
      "Diffusion",
      "Our First Pareto Frontier Plot"
    ]
  },
  {
    "objectID": "2_1_diffusion_afhq.html",
    "href": "2_1_diffusion_afhq.html",
    "title": "Training a Diffusion Model for Animal Face Images",
    "section": "",
    "text": "open in colab",
    "crumbs": [
      "Home",
      "Generating Animal Face Images",
      "Training a Diffusion Model for Animal Face Images"
    ]
  },
  {
    "objectID": "2_1_diffusion_afhq.html#introduction",
    "href": "2_1_diffusion_afhq.html#introduction",
    "title": "Training a Diffusion Model for Animal Face Images",
    "section": "Introduction",
    "text": "Introduction\nThis notebook demonstrates how to train a diffusion model on the Animal Faces-HQ (AFHQ) dataset. We’ll implement a simplified version of denoising diffusion probabilistic models (DDPM) to generate high-quality animal face images.\nThe training algorithm and generation (sampling) algorithm are the same as the 2D toy example. The main difference is the model architecture that needs to accepts image as an input, and generates image as the output.\n\nOverview\nHere is an overview of the tutorial:\n\nSet up the training configuration\nLoad and preprocess the AFHQ dataset\nDefine the UNet architecture\nImplement the diffusion process\nTrain the model\nGenerate new images\n\n\n\nPrerequisites\n\nPyTorch and torchvision\ndatasets library (pip install datasets==3.0.2)\nBasic understanding of deep learning and generative models\n\n\n\nRunning this notebook in the background\nThis notebook can take a while to run. If you want to run it in the background, you can use the following command:\ncd notebooks\npapermill 2_1_diffusion_afhq.ipynb 2_1_diffusion_afhq.ipynb  # requires `papermill`\n\n# !pip install datasets==3.0.2\n# !pip install torchvision==0.17.2\n\n\n%load_ext autoreload\n%autoreload 2\n\n\nimport torch\nimport torch.optim as optim\nfrom torchvision import transforms\nfrom torch.utils.data import DataLoader, random_split\nfrom torch.nn import MSELoss\nfrom torchvision.utils import make_grid\nfrom dataclasses import dataclass\nfrom typing import Dict, Tuple\nfrom torch.utils.data import DataLoader",
    "crumbs": [
      "Home",
      "Generating Animal Face Images",
      "Training a Diffusion Model for Animal Face Images"
    ]
  },
  {
    "objectID": "2_1_diffusion_afhq.html#training-configuration",
    "href": "2_1_diffusion_afhq.html#training-configuration",
    "title": "Training a Diffusion Model for Animal Face Images",
    "section": "Training Configuration",
    "text": "Training Configuration\nThe TrainingConfig class below defines all hyperparameters for our model: - Dataset: AFHQ dataset at 64x64 resolution - Model architecture: UNet with attention - Training parameters: batch size, learning rate, etc. - Data augmentation options\n\n@dataclass\nclass TrainingConfig:\n    dataset: str = \"zzsi/afhq64_16k\"\n    # Model architecture\n    resolution: int = 64 # resolution of the image\n    num_denoising_steps: int = 1000 # number of timesteps\n\n    # Training loop and optimizer\n    total_steps: int = 100000  # total number of training steps\n    batch_size: int = 32 # batch size\n    learning_rate: float = 5e-4 # initial learning rate\n    weight_decay: float = 1e-6 # weight decay\n\n    # Data augmentation\n    random_flip: bool = False # randomly flip images horizontally\n\n\nconfig = TrainingConfig(resolution=64)",
    "crumbs": [
      "Home",
      "Generating Animal Face Images",
      "Training a Diffusion Model for Animal Face Images"
    ]
  },
  {
    "objectID": "2_1_diffusion_afhq.html#load-data",
    "href": "2_1_diffusion_afhq.html#load-data",
    "title": "Training a Diffusion Model for Animal Face Images",
    "section": "Load data",
    "text": "Load data\nWe use the HuggingFace datasets library to load the AFHQ dataset. The data pipeline includes: - Loading images from HuggingFace - Resizing to the target resolution - Applying normalization and optional augmentations\n\nfrom torch.utils.data import Dataset\nfrom datasets import load_dataset\n\n\nclass HuggingFaceDataset(Dataset):\n    def __init__(self, dataset_path: str, transform=None):\n        self.dataset = load_dataset(dataset_path, split=\"train\")\n        self.transform = transform\n        self.image_key = self.find_image_key()\n\n    def find_image_key(self) -&gt; str:\n        # Check if the dataset has the \"image\" key\n        # NOTE: Can exapnd this to other common keys if needed\n        if \"image\" in self.dataset[0].keys():\n            return \"image\"\n        raise KeyError(\"Dataset does not have an 'image' key\")\n\n    def __len__(self):\n        return len(self.dataset)\n\n    def __getitem__(self, idx):\n        image = self.dataset[idx][self.image_key]\n        image = image.convert(\"RGB\")  # Convert to RGB to ensure 3 channels\n        # By default, set label to 0 to conform to current expected batch format\n        label = 0\n        if self.transform:\n            image = self.transform(image)\n        return image, label\n\n\ndef load_data(config: TrainingConfig) -&gt; Tuple[DataLoader, DataLoader]:\n    resolution = config.resolution\n    transforms_list = [\n        transforms.Resize((resolution, resolution)),\n        transforms.ToTensor(),\n        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n    ]\n    if config.random_flip:\n        transforms_list.insert(0, transforms.RandomHorizontalFlip())\n\n    transform = transforms.Compose(transforms_list)\n    full_dataset = HuggingFaceDataset(config.dataset, transform=transform)\n\n    train_size = int(0.9 * len(full_dataset))\n    val_size = len(full_dataset) - train_size\n    train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size], generator=torch.Generator().manual_seed(0))\n    train_dataloader = DataLoader(\n        train_dataset, batch_size=config.batch_size, shuffle=True, num_workers=2\n    )\n    val_dataloader = DataLoader(\n        val_dataset, batch_size=config.batch_size, shuffle=False, num_workers=2\n    )\n\n    return train_dataloader, val_dataloader\n\n\n\ntrain_dataloader, val_dataloader = load_data(config)\n\n\nx = next(iter(train_dataloader))\n\nfrom matplotlib import pyplot as plt\ngrid_img = make_grid(x[0]).permute(1, 2, 0)\ngrid_img = (grid_img - grid_img.min()) / (grid_img.max() - grid_img.min())\nplt.imshow(grid_img)",
    "crumbs": [
      "Home",
      "Generating Animal Face Images",
      "Training a Diffusion Model for Animal Face Images"
    ]
  },
  {
    "objectID": "2_1_diffusion_afhq.html#create-model-components-for-diffusion",
    "href": "2_1_diffusion_afhq.html#create-model-components-for-diffusion",
    "title": "Training a Diffusion Model for Animal Face Images",
    "section": "Create model components for diffusion",
    "text": "Create model components for diffusion\nFor image generation, a typical model architecture can be a UNet or a transformer. We use UNet in this notebook.\nBelow are some utility functions and classesfor the model architecture.\n\nLibrary code for model architecture\nThe code that defines the UNet model architecture is a bit long. Since it is a fairly standard architecture, we don’t need to go into the details here. Feel free to skip the code block below.\n\n\"\"\"\nFrom: https://github.com/VSehwag/minimal-diffusion/blob/main/unets.py\n\"\"\"\nfrom abc import abstractmethod\n\nimport math\nimport numpy as np\nimport torch as th\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass GroupNorm32(nn.GroupNorm):\n    def forward(self, x):\n        return super().forward(x.float()).type(x.dtype)\n\n\ndef conv_nd(dims, *args, **kwargs):\n    \"\"\"\n    Create a 1D, 2D, or 3D convolution module.\n    \"\"\"\n    if dims == 1:\n        return nn.Conv1d(*args, **kwargs)\n    elif dims == 2:\n        return nn.Conv2d(*args, **kwargs)\n    elif dims == 3:\n        return nn.Conv3d(*args, **kwargs)\n    raise ValueError(f\"unsupported dimensions: {dims}\")\n\n\ndef linear(*args, **kwargs):\n    \"\"\"\n    Create a linear module.\n    \"\"\"\n    return nn.Linear(*args, **kwargs)\n\n\ndef avg_pool_nd(dims, *args, **kwargs):\n    \"\"\"\n    Create a 1D, 2D, or 3D average pooling module.\n    \"\"\"\n    if dims == 1:\n        return nn.AvgPool1d(*args, **kwargs)\n    elif dims == 2:\n        return nn.AvgPool2d(*args, **kwargs)\n    elif dims == 3:\n        return nn.AvgPool3d(*args, **kwargs)\n    raise ValueError(f\"unsupported dimensions: {dims}\")\n\n\ndef update_ema(target_params, source_params, rate=0.99):\n    \"\"\"\n    Update target parameters to be closer to those of source parameters using\n    an exponential moving average.\n\n    :param target_params: the target parameter sequence.\n    :param source_params: the source parameter sequence.\n    :param rate: the EMA rate (closer to 1 means slower).\n    \"\"\"\n    for targ, src in zip(target_params, source_params):\n        targ.detach().mul_(rate).add_(src, alpha=1 - rate)\n\n\ndef zero_module(module):\n    \"\"\"\n    Zero out the parameters of a module and return it.\n    \"\"\"\n    for p in module.parameters():\n        p.detach().zero_()\n    return module\n\n\ndef normalization(channels):\n    \"\"\"\n    Make a standard normalization layer.\n\n    :param channels: number of input channels.\n    :return: an nn.Module for normalization.\n    \"\"\"\n    return GroupNorm32(32, channels)\n\n\ndef timestep_embedding(timesteps, dim, max_period=10000):\n    \"\"\"\n    Create sinusoidal timestep embeddings.\n\n    :param timesteps: a 1-D Tensor of N indices, one per batch element.\n                      These may be fractional.\n    :param dim: the dimension of the output.\n    :param max_period: controls the minimum frequency of the embeddings.\n    :return: an [N x dim] Tensor of positional embeddings.\n    \"\"\"\n    half = dim // 2\n    freqs = th.exp(\n        -math.log(max_period) * th.arange(start=0, end=half, dtype=th.float32) / half\n    ).to(device=timesteps.device)\n    if timesteps.ndim == 1:\n        args = timesteps[:, None].float() * freqs[None]\n    else:\n        args = timesteps.float() * freqs[None]\n    embedding = th.cat([th.cos(args), th.sin(args)], dim=-1)\n    if dim % 2:\n        embedding = th.cat([embedding, th.zeros_like(embedding[:, :1])], dim=-1)\n    return embedding\n\n\ndef checkpoint(func, inputs, params, flag):\n    \"\"\"\n    Evaluate a function without caching intermediate activations, allowing for\n    reduced memory at the expense of extra compute in the backward pass.\n\n    :param func: the function to evaluate.\n    :param inputs: the argument sequence to pass to `func`.\n    :param params: a sequence of parameters `func` depends on but does not\n                   explicitly take as arguments.\n    :param flag: if False, disable gradient checkpointing.\n    \"\"\"\n    if flag:\n        args = tuple(inputs) + tuple(params)\n        return CheckpointFunction.apply(func, len(inputs), *args)\n    else:\n        return func(*inputs)\n\n\nclass CheckpointFunction(th.autograd.Function):\n    @staticmethod\n    def forward(ctx, run_function, length, *args):\n        ctx.run_function = run_function\n        ctx.input_tensors = list(args[:length])\n        ctx.input_params = list(args[length:])\n        with th.no_grad():\n            output_tensors = ctx.run_function(*ctx.input_tensors)\n        return output_tensors\n\n    @staticmethod\n    def backward(ctx, *output_grads):\n        ctx.input_tensors = [x.detach().requires_grad_(True) for x in ctx.input_tensors]\n        with th.enable_grad():\n            # Fixes a bug where the first op in run_function modifies the\n            # Tensor storage in place, which is not allowed for detach()'d\n            # Tensors.\n            shallow_copies = [x.view_as(x) for x in ctx.input_tensors]\n            output_tensors = ctx.run_function(*shallow_copies)\n        input_grads = th.autograd.grad(\n            output_tensors,\n            ctx.input_tensors + ctx.input_params,\n            output_grads,\n            allow_unused=True,\n        )\n        del ctx.input_tensors\n        del ctx.input_params\n        del output_tensors\n        return (None, None) + input_grads\n\n\nclass AttentionPool2d(nn.Module):\n    \"\"\"\n    Adapted from CLIP: https://github.com/openai/CLIP/blob/main/clip/model.py\n    \"\"\"\n\n    def __init__(\n        self,\n        spacial_dim: int,\n        embed_dim: int,\n        num_heads_channels: int,\n        output_dim: int = None,\n    ):\n        super().__init__()\n        self.positional_embedding = nn.Parameter(\n            th.randn(embed_dim, spacial_dim ** 2 + 1) / embed_dim ** 0.5\n        )\n        self.qkv_proj = conv_nd(1, embed_dim, 3 * embed_dim, 1)\n        self.c_proj = conv_nd(1, embed_dim, output_dim or embed_dim, 1)\n        self.num_heads = embed_dim // num_heads_channels\n        self.attention = QKVAttention(self.num_heads)\n\n    def forward(self, x):\n        b, c, *_spatial = x.shape\n        x = x.reshape(b, c, -1)  # NC(HW)\n        x = th.cat([x.mean(dim=-1, keepdim=True), x], dim=-1)  # NC(HW+1)\n        x = x + self.positional_embedding[None, :, :].to(x.dtype)  # NC(HW+1)\n        x = self.qkv_proj(x)\n        x = self.attention(x)\n        x = self.c_proj(x)\n        return x[:, :, 0]\n\n\nclass TimestepBlock(nn.Module):\n    \"\"\"\n    Any module where forward() takes timestep embeddings as a second argument.\n    \"\"\"\n\n    @abstractmethod\n    def forward(self, x, emb):\n        \"\"\"\n        Apply the module to `x` given `emb` timestep embeddings.\n        \"\"\"\n\n\nclass TimestepEmbedSequential(nn.Sequential, TimestepBlock):\n    \"\"\"\n    A sequential module that passes timestep embeddings to the children that\n    support it as an extra input.\n    \"\"\"\n\n    def forward(self, x, emb):\n        for layer in self:\n            if isinstance(layer, TimestepBlock):\n                x = layer(x, emb)\n            else:\n                x = layer(x)\n        return x\n\n\nclass Upsample(nn.Module):\n    \"\"\"\n    An upsampling layer with an optional convolution.\n    :param channels: channels in the inputs and outputs.\n    :param use_conv: a bool determining if a convolution is applied.\n    :param dims: determines if the signal is 1D, 2D, or 3D. If 3D, then\n                 upsampling occurs in the inner-two dimensions.\n    \"\"\"\n\n    def __init__(self, channels, use_conv, dims=2, out_channels=None):\n        super().__init__()\n        self.channels = channels\n        self.out_channels = out_channels or channels\n        self.use_conv = use_conv\n        self.dims = dims\n        if use_conv:\n            self.conv = conv_nd(dims, self.channels, self.out_channels, 3, padding=1)\n\n    def forward(self, x):\n        assert x.shape[1] == self.channels\n        if self.dims == 3:\n            out = F.interpolate(\n                x, (x.shape[2], x.shape[3] * 2, x.shape[4] * 2), mode=\"nearest\"\n            )\n        else:\n            out = F.interpolate(x, scale_factor=2, mode=\"nearest\")\n        if x.shape[-1] == x.shape[-2] == 3:\n            # upsampling layer transform [3x3] to [6x6]. Manually paddding it to make [7x7]\n            out = F.pad(out, (1, 0, 1, 0))\n        if self.use_conv:\n            out = self.conv(out)\n        return out\n\n\nclass Downsample(nn.Module):\n    \"\"\"\n    A downsampling layer with an optional convolution.\n    :param channels: channels in the inputs and outputs.\n    :param use_conv: a bool determining if a convolution is applied.\n    :param dims: determines if the signal is 1D, 2D, or 3D. If 3D, then\n                 downsampling occurs in the inner-two dimensions.\n    \"\"\"\n\n    def __init__(self, channels, use_conv, dims=2, out_channels=None):\n        super().__init__()\n        self.channels = channels\n        self.out_channels = out_channels or channels\n        self.use_conv = use_conv\n        self.dims = dims\n        stride = 2 if dims != 3 else (1, 2, 2)\n        if use_conv:\n            self.op = conv_nd(\n                dims, self.channels, self.out_channels, 3, stride=stride, padding=1\n            )\n        else:\n            assert self.channels == self.out_channels\n            self.op = avg_pool_nd(dims, kernel_size=stride, stride=stride)\n\n    def forward(self, x):\n        assert x.shape[1] == self.channels\n        return self.op(x)\n\n\nclass ResBlock(TimestepBlock):\n    \"\"\"\n    A residual block that can optionally change the number of channels.\n    :param channels: the number of input channels.\n    :param emb_channels: the number of timestep embedding channels.\n    :param dropout: the rate of dropout.\n    :param out_channels: if specified, the number of out channels.\n    :param use_conv: if True and out_channels is specified, use a spatial\n        convolution instead of a smaller 1x1 convolution to change the\n        channels in the skip connection.\n    :param dims: determines if the signal is 1D, 2D, or 3D.\n    :param use_checkpoint: if True, use gradient checkpointing on this module.\n    :param up: if True, use this block for upsampling.\n    :param down: if True, use this block for downsampling.\n    \"\"\"\n\n    def __init__(\n        self,\n        channels,\n        emb_channels,\n        dropout,\n        out_channels=None,\n        use_conv=False,\n        use_scale_shift_norm=False,\n        dims=2,\n        use_checkpoint=False,\n        up=False,\n        down=False,\n    ):\n        super().__init__()\n        self.channels = channels\n        self.emb_channels = emb_channels\n        self.dropout = dropout\n        self.out_channels = out_channels or channels\n        self.use_conv = use_conv\n        self.use_checkpoint = use_checkpoint\n        self.use_scale_shift_norm = use_scale_shift_norm\n\n        self.in_layers = nn.Sequential(\n            normalization(channels),\n            nn.SiLU(),\n            conv_nd(dims, channels, self.out_channels, 3, padding=1),\n        )\n\n        self.updown = up or down\n\n        if up:\n            self.h_upd = Upsample(channels, False, dims)\n            self.x_upd = Upsample(channels, False, dims)\n        elif down:\n            self.h_upd = Downsample(channels, False, dims)\n            self.x_upd = Downsample(channels, False, dims)\n        else:\n            self.h_upd = self.x_upd = nn.Identity()\n\n        self.emb_layers = nn.Sequential(\n            nn.SiLU(),\n            linear(\n                emb_channels,\n                2 * self.out_channels if use_scale_shift_norm else self.out_channels,\n            ),\n        )\n        self.out_layers = nn.Sequential(\n            normalization(self.out_channels),\n            nn.SiLU(),\n            nn.Dropout(p=dropout),\n            zero_module(\n                conv_nd(dims, self.out_channels, self.out_channels, 3, padding=1)\n            ),\n        )\n\n        if self.out_channels == channels:\n            self.skip_connection = nn.Identity()\n        elif use_conv:\n            self.skip_connection = conv_nd(\n                dims, channels, self.out_channels, 3, padding=1\n            )\n        else:\n            self.skip_connection = conv_nd(dims, channels, self.out_channels, 1)\n\n    def forward(self, x, emb):\n        \"\"\"\n        Apply the block to a Tensor, conditioned on a timestep embedding.\n        :param x: an [N x C x ...] Tensor of features.\n        :param emb: an [N x emb_channels] Tensor of timestep embeddings.\n        :return: an [N x C x ...] Tensor of outputs.\n        \"\"\"\n        return checkpoint(\n            self._forward, (x, emb), self.parameters(), self.use_checkpoint\n        )\n\n    def _forward(self, x, emb):\n        if self.updown:\n            in_rest, in_conv = self.in_layers[:-1], self.in_layers[-1]\n            h = in_rest(x)\n            h = self.h_upd(h)\n            x = self.x_upd(x)\n            h = in_conv(h)\n        else:\n            h = self.in_layers(x)\n        emb_out = self.emb_layers(emb).type(h.dtype)\n        while len(emb_out.shape) &lt; len(h.shape):\n            emb_out = emb_out[..., None]\n        if self.use_scale_shift_norm:\n            out_norm, out_rest = self.out_layers[0], self.out_layers[1:]\n            scale, shift = th.chunk(emb_out, 2, dim=1)\n            h = out_norm(h) * (1 + scale) + shift\n            h = out_rest(h)\n        else:\n            h = h + emb_out\n            h = self.out_layers(h)\n        return self.skip_connection(x) + h\n\n\nclass AttentionBlock(nn.Module):\n    \"\"\"\n    An attention block that allows spatial positions to attend to each other.\n    Originally ported from here, but adapted to the N-d case.\n    https://github.com/hojonathanho/diffusion/blob/1e0dceb3b3495bbe19116a5e1b3596cd0706c543/diffusion_tf/models/unet.py#L66.\n    \"\"\"\n\n    def __init__(\n        self,\n        channels,\n        num_heads=1,\n        num_head_channels=-1,\n        use_checkpoint=False,\n        use_new_attention_order=False,\n    ):\n        super().__init__()\n        self.channels = channels\n        if num_head_channels == -1:\n            self.num_heads = num_heads\n        else:\n            assert (\n                channels % num_head_channels == 0\n            ), f\"q,k,v channels {channels} is not divisible by num_head_channels {num_head_channels}\"\n            self.num_heads = channels // num_head_channels\n        self.use_checkpoint = use_checkpoint\n        self.norm = normalization(channels)\n        self.qkv = conv_nd(1, channels, channels * 3, 1)\n        if use_new_attention_order:\n            # split qkv before split heads\n            self.attention = QKVAttention(self.num_heads)\n        else:\n            # split heads before split qkv\n            self.attention = QKVAttentionLegacy(self.num_heads)\n\n        self.proj_out = zero_module(conv_nd(1, channels, channels, 1))\n\n    def forward(self, x):\n        return checkpoint(self._forward, (x,), self.parameters(), True)\n\n    def _forward(self, x):\n        b, c, *spatial = x.shape\n        x = x.reshape(b, c, -1)\n        qkv = self.qkv(self.norm(x))\n        h = self.attention(qkv)\n        h = self.proj_out(h)\n        return (x + h).reshape(b, c, *spatial)\n\n\ndef count_flops_attn(model, _x, y):\n    \"\"\"\n    A counter for the `thop` package to count the operations in an\n    attention operation.\n    Meant to be used like:\n        macs, params = thop.profile(\n            model,\n            inputs=(inputs, timestamps),\n            custom_ops={QKVAttention: QKVAttention.count_flops},\n        )\n    \"\"\"\n    b, c, *spatial = y[0].shape\n    num_spatial = int(np.prod(spatial))\n    # We perform two matmuls with the same number of ops.\n    # The first computes the weight matrix, the second computes\n    # the combination of the value vectors.\n    matmul_ops = 2 * b * (num_spatial ** 2) * c\n    model.total_ops += th.DoubleTensor([matmul_ops])\n\n\nclass QKVAttentionLegacy(nn.Module):\n    \"\"\"\n    A module which performs QKV attention. Matches legacy QKVAttention + input/ouput heads shaping\n    \"\"\"\n\n    def __init__(self, n_heads):\n        super().__init__()\n        self.n_heads = n_heads\n\n    def forward(self, qkv):\n        \"\"\"\n        Apply QKV attention.\n        :param qkv: an [N x (H * 3 * C) x T] tensor of Qs, Ks, and Vs.\n        :return: an [N x (H * C) x T] tensor after attention.\n        \"\"\"\n        bs, width, length = qkv.shape\n        assert width % (3 * self.n_heads) == 0\n        ch = width // (3 * self.n_heads)\n        q, k, v = qkv.reshape(bs * self.n_heads, ch * 3, length).split(ch, dim=1)\n        scale = 1 / math.sqrt(math.sqrt(ch))\n        weight = th.einsum(\n            \"bct,bcs-&gt;bts\", q * scale, k * scale\n        )  # More stable with f16 than dividing afterwards\n        weight = th.softmax(weight.float(), dim=-1).type(weight.dtype)\n        a = th.einsum(\"bts,bcs-&gt;bct\", weight, v)\n        return a.reshape(bs, -1, length)\n\n    @staticmethod\n    def count_flops(model, _x, y):\n        return count_flops_attn(model, _x, y)\n\n\nclass QKVAttention(nn.Module):\n    \"\"\"\n    A module which performs QKV attention and splits in a different order.\n    \"\"\"\n\n    def __init__(self, n_heads):\n        super().__init__()\n        self.n_heads = n_heads\n\n    def forward(self, qkv):\n        \"\"\"\n        Apply QKV attention.\n        :param qkv: an [N x (3 * H * C) x T] tensor of Qs, Ks, and Vs.\n        :return: an [N x (H * C) x T] tensor after attention.\n        \"\"\"\n        bs, width, length = qkv.shape\n        assert width % (3 * self.n_heads) == 0\n        ch = width // (3 * self.n_heads)\n        q, k, v = qkv.chunk(3, dim=1)\n        scale = 1 / math.sqrt(math.sqrt(ch))\n        weight = th.einsum(\n            \"bct,bcs-&gt;bts\",\n            (q * scale).view(bs * self.n_heads, ch, length),\n            (k * scale).view(bs * self.n_heads, ch, length),\n        )  # More stable with f16 than dividing afterwards\n        weight = th.softmax(weight.float(), dim=-1).type(weight.dtype)\n        a = th.einsum(\"bts,bcs-&gt;bct\", weight, v.reshape(bs * self.n_heads, ch, length))\n        return a.reshape(bs, -1, length)\n\n    @staticmethod\n    def count_flops(model, _x, y):\n        return count_flops_attn(model, _x, y)\n\n\nclass UNetModel(nn.Module):\n    \"\"\"\n    The full UNet model with attention and timestep embedding.\n    :param in_channels: channels in the input Tensor.\n    :param emb_dim: base dimension of timestep embedding.\n    :param model_channels: base channel count for the model.\n    :param out_channels: channels in the output Tensor.\n    :param num_res_blocks: number of residual blocks per downsample.\n    :param attention_resolutions: a collection of downsample rates at which\n        attention will take place. May be a set, list, or tuple.\n        For example, if this contains 4, then at 4x downsampling, attention\n        will be used.\n    :param dropout: the dropout probability.\n    :param channel_mult: channel multiplier for each level of the UNet.\n    :param conv_resample: if True, use learned convolutions for upsampling and\n        downsampling.\n    :param dims: determines if the signal is 1D, 2D, or 3D.\n    :param num_classes: if specified (as an int), then this model will be\n        class-conditional with `num_classes` classes.\n    :param use_checkpoint: use gradient checkpointing to reduce memory usage.\n    :param num_heads: the number of attention heads in each attention layer.\n    :param num_heads_channels: if specified, ignore num_heads and instead use\n                               a fixed channel width per attention head.\n    :param num_heads_upsample: works with num_heads to set a different number\n                               of heads for upsampling. Deprecated.\n    :param use_scale_shift_norm: use a FiLM-like conditioning mechanism.\n    :param resblock_updown: use residual blocks for up/downsampling.\n    :param use_new_attention_order: use a different attention pattern for potentially\n                                    increased efficiency.\n    \"\"\"\n\n    def __init__(\n        self,\n        image_size,\n        in_channels,\n        model_channels,\n        out_channels,\n        num_res_blocks,\n        attention_resolutions,\n        time_emb_factor=4,\n        dropout=0,\n        channel_mult=(1, 2, 4, 8),\n        conv_resample=True,\n        dims=2,\n        num_classes=None,\n        use_checkpoint=False,\n        use_fp16=False,\n        num_heads=1,\n        num_head_channels=-1,\n        num_heads_upsample=-1,\n        use_scale_shift_norm=False,\n        resblock_updown=False,\n        use_new_attention_order=False,\n    ):\n        super().__init__()\n\n        if num_heads_upsample == -1:\n            num_heads_upsample = num_heads\n\n        self.image_size = image_size\n        self.in_channels = in_channels\n        self.model_channels = model_channels\n        self.out_channels = out_channels\n        self.num_res_blocks = num_res_blocks\n        self.attention_resolutions = attention_resolutions\n        self.dropout = dropout\n        self.channel_mult = channel_mult\n        self.conv_resample = conv_resample\n        self.num_classes = num_classes\n        self.use_checkpoint = use_checkpoint\n        self.dtype = th.float16 if use_fp16 else th.float32\n        self.num_heads = num_heads\n        self.num_head_channels = num_head_channels\n        self.num_heads_upsample = num_heads_upsample\n\n        time_embed_dim = model_channels * time_emb_factor\n        self.time_embed = nn.Sequential(\n            linear(model_channels, time_embed_dim),\n            nn.SiLU(),\n            linear(time_embed_dim, time_embed_dim),\n        )\n\n        if self.num_classes is not None:\n            self.label_emb = nn.Embedding(num_classes, time_embed_dim)\n\n        ch = input_ch = int(channel_mult[0] * model_channels)\n        self.input_blocks = nn.ModuleList(\n            [TimestepEmbedSequential(conv_nd(dims, in_channels, ch, 3, padding=1))]\n        )\n        self._feature_size = ch\n        input_block_chans = [ch]\n        ds = 1\n        for level, mult in enumerate(channel_mult):\n            for _ in range(num_res_blocks):\n                layers = [\n                    ResBlock(\n                        ch,\n                        time_embed_dim,\n                        dropout,\n                        out_channels=int(mult * model_channels),\n                        dims=dims,\n                        use_checkpoint=use_checkpoint,\n                        use_scale_shift_norm=use_scale_shift_norm,\n                    )\n                ]\n                ch = int(mult * model_channels)\n                if ds in attention_resolutions:\n                    layers.append(\n                        AttentionBlock(\n                            ch,\n                            use_checkpoint=use_checkpoint,\n                            num_heads=num_heads,\n                            num_head_channels=num_head_channels,\n                            use_new_attention_order=use_new_attention_order,\n                        )\n                    )\n                self.input_blocks.append(TimestepEmbedSequential(*layers))\n                self._feature_size += ch\n                input_block_chans.append(ch)\n            if level != len(channel_mult) - 1:\n                out_ch = ch\n                self.input_blocks.append(\n                    TimestepEmbedSequential(\n                        ResBlock(\n                            ch,\n                            time_embed_dim,\n                            dropout,\n                            out_channels=out_ch,\n                            dims=dims,\n                            use_checkpoint=use_checkpoint,\n                            use_scale_shift_norm=use_scale_shift_norm,\n                            down=True,\n                        )\n                        if resblock_updown\n                        else Downsample(\n                            ch, conv_resample, dims=dims, out_channels=out_ch\n                        )\n                    )\n                )\n                ch = out_ch\n                input_block_chans.append(ch)\n                ds *= 2\n                self._feature_size += ch\n\n        self.middle_block = TimestepEmbedSequential(\n            ResBlock(\n                ch,\n                time_embed_dim,\n                dropout,\n                dims=dims,\n                use_checkpoint=use_checkpoint,\n                use_scale_shift_norm=use_scale_shift_norm,\n            ),\n            AttentionBlock(\n                ch,\n                use_checkpoint=use_checkpoint,\n                num_heads=num_heads,\n                num_head_channels=num_head_channels,\n                use_new_attention_order=use_new_attention_order,\n            ),\n            ResBlock(\n                ch,\n                time_embed_dim,\n                dropout,\n                dims=dims,\n                use_checkpoint=use_checkpoint,\n                use_scale_shift_norm=use_scale_shift_norm,\n            ),\n        )\n        self._feature_size += ch\n\n        self.output_blocks = nn.ModuleList([])\n        for level, mult in list(enumerate(channel_mult))[::-1]:\n            for i in range(num_res_blocks + 1):\n                ich = input_block_chans.pop()\n                layers = [\n                    ResBlock(\n                        ch + ich,\n                        time_embed_dim,\n                        dropout,\n                        out_channels=int(model_channels * mult),\n                        dims=dims,\n                        use_checkpoint=use_checkpoint,\n                        use_scale_shift_norm=use_scale_shift_norm,\n                    )\n                ]\n                ch = int(model_channels * mult)\n                if ds in attention_resolutions:\n                    layers.append(\n                        AttentionBlock(\n                            ch,\n                            use_checkpoint=use_checkpoint,\n                            num_heads=num_heads_upsample,\n                            num_head_channels=num_head_channels,\n                            use_new_attention_order=use_new_attention_order,\n                        )\n                    )\n                if level and i == num_res_blocks:\n                    out_ch = ch\n                    layers.append(\n                        ResBlock(\n                            ch,\n                            time_embed_dim,\n                            dropout,\n                            out_channels=out_ch,\n                            dims=dims,\n                            use_checkpoint=use_checkpoint,\n                            use_scale_shift_norm=use_scale_shift_norm,\n                            up=True,\n                        )\n                        if resblock_updown\n                        else Upsample(ch, conv_resample, dims=dims, out_channels=out_ch)\n                    )\n                    ds //= 2\n                self.output_blocks.append(TimestepEmbedSequential(*layers))\n                self._feature_size += ch\n\n        self.out = nn.Sequential(\n            normalization(ch),\n            nn.SiLU(),\n            zero_module(conv_nd(dims, input_ch, out_channels, 3, padding=1)),\n        )\n\n    def forward(self, t, x, y=None, *args, **kwargs):\n        \"\"\"\n        Apply the model to an input batch.\n        :param t: a 1-D batch of timesteps.\n        :param x: an [N x C x ...] Tensor of inputs.\n        :param y: an [N] Tensor of labels, if class-conditional.\n        :return: an [N x C x ...] Tensor of outputs.\n        \"\"\"\n\n        assert (y is not None) == (\n            self.num_classes is not None\n        ), \"must specify y if and only if the model is class-conditional\"\n\n        hs = []\n        emb = self.time_embed(timestep_embedding(t, self.model_channels))\n        if self.num_classes is not None:\n            assert y.shape == (x.shape[0],)\n            emb = emb + self.label_emb(y)\n        h = x.type(self.dtype)\n        for module in self.input_blocks:\n            h = module(h, emb)\n            hs.append(h)\n        h = self.middle_block(h, emb)\n        for module in self.output_blocks:\n            h = th.cat([h, hs.pop()], dim=1)\n            h = module(h, emb)\n        h = h.type(x.dtype)\n        return self.out(h)\n\n\ndef UNetBig(\n    image_size,\n    in_channels=3,\n    out_channels=3,\n    base_width=192,\n    num_classes=None,\n):\n    if image_size == 128:\n        channel_mult = (1, 1, 2, 3, 4)\n    elif image_size == 64:\n        channel_mult = (1, 2, 3, 4)\n    elif image_size == 32:\n        channel_mult = (1, 2, 2, 2)\n    elif image_size == 28:\n        channel_mult = (1, 2, 2, 2)\n    else:\n        raise ValueError(f\"unsupported image size: {image_size}\")\n\n    attention_ds = []\n    if image_size == 28:\n        attention_resolutions = \"28,14,7\"\n    else:\n        attention_resolutions = \"32,16,8\"\n    for res in attention_resolutions.split(\",\"):\n        attention_ds.append(image_size // int(res))\n\n    return UNetModel(\n        image_size=image_size,\n        in_channels=in_channels,\n        model_channels=base_width,\n        out_channels=out_channels,\n        num_res_blocks=3,\n        attention_resolutions=tuple(attention_ds),\n        dropout=0.1,\n        channel_mult=channel_mult,\n        num_classes=num_classes,\n        use_checkpoint=False,\n        use_fp16=False,\n        num_heads=4,\n        num_head_channels=64,\n        num_heads_upsample=-1,\n        use_scale_shift_norm=True,\n        resblock_updown=True,\n        use_new_attention_order=True,\n    )\n\n\ndef UNet(\n    image_size,\n    in_channels=3,\n    out_channels=3,\n    base_width=64,\n    num_classes=None,\n):\n    if image_size == 128:\n        channel_mult = (1, 1, 2, 3, 4)\n    elif image_size == 64:\n        channel_mult = (1, 2, 3, 4)\n    elif image_size == 32:\n        channel_mult = (1, 2, 2, 2)\n    elif image_size == 28:\n        channel_mult = (1, 2, 2, 2)\n    else:\n        raise ValueError(f\"unsupported image size: {image_size}\")\n\n    attention_ds = []\n    if image_size == 28:\n        attention_resolutions = \"28,14,7\"\n    else:\n        attention_resolutions = \"32,16,8\"\n    for res in attention_resolutions.split(\",\"):\n        attention_ds.append(image_size // int(res))\n\n    return UNetModel(\n        image_size=image_size,\n        in_channels=in_channels,\n        model_channels=base_width,\n        out_channels=out_channels,\n        num_res_blocks=3,\n        attention_resolutions=tuple(attention_ds),\n        dropout=0.1,\n        channel_mult=channel_mult,\n        num_classes=num_classes,\n        use_checkpoint=False,\n        use_fp16=False,\n        num_heads=4,\n        num_head_channels=64,\n        num_heads_upsample=-1,\n        use_scale_shift_norm=True,\n        resblock_updown=True,\n        use_new_attention_order=True,\n    )\n\n\ndef UNetSmall(\n    image_size,\n    in_channels=3,\n    out_channels=3,\n    base_width=32,\n    num_classes=None,\n):\n    if image_size == 128:\n        channel_mult = (1, 1, 2, 3, 4)\n    elif image_size == 64:\n        channel_mult = (1, 2, 3, 4)\n    elif image_size == 32:\n        channel_mult = (1, 2, 2, 2)\n    elif image_size == 28:\n        channel_mult = (1, 2, 2, 2)\n    else:\n        raise ValueError(f\"unsupported image size: {image_size}\")\n\n    attention_ds = []\n    if image_size == 28:\n        attention_resolutions = \"28,14,7\"\n    else:\n        attention_resolutions = \"32,16,8\"\n    for res in attention_resolutions.split(\",\"):\n        attention_ds.append(image_size // int(res))\n\n    return UNetModel(\n        image_size=image_size,\n        in_channels=in_channels,\n        model_channels=base_width,\n        out_channels=out_channels,\n        num_res_blocks=2,\n        attention_resolutions=tuple(attention_ds),\n        time_emb_factor=2,\n        dropout=0.1,\n        channel_mult=channel_mult,\n        num_classes=num_classes,\n        use_checkpoint=False,\n        use_fp16=False,\n        num_heads=4,\n        num_head_channels=32,\n        num_heads_upsample=-1,\n        use_scale_shift_norm=True,\n        resblock_updown=True,\n        use_new_attention_order=True,\n    )\n\n\n\nCreate model (the user logic)\n\ndevice = torch.device(\"cuda\")\ndenoising_model = UNet(\n    image_size=config.resolution,\n).to(device)\n\nprint(f\"model params: {sum(p.numel() for p in denoising_model.parameters()) / 1e6:.2f} M\")\n\nmodel params: 32.85 M\n\n\n\n\nOptimizer\n\noptimizer = optim.AdamW(denoising_model.parameters(), lr=config.learning_rate, weight_decay=config.weight_decay)",
    "crumbs": [
      "Home",
      "Generating Animal Face Images",
      "Training a Diffusion Model for Animal Face Images"
    ]
  },
  {
    "objectID": "2_1_diffusion_afhq.html#train",
    "href": "2_1_diffusion_afhq.html#train",
    "title": "Training a Diffusion Model for Animal Face Images",
    "section": "Train",
    "text": "Train\n\nForward diffusion\nForward diffusion can be considered as “data augmentation” in the training step. It adds noise to the data to challenge the model to be able to tell apart signal from noise. Diffusion is a particular way of adding noise, with a carefully designed noise schedule.\nThe forward diffusion defines a sequence of images:\n\\[x_0, x_1, x_2, ..., x_T\\]\nwhere T is the number of diffusion steps (e.g. 1000).\nSimilar to an algebraic sequence with a common difference, the diffusion sequence has a common noise \\(\\mathbf\\epsilon\\). The recursive formula is:\n\\[\nx_t = \\sqrt{\\alpha_t} x_{t-1} + \\sqrt{\\beta_t} \\mathbf\\epsilon\n\\]\nwhere \\(\\beta_t = 1 - \\alpha_t\\) is the variance of the noise at time \\(t\\). This relationship is primarily a design choice that simplifies the mathematics and makes the derivations more elegant and manageable.\nOne can then derive the explicit formula for \\(x_t\\) expressed in terms of \\(x_0\\) and \\(\\mathbf\\epsilon\\):\n\\[\nx_t = \\sqrt{\\bar\\alpha_t} x_0 + \\sqrt{1 - \\bar\\alpha_t} \\mathbf\\epsilon\n\\]\nwhere \\(\\bar\\alpha_t = \\prod_{i=1}^t \\alpha_i\\) is the cumulative product of the variance of the noise at time \\(t\\).\nThe forward diffusion process is thus defined by this noise schedule: \\(\\beta_1, \\beta_2, ..., \\beta_T\\).\n\ndef forward_diffusion(x_0, t, noise_schedule, noise=None):\n    _ts = t.view(-1, 1, 1, 1)\n    if noise is None:\n        noise = torch.randn_like(x_0)\n    assert _ts.max() &lt; len(noise_schedule[\"alphas_cumprod\"]), f\"t={_ts.max()} is larger than the length of noise_schedule: {len(noise_schedule['alphas_cumprod'])}\"\n    alpha_prod_t = noise_schedule[\"alphas_cumprod\"][_ts]\n    x_t = (alpha_prod_t ** 0.5) * x_0 + ((1 - alpha_prod_t) ** 0.5) * noise\n    return x_t, noise\n\n\nNoise schedule\nThe range of \\(\\beta_t\\) is empirically chosen to be between 1e-4 and 0.02. We can plot the noise schedule curves to visualize the blending ratio between the clean data and the noise, and how it changes over “time” through the diffusion process.\n\nbeta_min, beta_max = 1e-4, 0.02\n\ndef create_noise_schedule(n_T: int, device: torch.device) -&gt; Dict[str, torch.Tensor]:\n    betas = torch.linspace(beta_min, beta_max, n_T).to(device)\n    alphas = 1. - betas\n    alphas_cumprod = torch.cumprod(alphas, axis=0).to(device)\n    alphas_cumprod_prev = torch.cat([torch.ones(1).to(device), alphas_cumprod[:-1].to(device)])\n    sqrt_recip_alphas = torch.sqrt(1.0 / alphas).to(device)\n    sqrt_alphas_cumprod = torch.sqrt(alphas_cumprod).to(device)\n    sqrt_one_minus_alphas_cumprod = torch.sqrt(1. - alphas_cumprod)\n    posterior_variance = betas * (1. - alphas_cumprod_prev) / (1. - alphas_cumprod)\n\n    return {\n        \"betas\": betas,\n        \"alphas\": alphas,\n        \"alphas_cumprod\": alphas_cumprod,\n        \"sqrt_recip_alphas\": sqrt_recip_alphas,\n        \"sqrt_alphas_cumprod\": sqrt_alphas_cumprod,\n        \"sqrt_one_minus_alphas_cumprod\": sqrt_one_minus_alphas_cumprod,\n        \"posterior_variance\": posterior_variance,\n    }\n\n\nnoise_schedule = create_noise_schedule(config.num_denoising_steps, device)\n\n# plot the schedule\nplt.figure(figsize=(7, 3))\n\nplt.subplot(1, 2, 1); plt.plot(range(1000), noise_schedule[\"sqrt_alphas_cumprod\"].cpu().numpy())\nplt.title(r\"$\\sqrt{\\bar\\alpha_t}$: signal level\")\n\nplt.subplot(1, 2, 2); plt.plot(range(1000), noise_schedule[\"sqrt_one_minus_alphas_cumprod\"].cpu().numpy())\n_ = plt.title(r\"$\\sqrt{1-\\bar\\alpha_t}$: noise level\")\n\n\n\n\n\n\n\n\n\n\n\nVisualizing forward diffusion on an image\n\n# Let's see what forward diffusion does.\nx_0, _ = next(iter(val_dataloader))\nx_0 = x_0.to(device)\nx_t_list = []\ncommon_noise = torch.randn_like(x_0)\n# print(f\"x_0 std:\", x_0[0].std().item())\n\nnoise_levels = [0, 10, 50, 100, 500]\nfor t in noise_levels:\n  t = torch.full((x_0.shape[0],), t, device=device)\n  x_t_list.append(forward_diffusion(x_0, t, noise_schedule, noise=common_noise)[0])\n\n# visualize\nplt.figure(figsize=(20, 5))\nfor i, (t, x_t) in enumerate(zip(noise_levels, x_t_list)):\n  # print(x_t[0].min().item(), x_t[0].max().item())\n  plt.subplot(1, 10, i+1)\n  plt.title(f\"t={t}, std={x_t[0].std():.2f}\")\n  img = x_t[0].cpu().numpy().transpose(1, 2, 0)\n  img = (img - img.min()) / (img.max() - img.min())\n  plt.imshow(img)\n  if i &gt;= 10:\n    break\n\n\n\n\n\n\n\n\nThrough forward diffusion, the image quality actually appears to degrade quickly. By the time \\(t=100\\), the cat is almost not recognizable. By \\(t=500\\), the image is almost completely noise to the eye. Much of the diffusion process is spent on the “noise-dominant” regime, where the structured image is lost and what remains is a Gaussian-like noisy pattern.\nThis is helpful for the model to learn to denoise the image.\n\nBy including a long regime where the image is essentially noise, the model learns how to reconstruct a clean image from a broad range of noise levels. This comprehensive exposure helps the model become more robust and generalize better.\nThe final goal at sampling time is to start from pure noise (around \\(x_T\\)) and iteratively denoise until reaching a plausible sample \\(x_0\\). If the model had fewer steps and tried to denoise a still somewhat structured image in fewer transitions, it might be harder to learn a smooth, stable backward trajectory. The long noise regime ensures the model is comfortable dealing with extremely noisy inputs and making small, incremental corrections. ​",
    "crumbs": [
      "Home",
      "Generating Animal Face Images",
      "Training a Diffusion Model for Animal Face Images"
    ]
  },
  {
    "objectID": "2_1_diffusion_afhq.html#training-loop",
    "href": "2_1_diffusion_afhq.html#training-loop",
    "title": "Training a Diffusion Model for Animal Face Images",
    "section": "Training loop",
    "text": "Training loop\nThe core training step can be implemented in under 10 lines of code.\nWe teach the model to predict the “common noise” \\(\\mathbf\\epsilon\\) that produced the diffusion sequence, based on \\(x_t\\) at any time \\(t\\). No matter where it is, the model needs to help the image get back to \\(x_0\\).\n\n%%time\nfrom tqdm.auto import tqdm\nimport itertools\n\ncriterion = MSELoss()\ndenoising_model.train()\n\ndef train(denoising_model, steps=100):\n  print(\"Training on device:\", device)\n  max_train_steps = steps\n\n  train_progress_bar = tqdm(enumerate(itertools.cycle(train_dataloader)))\n\n  num_examples = 0\n  for step, (x_0, _) in train_progress_bar:\n    x_0 = x_0.to(device)  # x_0 is the clean data to teach the model to generate\n    optimizer.zero_grad()\n\n    #######################################\n    # Start of Core training step\n    #######################################\n    noise = torch.randn(x_0.shape).to(device)\n    t = torch.randint(0, config.num_denoising_steps, (x_0.shape[0],), device=device).long()\n    x_t, true_noise = forward_diffusion(x_0, t, noise_schedule, noise=noise)\n\n    predicted_noise = denoising_model(t=t, x=x_t)\n\n    loss = criterion(predicted_noise, true_noise)\n    loss.backward();\n    # torch.nn.utils.clip_grad_norm_(denoising_model.parameters(), 1)  # try commenting it out\n    optimizer.step()\n    #######################################\n    # End of Core training step\n    #######################################\n\n    train_progress_bar.set_postfix({\"loss\": loss.cpu().item()})\n    num_examples += len(x_0)\n\n    if step &gt;= max_train_steps:\n      print(f\"Reached the max training steps:\", max_train_steps)\n      break\n\n  print(f\"Trained on {num_examples} examples.\")\n  return loss\n\n\nloss = train(denoising_model, steps=100)\n\nTraining on device: cuda\n\n\n\n\n\nReached the max training steps: 100\nTrained on 3232 examples.\nCPU times: user 27.7 s, sys: 10 s, total: 37.7 s\nWall time: 37.9 s",
    "crumbs": [
      "Home",
      "Generating Animal Face Images",
      "Training a Diffusion Model for Animal Face Images"
    ]
  },
  {
    "objectID": "2_1_diffusion_afhq.html#generate",
    "href": "2_1_diffusion_afhq.html#generate",
    "title": "Training a Diffusion Model for Animal Face Images",
    "section": "Generate",
    "text": "Generate\n\nThe sampling algorithm: reversing the diffusion process\nThe sampling algorithm is the reverse of the forward diffusion process. It starts from \\(x_T\\), a pure noise image, and iteratively denoise until reaching \\(x_0\\).\nThe formula for one “denoising” step is:\n\\[\nx_{t-1} = \\frac{1}{\\sqrt{\\alpha_t}} (x_t - \\frac{1-\\alpha_t}{\\sqrt{1 - \\bar\\alpha_t}} \\mathbf\\epsilon_{\\theta}(\\mathbf{x_t}, t)) + \\sqrt{\\tilde\\beta_t} \\mathbf z_t \\tag{1}\n\\]\nwhere \\(z_t ~ \\mathcal{N}(0, 1)\\) is Gaussian noise (independent from \\(\\mathbf\\epsilon\\)) that is sampled at each denoising step, and \\(\\tilde\\beta_t = \\frac{1 - \\bar\\alpha_{t-1}}{1 - \\bar\\alpha_t} \\beta_t\\) is the variance of the noise at time \\(t\\).\n\nThe clamping\nIf you compare the denoising_step function below with the implementation in the 2D point cloud tutorial, you will find that the one here to be more complex. It implements the following formula:\n\\[\n\\hat x_{0} = \\frac{1}{\\sqrt{\\bar\\alpha_t}} \\left(x_t - \\sqrt{1-\\bar\\alpha_t} \\cdot \\mathbf\\epsilon_{\\theta}(\\mathbf{x_t}, t)\\right) \\tag{2}\n\\]\nthen\n\\[\nx_{t-1} =  \\frac{\\beta_t\\sqrt{\\bar\\alpha_{t-1}}}{1-\\bar\\alpha_t} \\cdot \\textrm{Clamp}(\\hat x_{0}) + \\frac{\\sqrt{\\alpha_t}(1-\\bar\\alpha_{t-1})}{1-\\bar\\alpha_t} x_t  + \\sqrt{\\beta_t} \\mathbf z_t \\tag{3}\n\\]\nwhere \\(\\textrm{Clamp}(\\hat x_{0})\\) is the clamping function that limits the range of the predicted image to be within \\([-1, 1]\\).\nIf there is no clamping, equation (1) is mathematically equivalent to equations (2) and (3) combined (derivations here). However, clamping is found to be helpful in generating high quality images, especially during the early iterations of the training.\nIn summary, instead of directly predicting the previous sample,\n\\[\nx_{t} \\rightarrow x_{t-1}\n\\]\nwe predict the clean sample\n\\[\nx_{t} \\rightarrow x_{0}\n\\]\nand then apply forward diffusion on the clean sample to get \\(x_{t-1}\\), with an optional clamping:\n\\[\nx_{0} \\rightarrow \\textrm{Clamp}(x_{0}) \\rightarrow x_{t-1}\n\\].\nTo see its effect, the more direct implementation denoising_step_direct() is also included for comparison. We will see why the new implementation with clamping is necessary.\n\ndef denoising_step(denoising_model, x_t, t, noise_schedule, thresholding=False, clip_sample=True, clip_sample_range=1.0):\n    \"\"\"\n    This is the backward diffusion step, with the effect of denoising.\n    \"\"\"\n    if isinstance(t, int):\n        t_tensor = torch.full((x_t.shape[0],), t, device=x_t.device)\n    else:\n        t_tensor = t\n    with torch.no_grad():\n        model_output = denoising_model(t=t_tensor, x=x_t)\n    if hasattr(model_output, \"sample\"):\n        model_output = model_output.sample\n\n    # Extract relevant values from noise_schedule\n    alpha_prod_t = noise_schedule[\"alphas_cumprod\"][t_tensor]\n    # deal with t=0 case where t can be a tensor\n    alpha_prod_t_prev = torch.where(t_tensor &gt; 0,\n                                    noise_schedule[\"alphas_cumprod\"][t_tensor - 1],\n                                    torch.ones_like(t_tensor, device=x_t.device))\n\n    # Reshape alpha_prod_t_prev for proper broadcasting\n    alpha_prod_t = alpha_prod_t.view(-1, 1, 1, 1)\n    alpha_prod_t_prev = alpha_prod_t_prev.view(-1, 1, 1, 1)\n\n    beta_prod_t = 1 - alpha_prod_t\n    beta_prod_t_prev = 1 - alpha_prod_t_prev\n    current_alpha_t = alpha_prod_t / alpha_prod_t_prev\n    current_beta_t = 1 - current_alpha_t\n\n    # Compute the previous sample mean\n    pred_original_sample = (x_t - beta_prod_t ** 0.5 * model_output) / alpha_prod_t ** 0.5\n\n    if clip_sample:\n        pred_original_sample = torch.clamp(pred_original_sample, -clip_sample_range, clip_sample_range)\n\n    # Compute the coefficients for pred_original_sample and current sample\n    pred_original_sample_coeff = (alpha_prod_t_prev ** 0.5 * current_beta_t) / beta_prod_t\n    current_sample_coeff = current_alpha_t ** 0.5 * beta_prod_t_prev / beta_prod_t\n\n    # Compute the previous sample\n    pred_prev_sample = pred_original_sample_coeff * pred_original_sample + current_sample_coeff * x_t\n\n    # Add noise\n    variance = torch.zeros_like(x_t)\n    variance_noise = torch.randn_like(x_t)\n\n    # Handle t=0 case where t can be a tensor\n    non_zero_mask = (t_tensor != 0).float().view(-1, 1, 1, 1)\n    variance = non_zero_mask * ((1 - alpha_prod_t_prev) / (1 - alpha_prod_t) * current_beta_t)\n    variance = torch.clamp(variance, min=1e-20)\n\n    pred_prev_sample = pred_prev_sample + (variance ** 0.5) * variance_noise\n\n    return pred_prev_sample\n\n\ndef denoising_step_direct(\n    denoising_model,\n    x_t,\n    t,\n    noise_schedule,\n    clip_sample=True,\n    clip_sample_range=1.0,\n):\n    \"\"\"\n    This is the backward diffusion step, with the effect of denoising.\n    \"\"\"\n    if isinstance(t, int):\n        t_tensor = torch.full((x_t.shape[0],), t, device=x_t.device)\n    else:\n        t_tensor = t\n\n    with torch.no_grad():\n        eps_theta = denoising_model(t=t_tensor, x=x_t)\n    if hasattr(eps_theta, \"sample\"):\n        eps_theta = eps_theta.sample\n\n    # Extract alphas from noise schedule\n    alpha_t = noise_schedule[\"alphas\"][t_tensor]\n    alpha_t_cumprod = noise_schedule[\"alphas_cumprod\"][t_tensor]\n\n    # Reshape for broadcasting\n    view_shape = (-1,) + (1,) * (x_t.ndim - 1)\n    alpha_t = alpha_t.view(*view_shape)\n    alpha_t_cumprod = alpha_t_cumprod.view(*view_shape)\n\n    # Calculate epsilon factor\n    eps_factor = (1 - alpha_t) / (1 - alpha_t_cumprod).sqrt()\n\n    # Calculate mean for reverse process\n    mean = (1 / torch.sqrt(alpha_t)) * (x_t - eps_factor * eps_theta)\n\n    # Add noise scaled by variance for non-zero timesteps\n    noise = torch.randn_like(x_t)\n    beta_t = 1 - alpha_t\n    variance = beta_t * (1 - alpha_t_cumprod / alpha_t) / (1 - alpha_t_cumprod)\n    variance = torch.clamp(variance, min=1e-20)  # Add clamp to prevent numerical instability\n    \n    # Mask out noise for t=0 timesteps\n    non_zero_mask = (t_tensor &gt; 0).float().view(*view_shape)\n    noise_scale = torch.sqrt(variance) * non_zero_mask\n    \n    pred_prev_sample = mean + noise_scale * noise\n\n    # Apply clipping\n    if clip_sample:\n        pred_prev_sample = torch.clamp(pred_prev_sample, -clip_sample_range, clip_sample_range)\n\n    return pred_prev_sample\n\ndef generate_samples_by_denoising(denoising_model, x_T, noise_schedule, n_T, device, thresholding=False, clip_sample=True, clip_sample_range=1.0, seed=0, method=\"direct\"):\n    \"\"\"\n    This is the generation process.\n    \"\"\"\n    torch.manual_seed(seed)\n\n    x_t = x_T.to(device)\n    pbar = tqdm(range(n_T - 1, -1, -1))\n    for t in pbar:\n        if method == \"direct\":\n            x_t = denoising_step_direct(denoising_model, x_t, t, noise_schedule, clip_sample, clip_sample_range)\n        else:\n            x_t = denoising_step(denoising_model, x_t, t, noise_schedule, thresholding, clip_sample, clip_sample_range)\n        pbar.set_postfix({\"std\": x_t.std().item()})\n\n    # print(\"raw x_t range\", x_t.min(), x_t.max())\n    x_t = (x_t / 2 + 0.5).clamp(0, 1)\n    # print(\"after clamp\", x_t.min(), x_t.max())\n    return x_t\n\n\n\n\nVisualize sampled images\n\nWithout clamping\nFirst we visualize the sampled images without clamping.\n\n# visualize the sampled images\ndef visualize_sampled_images(method=None):\n  print(\"Loss of the denoising model:\", loss.item())\n  x_T = torch.randn(16, 3, config.resolution, config.resolution)\n  x_sampled = generate_samples_by_denoising(\n    denoising_model, x_T,\n    noise_schedule, n_T=1000,\n    device=device,\n    clip_sample=False if method == \"direct\" else True,\n    clip_sample_range=1.0,\n    method=method,\n  )\n  x_sampled = x_sampled * 255\n\n  sampled = make_grid(x_sampled).permute(1, 2, 0).cpu().numpy().astype(np.uint8)\n  _ = plt.imshow(sampled)\n\n\nvisualize_sampled_images(method=\"direct\")\n\nLoss of the denoising model: 0.049650758504867554\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWith clamping\nNow we visualize the sampled images with clamping.\n\n%%time\nvisualize_sampled_images()\n\nLoss of the denoising model: 0.049650758504867554\n\n\n\n\n\nCPU times: user 57 s, sys: 116 ms, total: 57.1 s\nWall time: 57 s\n\n\n\n\n\n\n\n\n\n\n\n\nTrain some more\n\n%%time\n# Train some more\n\nloss = train(denoising_model, steps=1000)\nprint(\"loss:\", loss.item())\n\nTraining on device: cuda\n\n\n\n\n\nReached the max training steps: 1000\nTrained on 31998 examples.\nloss: 0.016148239374160767\nCPU times: user 4min 37s, sys: 1min 43s, total: 6min 20s\nWall time: 6min 31s\n\n\nAgain, let’s compare the sampled images with and without clamping.\n\nGenerate with clamping\n\nvisualize_sampled_images()\n\nLoss of the denoising model: 0.016148239374160767\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGenerate without clamping\n\nvisualize_sampled_images(method=\"direct\")\n\nLoss of the denoising model: 0.016148239374160767\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTrain even more\n\nGenerate with clamping\n\n%%time\nfor g in optimizer.param_groups:\n    g['lr'] = 1e-4  # reduce learning rate\nloss = train(denoising_model, steps=5000)\nprint(\"loss:\", loss.item())\nvisualize_sampled_images()\n\nTraining on device: cuda\n\n\n\n\n\nReached the max training steps: 5000\nTrained on 159828 examples.\nloss: 0.017624855041503906\nLoss of the denoising model: 0.017624855041503906\n\n\n\n\n\nCPU times: user 24min 14s, sys: 8min 50s, total: 33min 5s\nWall time: 33min 5s\n\n\n\n\n\n\n\n\n\n\n\nGenerate without clamping\n\nvisualize_sampled_images(method=\"direct\")\n\nLoss of the denoising model: 0.017624855041503906\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnd, some more\n\n%%time\nfor g in optimizer.param_groups:\n    g['lr'] = 1e-4  # reduce learning rate\nloss = train(denoising_model, steps=12000)\nprint(\"loss:\", loss.item())\n\nTraining on device: cuda\n\n\n\n\n\nReached the max training steps: 12000\nTrained on 383539 examples.\nloss: 0.049816668033599854\nCPU times: user 55min 12s, sys: 21min 2s, total: 1h 16min 15s\nWall time: 1h 16min 15s\n\n\n\nGenerate without clamping\n\nvisualize_sampled_images(method=\"direct\")\n\nLoss of the denoising model: 0.049816668033599854\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGenerate with clamping\n\nvisualize_sampled_images()\n\nLoss of the denoising model: 0.049816668033599854\n\n\n\n\n\n\n\n\n\n\n\n\nWe can see that the images generating without clamping now almost caught up in quality, as the model becomes more robust.\nWith almost 20k steps of training, the model seems to grasp the main features of animal faces. It is a successful proof of concept. However, there are still frequent distortion and artifacts which give a clear “bot smell”.\nThe model will continue to learn from here but will progress more slowly. Here is an 8 hour training run on a Lambda Labs A10 instance.\nTo faciliate open source research and reproducibility, we release the complete training logs in this course on Weights and Biases.\n\n\n\nQuantitative Evaluation of Image Quality\nIn the image quality evaluation tutorial, we will go over several metrics to measure the quality of generated images. A common metric is the “FID” score, which measures the Frechet Inception Distance between the image embeddings of generated images and the real images.",
    "crumbs": [
      "Home",
      "Generating Animal Face Images",
      "Training a Diffusion Model for Animal Face Images"
    ]
  },
  {
    "objectID": "4_1a_generate_t2i_ddpm.html",
    "href": "4_1a_generate_t2i_ddpm.html",
    "title": "Generate images with text conditioning",
    "section": "",
    "text": "Now we will load the model checkpoint, update the sampling logic to handle text embeddings, and experiment with different text prompts, guidance scales, and random seeds.",
    "crumbs": [
      "Home",
      "Text Conditioning",
      "Generate images with text conditioning"
    ]
  },
  {
    "objectID": "4_1a_generate_t2i_ddpm.html#creating-a-captioned-image-dataset",
    "href": "4_1a_generate_t2i_ddpm.html#creating-a-captioned-image-dataset",
    "title": "Generate images with text conditioning",
    "section": "Creating a captioned image dataset",
    "text": "Creating a captioned image dataset\nIn the next tutorial, we will walk through how to create a captioned image dataset using a pre-trained vision-language model (VLM).",
    "crumbs": [
      "Home",
      "Text Conditioning",
      "Generate images with text conditioning"
    ]
  },
  {
    "objectID": "4_2a_generate_t2i_cfm.html",
    "href": "4_2a_generate_t2i_cfm.html",
    "title": "Generate images with text prompts using flow matching",
    "section": "",
    "text": "from torchdyn.models import NeuralODE\nimport sys\nimport torch\nimport torch.nn as nn\nfrom transformers import CLIPTextModel, CLIPTokenizer\n\nsys.path.append(\"..\")\nclass TextEncoder(nn.Module):\n    def __init__(self, model_name: str, device: str):\n        super().__init__()\n        self.model_name = model_name\n        self.model = CLIPTextModel.from_pretrained(model_name).to(device)\n        self.tokenizer = CLIPTokenizer.from_pretrained(model_name)\n        self.device = device\n        # Get the text embedding dimension from the config\n        self.text_embed_dim = self.model.config.hidden_size\n\n    def forward(self, text: str) -&gt; torch.Tensor:\n        tokens = self.tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\").to(self.device)\n        return self.model(**tokens).pooler_output\n\n2025-01-09 04:03:05.972954: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n2025-01-09 04:03:05.985018: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1736395385.999750 3749567 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1736395386.004122 3749567 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2025-01-09 04:03:06.021886: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI, in other operations, rebuild TensorFlow with the appropriate compiler flags.",
    "crumbs": [
      "Home",
      "Text Conditioning",
      "Generate images with text prompts using flow matching"
    ]
  },
  {
    "objectID": "4_2a_generate_t2i_cfm.html#load-pretrained-model",
    "href": "4_2a_generate_t2i_cfm.html#load-pretrained-model",
    "title": "Generate images with text prompts using flow matching",
    "section": "Load pretrained model",
    "text": "Load pretrained model\n\nfrom lib_4_1.model import create_unet_model\nfrom lib_4_1.config import TrainingConfig\n# generate images with text conditioning\n# load the model\nconfig = TrainingConfig(dataset=\"reese-green/afhq64_captions_64k\", caption_column=\"caption_blip2-opt-2.7b\", batch_size=8, resolution=32)\ndenoising_model = create_unet_model(config, config.device)\ndenoising_model.load_state_dict(torch.load(\"denoising_model_4_2.pth\", map_location=\"cuda:0\"))\ndenoising_model.eval()\ntext_encoder = TextEncoder(\"openai/clip-vit-large-patch14\", \"cuda:0\")\ntext_encoder.eval()\n\nmodel params: 14.68 M\n\n\n/tmp/ipykernel_3749567/1191717415.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  denoising_model.load_state_dict(torch.load(\"denoising_model_4_2.pth\", map_location=\"cuda:0\"))\n\n\nTextEncoder(\n  (model): CLIPTextModel(\n    (text_model): CLIPTextTransformer(\n      (embeddings): CLIPTextEmbeddings(\n        (token_embedding): Embedding(49408, 768)\n        (position_embedding): Embedding(77, 768)\n      )\n      (encoder): CLIPEncoder(\n        (layers): ModuleList(\n          (0-11): 12 x CLIPEncoderLayer(\n            (self_attn): CLIPSdpaAttention(\n              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n            )\n            (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (mlp): CLIPMLP(\n              (activation_fn): QuickGELUActivation()\n              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n            )\n            (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          )\n        )\n      )\n      (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n    )\n  )\n)",
    "crumbs": [
      "Home",
      "Text Conditioning",
      "Generate images with text prompts using flow matching"
    ]
  },
  {
    "objectID": "4_2a_generate_t2i_cfm.html#text-prompts",
    "href": "4_2a_generate_t2i_cfm.html#text-prompts",
    "title": "Generate images with text prompts using flow matching",
    "section": "Text prompts",
    "text": "Text prompts\nSimilar to the diffusion text2image example, we will use the same text prompts to generate images.\n\ntexts = [\"null\", \"\", \"a black cat with a lot of fur\", \"a male lion\"]\nwith torch.no_grad():\n    text_embeddings = text_encoder(texts)\nprint(text_embeddings.shape)\n# TODO: for empty text, set the text_embeddings to null_embedding\ntry:\n    text_embeddings[0] = denoising_model.get_null_text_embed()\nexcept Exception as e:\n    print(\"Error: get_null_text_embed is not defined in the denoising_model\")\n    print(e)\n\ntorch.Size([4, 768])\n\n\n\nClassifier-Free Guidance in Generation\nFor flow matching, we use the denoising model to predict the velocity, with and without text conditioning. So we will have two predictions:\n\n\\(v_t(x)\\)\n\\(v_t(x, \\text{text})\\)\n\nThe classifier-free guidance means that we will use a weighted sum of the two predictions as the velocity:\n\\[ v_t = (1 - g) \\cdot v_t(x) + g \\cdot v_t(x, \\text{text}) \\]\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\nclass GeneratorModel(nn.Module):\n    def __init__(self, denoising_model):\n        super().__init__()\n        self.denoising_model = denoising_model\n        self.text_embeddings = None\n        self.guidance_scale = 0\n    \n    def set_cond(self, text_embeddings, guidance_scale):\n        self.text_embeddings = text_embeddings\n        self.guidance_scale = guidance_scale\n\n    def forward(self, t, x, *args, **kwargs):\n        model = self.denoising_model\n        text_embeddings = self.text_embeddings\n        guidance_scale = self.guidance_scale\n        if isinstance(t, int):\n            t = torch.full((x.shape[0],), t, device=x.device)\n        elif isinstance(t, torch.Tensor):\n            # if t is a scalar\n            if len(t.shape) == 0:\n                t = torch.full((x.shape[0],), t, device=x.device)\n            elif t.shape[0] == 1:\n                t = t.repeat(x.shape[0])\n        else:\n            raise ValueError(\"t must be an integer or a tensor with the same number of elements as x\")\n        \n        x_twice = torch.cat([x] * 2)\n        t_twice = torch.cat([t] * 2)\n        if text_embeddings is not None:\n            uncond_embeddings = denoising_model.get_null_text_embed(batch_size=x.shape[0])\n            embeddings_cat = torch.cat([uncond_embeddings, text_embeddings])\n        else:\n            embeddings_cat = None\n            # print(\"No text embeddings for generation\")\n        \n        with torch.no_grad():\n            model_output = model(t=t_twice, x=x_twice, text_embeddings=embeddings_cat, p_uncond=0)\n        \n        # Split predictions and perform guidance\n        v_t_uncond, v_t_cond = model_output.chunk(2)\n        # print(f\"v_t_uncond min: {v_t_uncond.min().item()}, max: {v_t_uncond.max().item()}, mean: {v_t_uncond.mean().item()}, std: {v_t_uncond.std().item()}\")\n        v_t = (1 - guidance_scale) * v_t_uncond + guidance_scale * v_t_cond\n        # print(f\"delta in v: {(v_t - v_t_uncond).mean().item()}\")\n        return v_t\n\n\ndef generate_samples_with_flow_matching(denoising_model, device, text_embeddings, guidance_scale, resolution: int = 32, in_channels: int = 3, parallel: bool = False, seed: int = 0, num_denoising_steps: int = 100):\n    \"\"\"Generate samples.\n\n    Parameters\n    ----------\n    denoising_model:\n        represents the neural network that we want to generate samples from\n    parallel: bool\n        represents the parallel training flag. Torchdyn only runs on 1 GPU, we need to send the models from several GPUs to 1 GPU.\n    \"\"\"\n    model = denoising_model\n    generator_model = GeneratorModel(denoising_model)\n    generator_model.set_cond(text_embeddings, guidance_scale)\n    num_samples = text_embeddings.shape[0]\n    print(f\"guidance scale: {guidance_scale}\")\n    \n    if parallel:\n        import copy\n        model = copy.deepcopy(denoising_model)\n        # Send the models from GPU to CPU for inference with NeuralODE from Torchdyn\n        model = model.to(device)\n\n    with torch.no_grad():\n        torch.manual_seed(seed)\n        \n        # node = NeuralODE(f, solver=\"euler\", sensitivity=\"adjoint\")\n        node = NeuralODE(generator_model, solver=\"euler\", sensitivity=\"adjoint\")\n        with torch.no_grad():\n            # Generate latents\n            traj = node.trajectory(\n                torch.randn(num_samples, in_channels, resolution, resolution, device=device),\n                t_span=torch.linspace(0, 1, num_denoising_steps, device=device),\n            )\n            data = traj[-1, :].view([-1, in_channels, resolution, resolution]) # .clip(-1, 1)\n    \n    return data\n\n\ndef generate_and_show(**kwargs):\n    torch.manual_seed(0)\n    a = generate_samples_with_flow_matching(\n        denoising_model=denoising_model,\n        device=\"cuda:0\",\n        text_embeddings=text_embeddings,\n        resolution=config.resolution,\n        **kwargs,\n    )\n    \n    # display images\n    # convert from cuda to cpu\n    a = a.cpu()\n    # convert from torch to numpy\n    a = a.numpy()\n    # convert from (C, H, W) to (H, W, C)\n    a = np.transpose(a, (0, 2, 3, 1))\n    a = a - a.min()\n    a = a / a.max()\n\n    # make the figure a bit taller\n    fig, axes = plt.subplots(2, 2, figsize=(6, 7))\n    axes = axes.ravel()\n    for i in range(4):\n        axes[i].imshow(a[i])\n        axes[i].set_title(texts[i])\n    plt.tight_layout()\n    # a caption for the whole figure\n    plt.suptitle(f\"Guidance scale: {kwargs.get('guidance_scale', 0)}, Seed: {kwargs.get('seed', 0)}\")\n    plt.show()",
    "crumbs": [
      "Home",
      "Text Conditioning",
      "Generate images with text prompts using flow matching"
    ]
  },
  {
    "objectID": "4_2a_generate_t2i_cfm.html#generate-images-with-no-text-conditioning",
    "href": "4_2a_generate_t2i_cfm.html#generate-images-with-no-text-conditioning",
    "title": "Generate images with text prompts using flow matching",
    "section": "Generate images with no text conditioning",
    "text": "Generate images with no text conditioning\nSetting the guidance scale to 0 means that we are doing unconditional generation. In this case, the text prompt is expected to have no effect on the generation.\n\ngenerate_and_show(\n    guidance_scale=0, seed=10,\n)\n\nguidance scale: 0\n\n\n\n\n\n\n\n\n\nIncreasing the guidance scale to 5 does seem to produce a black cat and a male lion at the bottom row.\nThis time, the empty string prompt produced a cat, rather than a wolf like in the diffusion example.\n\ngenerate_and_show(\n    guidance_scale=5, seed=10,\n)\n\nguidance scale: 5\n\n\n\n\n\n\n\n\n\nIncreasing the guidance scale to 7.5 produces a similar result to the guidance scale of 5.\n\ngenerate_and_show(\n    guidance_scale=7.5, seed=10,\n)\n\nguidance scale: 7.5\n\n\n\n\n\n\n\n\n\nIncrease it to 10. Not much difference.\n\ngenerate_and_show(\n    guidance_scale=10, seed=10,\n)\n\nguidance scale: 10\n\n\n\n\n\n\n\n\n\nHere is a different random seed. We expect to see a different black cat and male lion in the bottom row.\n\ngenerate_and_show(\n    guidance_scale=5, seed=100,\n)\n\nguidance scale: 5\n\n\n\n\n\n\n\n\n\nAnother seed.\n\ngenerate_and_show(\n    guidance_scale=5, seed=200,\n)\n\nguidance scale: 5\n\n\n\n\n\n\n\n\n\nYet another seed. There seems to be a good amount diversity in the generated animal faces, while having good instruction following.\n\ngenerate_and_show(\n    guidance_scale=5, seed=500,\n)\n\nguidance scale: 5\n\n\n\n\n\n\n\n\n\nWe now have a flow matching model that follows our instructions.",
    "crumbs": [
      "Home",
      "Text Conditioning",
      "Generate images with text prompts using flow matching"
    ]
  },
  {
    "objectID": "4_2a_generate_t2i_cfm.html#discussion",
    "href": "4_2a_generate_t2i_cfm.html#discussion",
    "title": "Generate images with text prompts using flow matching",
    "section": "Discussion",
    "text": "Discussion\n\nThe empty string mystery\nRecall that in the diffusion example, the empty string tends to produce a wolf. We had two hypotheses:\n\nThe denoising model is biased and tends to associate the empty string with a wolf.\nThe text encoder is biased and the empty string is close to a wolf in the latent embedding space.\n\nCompare the results of the flow matching and the diffusion: the text encoder is the same, so the explanation is likely the first one.\nThis is an interesting property of diffusion and flow matching models. When prompting it with an empty text or other texts that the model is not familiar with (it is quite different from any training examples), the model tends to arbitrarily associate with a certain class of images.\n\n\nOther Questions to Explore\n\nHow does flow matching compare to diffusion in terms of quality and diversity?\nWhat is the impact of the conditioning methods (e.g. cross attention) on the model quality?",
    "crumbs": [
      "Home",
      "Text Conditioning",
      "Generate images with text prompts using flow matching"
    ]
  },
  {
    "objectID": "5_2_mj_latents.html",
    "href": "5_2_mj_latents.html",
    "title": "Training a Diffusion Model on Latent Representations",
    "section": "",
    "text": "In previous tutorials, we trained diffusion models to generate small, domain-specific images like animal faces. While this was a good starting point, real-world applications often require generating larger, more diverse images.\nThis tutorial explores a more scalable approach by training on latent representations rather than raw pixels. We’ll follow the Stable Diffusion approach and build upon the Transformer Latent Diffusion project by Alexandru Papiu, which efficiently handles high-resolution, diverse image generation by operating in a compressed latent space.\n\n\nTraining large diffusion models requires significant computational resources. Let’s examine some common GPU options available in cloud computing platforms.\nThe NVIDIA A10, A100, and H100 GPUs represent different performance tiers:\n\n\n\nGPU Model\nVRAM Options\nPerformance Comparison\nCost (per hour)\n\n\n\n\nA10\n24GB\nBaseline\n$0.75\n\n\nA100\n40GB, 80GB\n~3x faster than A10\n$1.79\n\n\nH100\n40GB, 80GB\n~2x faster than A100\n$2.99\n\n\n\nThe A10 offers a cost-effective entry point with 24GB VRAM. The A100 and H100 both come in 40GB and 80GB variants, providing substantially more memory and computing power. The A100 performs about 3 times faster than the A10, while the H100 delivers roughly twice the performance of an A100.\nThese prices reflect Lambda Cloud’s on-demand pricing for instances (as of Feb 2025). While the H100 has the highest hourly cost, its superior performance makes it the most efficient choice when considering total compute cost needed for training.\n\n\n\n\n# Load the Latents\n# Run this only once.\n%cd ..\n\nimport sys\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch\nfrom nanodiffusion.datasets import MJLatentsDataset\n\nds = MJLatentsDataset()\n\n/home/ubuntu/.local/lib/python3.11/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n\n\n/home/ubuntu/zz/nano-diffusion\nFile data/raw/mj_latents.npy already exists. Skipping download.\nFile data/raw/mj_text_emb.npy already exists. Skipping download.\n\n\n\n# Print out the first example\nfor k, v in ds[0].items():\n    print(k, v.shape)\n\nimage_emb (4, 32, 32)\ntext_emb (768,)\n\n\nEach training example has one latent image image_emb that has 4 channels and 32x32 resolution, and a text embedding text_emb which is a 768 dimensional vector.\nWhat does the actual image look like? Let’s decode image_emb back to the image space.\n\n# Load VAE\nimport torch\nfrom diffusers import AutoencoderKL\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nvae = AutoencoderKL.from_pretrained(\"madebyollin/sdxl-vae-fp16-fix\", torch_dtype=torch.float32).to(device)\n_ = vae.eval()\n\n\nwith torch.no_grad():\n    latent = torch.asarray(ds[2][\"image_emb\"]).unsqueeze(0).float().to(device)\n    print(\"latent vector:\", latent.shape)\n    img = vae.decode(latent).sample\n    img = img.to(device).cpu().permute(0, 2, 3, 1).numpy()\n    print(\"image:\", img.shape)\n\nlatent vector: torch.Size([1, 4, 32, 32])\nimage: (1, 256, 256, 3)\n\n\n\nimg = (img - img.min()) / (img.max() - img.min()) * 255\nimg = img.astype(np.uint8)\nplt.imshow(img[0])\nplt.show()\n\n\n\n\n\n\n\n\nWhat a cute gathering of cartoon dinos.\nDuring training, we only need the latents. So don’t need to directly use these images.\n\n\n\nWe are going to create a variant of the standard diffusion algorithm based on Variational Diffusion Models (VDM).\n\n\nIn forward diffusion, a simpler formula is used (notice the absence square root):\n\\[\\mathbf{x}_t = (1-\\sigma_t) \\cdot \\mathbf{x}_0 + \\sigma_t \\mathbf{\\epsilon}\n\\]\nwhere \\(\\sigma_t\\) is noise level, and \\(\\mathbf{\\epsilon}\\) is a noise sample from a standard normal distribution.\n\n\n\nIn reverse diffusion, we first predict the clean image \\(\\mathbf{\\hat{x}}_0\\) from the noisy image \\(\\mathbf{x}_t\\), and then apply forward diffusion to get the less noisy image \\(\\mathbf{x}_{t-1}\\) in the previous noise level:\n\\[\\mathbf{x}_{t-1} = (1 - \\frac{\\sigma_{t-1}}{\\sigma_{t}}) \\cdot \\mathbf{\\hat{x}}_0 + \\frac{\\sigma_{t-1}}{\\sigma_{t}} \\cdot \\mathbf{x}_{t}\n\\]\nwhere \\(\\mathbf{\\hat{x}}_0\\) is the predicted clean image from the denoising model \\(f_\\theta(x_t, t)\\). Here the denoising model no longer predicts the noise, but the clean image. This simplifies the parameterization.\n\n\n\nAn improved sampling method, DDPM+, uses a combination of the current and previous predictions to get better estimates:\n\\[\nx_{t-1} = \\frac{(\\sigma_t - \\sigma_{t-1})}{\\sigma_t} D_t + \\frac{\\sigma_{t-1}}{\\sigma_t} x_t\n\\]\nHere \\(D_t\\) is a combination of the current and previous predictions:\n\\[\nD_t =  f_\\theta(x_t, t) + \\frac{1}{2r_t} \\left( f_\\theta(x_t, t) - f_\\theta(x_{t+1}, t+1) \\right)\n\\]\nwhere \\(r_t\\) is the ratio of consecutive step sizes in log-SNR space:\n\\[\nr_t = \\frac{h_t}{h_{t+1}} = \\frac{\\log(\\sigma_t/\\sigma_{t-1})}{\\log(\\sigma_{t+1}/\\sigma_t)}\n\\]\nThis is essentially a form of multistep method that uses information from multiple timesteps to get better estimates, similar to how higher-order numerical methods work.\nWith above improvements, much fewer steps (e.g. 40 instead of 1000) are needed to generate high-quality images.\n\n\n\nFor conditional generation, the classifier-free guidance is applied on the predicted clean image \\(\\mathbf{\\hat{x}}_0\\):\n\\[\n\\mathbf{\\hat{x}}_0 = s \\cdot \\mathbf{\\hat{x}}_0^{\\textrm{cond}} + (1-s) \\cdot \\mathbf{\\hat{x}}_0^{\\textrm{uncond}}\n\\]\nwhere \\(\\mathbf{\\hat{x}}_0^{\\textrm{cond}}\\) is the predicted clean image from the denoising model with the text prompt, and \\(\\mathbf{\\hat{x}}_0^{\\textrm{uncond}}\\) is the predicted clean image from the denoising model without the text prompt, and \\(s\\) is the guidance scale.\n\n\n\n\nTransformer architecture is an increasingly popular architecture for denoising models. Its benefits include:\n\nFlexibility Across Modalities: The inherent design of transformers allows them to be adapted across various data types with minimal modifications. This flexibility is advantageous in denoising applications that may involve different modalities, such as images, text, or audio. U-Net architectures, being convolution-based, are primarily tailored for spatial data and may require significant adjustments to handle other modalities effectively.\nGlobal Context Modeling: Transformers use self-attention to capture long-range dependencies across the entire input, providing better understanding of global image context for denoising. U-Nets, being convolutional, primarily focus on local features and may struggle with distant relationships.\nScalability: Transformers process all elements simultaneously, enabling efficient parallel computation and better scaling for high-resolution images. Although the quadratic complexity of attention layers increases memory usage and compute time. U-Nets may face scaling limitations due to their hierarchical convolutional structure.\n\nWe use a model architecture from the tld project. It is inspired by DiT and Pixart-Alpha. To address the “patchy” outputs common with Transformers processing spatial data, the model incorporates a depth-wise convolution in the FFN layer (borrowed from the Local ViT paper).\nFor encoding, the model:\n\nProcesses 4×32×32 image latent inputs using a patch size of 2, creating 256 flattened 16-dimensional “pixels”\nUses simple conditioning by concatenating a 768-dimensional pooled CLIP text embedding (ViT/L14) with sinusoidal noise embedding\nFeeds this combined conditioning through cross-attention layers in each transformer block\n\n\n\n\nWe will add some readability improvements to the codebase. In notebooks/lib_5, you can find the improved codebase. Here are some notable changes:\n\n\nTo consolidate the different diffusion algorithms, we introduce a BaseDiffusionAlgorithm class that contains the common logic for all diffusion algorithms. We will then extend this class to implement DDPM (notebooks/lib_5/ddpm.py), VDM (notebooks/lib_5/vdm.py). Even flow matching can be implemented as a subclass of BaseDiffusionAlgorithm. This way, you can easily switch between different diffusion algorithms, without changing the model architecture, the VAE, or the training loop.\nThis abstraction exposes 2 methods:\n\nprepare_training_examples: Prepare a training example for the denoising model by adding noise to the input data. This is the forward diffusion process, i.e. the teaching.\nsample: Sample from the denoising model. For diffusion, this is the reverse diffusion process. This is the generation process using a learned denoising model.\n\nclass BaseDiffusionAlgorithm:\n    \"\"\"\n    A common interface for diffusion algorithms.\n    \"\"\"\n    def prepare_training_examples(self, batch: MiniBatch, **kwargs) -&gt; Tuple[Dict[str, torch.Tensor], torch.Tensor]:\n        \"\"\"\n        Prepare a training example for the denoising model by adding noise to the input data.\n\n        This is used in the training step.\n\n        For diffusion, this is the forward diffusion process.\n        \"\"\"\n        ...\n    \n    def sample(self, x_T, y = None, guidance_scale: float = None, seed: int = None, **kwargs):\n        \"\"\"\n        Sample from the denoising model. For diffusion, this is the reverse diffusion process.\n\n        Args:\n            x_T: the initial random noise.\n            y: the conditional input.\n            guidance_scale: the guidance scale for classifier-free guidance.\n            seed: the random seed for reproducibility.\n        \"\"\"\n        ...\nDifferent diffusion algorithms implements variants of noise scheduling, forward diffusion as well as sampling. These are encapsulated as helper classes. For example, in the improved diffusion algorithm in vdm.py, we have VDMForwardDiffusion and VDMTrainingExampleGenerator to handle generating training examples. VDMSampler handles the sampling process.\nWhen a model is trained and you are ready to distribute it for inference, you may only need to distribute the Sampler class to go with your model.\n\n\n\nTo reduce the memory usage, we use fp16 training with the help of accelerator:\naccelerator = Accelerator(\n    mixed_precision=\"fp16\" if config.fp16 else \"no\",\n)\ndenoising_model, train_dataloader, optimizer = accelerator.prepare(denoising_model, train_dataloader, optimizer)\n\n\n\nA utility class MiniBatch is added to translate raw mini batch from the dataloader into a stronger typed object.\n@dataclass\nclass MiniBatch:\n    x: torch.Tensor  # latent or pixel space input\n    text_emb: Optional[torch.Tensor] = None\n    cond_emb_dict: Optional[Dict[str, torch.Tensor]] = None\nWith this definition, we can use .x to access the latent or pixel space input, .text_emb to access the text embedding, and .cond_emb_dict to access the conditional embeddings.\n\n\n\n\nOn the GPU instance, run the following commands to start training:\ncd notebooks/lib_5\nbash build.sh\nbash train.sh --fp16\nTo see the command line arguments, run bash train.sh --help. By default, we train a small variant (3 layers) of the model with the batch size of 128 for about 1000000 steps. We generate 8 sampled images every 1500 steps. You should begin to see generated images every few minutes. For better quality, we will need to use a larger model. You can edit the model.py file to increase the number of layers to 12, which will create a model with roughly 100M parameters, still quite small. We will use this 100M to do some experiments.\n\n\n\nLet’s examine how the model’s image generation capabilities evolve during training. In the beginning, the model generates random image latents which decode to random “blocky” images with no discernible patterns:\n\nAs training progresses to around 5000 steps, the images start showing distinct global patterns and textures, though the content remains unrecognizable:\n\nBy 30000 steps, rough object shapes and forms begin emerging in the generated images:\n\nAt 265000 steps (approximately 14 hours of training on an H100 GPU), the model produces clearly recognizable objects like purple dinosaurs, mermaids, and cute cartoon dogs. However, noticeable distortions remain, particularly in facial features where human perception is especially sensitive:\n\nAfter extensive training of 1,155,000 steps (roughly 3 days on an H100), the image quality continues improving gradually. While some distortions persist and quality varies between samples, the results are quite impressive. The full training log is track in this wandb run.\n\n\n\n\nExperiment\nLink\nSample Images\n\n\n\n\nAnother VDM example\nwandb\n\n\n\nAdding EMA\nwandb\n\n\n\nTrained using FP32\nwandb\n\n\n\nChanged model architecture to DiT-B2\nwandb\n\n\n\nUsing flow matching and DiT-B2\nwandb\n\n\n\nBaseline DDPM training\nwandb\n\n\n\nBaseline DDPM training with DiT-B2\nwandb\n\n\n\nBaseline DDPM training with UNet-Big\nwandb\n\n\n\n\n\nRemarkably, with just one day of training and approximately $50 in compute costs, you can build a model that demonstrates the “vibe”, the capabilities reminiscent of state-of-the-art image generation systems.",
    "crumbs": [
      "Home",
      "Scaling up: generate bigger and more diverse images",
      "Training a Diffusion Model on Latent Representations"
    ]
  },
  {
    "objectID": "5_2_mj_latents.html#hardware-requirements-and-considerations",
    "href": "5_2_mj_latents.html#hardware-requirements-and-considerations",
    "title": "Training a Diffusion Model on Latent Representations",
    "section": "",
    "text": "Training large diffusion models requires significant computational resources. Let’s examine some common GPU options available in cloud computing platforms.\nThe NVIDIA A10, A100, and H100 GPUs represent different performance tiers:\n\n\n\nGPU Model\nVRAM Options\nPerformance Comparison\nCost (per hour)\n\n\n\n\nA10\n24GB\nBaseline\n$0.75\n\n\nA100\n40GB, 80GB\n~3x faster than A10\n$1.79\n\n\nH100\n40GB, 80GB\n~2x faster than A100\n$2.99\n\n\n\nThe A10 offers a cost-effective entry point with 24GB VRAM. The A100 and H100 both come in 40GB and 80GB variants, providing substantially more memory and computing power. The A100 performs about 3 times faster than the A10, while the H100 delivers roughly twice the performance of an A100.\nThese prices reflect Lambda Cloud’s on-demand pricing for instances (as of Feb 2025). While the H100 has the highest hourly cost, its superior performance makes it the most efficient choice when considering total compute cost needed for training.",
    "crumbs": [
      "Home",
      "Scaling up: generate bigger and more diverse images",
      "Training a Diffusion Model on Latent Representations"
    ]
  },
  {
    "objectID": "5_2_mj_latents.html#inpsecting-the-dataset-the-latents-from-midjourney-images",
    "href": "5_2_mj_latents.html#inpsecting-the-dataset-the-latents-from-midjourney-images",
    "title": "Training a Diffusion Model on Latent Representations",
    "section": "",
    "text": "# Load the Latents\n# Run this only once.\n%cd ..\n\nimport sys\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch\nfrom nanodiffusion.datasets import MJLatentsDataset\n\nds = MJLatentsDataset()\n\n/home/ubuntu/.local/lib/python3.11/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n\n\n/home/ubuntu/zz/nano-diffusion\nFile data/raw/mj_latents.npy already exists. Skipping download.\nFile data/raw/mj_text_emb.npy already exists. Skipping download.\n\n\n\n# Print out the first example\nfor k, v in ds[0].items():\n    print(k, v.shape)\n\nimage_emb (4, 32, 32)\ntext_emb (768,)\n\n\nEach training example has one latent image image_emb that has 4 channels and 32x32 resolution, and a text embedding text_emb which is a 768 dimensional vector.\nWhat does the actual image look like? Let’s decode image_emb back to the image space.\n\n# Load VAE\nimport torch\nfrom diffusers import AutoencoderKL\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nvae = AutoencoderKL.from_pretrained(\"madebyollin/sdxl-vae-fp16-fix\", torch_dtype=torch.float32).to(device)\n_ = vae.eval()\n\n\nwith torch.no_grad():\n    latent = torch.asarray(ds[2][\"image_emb\"]).unsqueeze(0).float().to(device)\n    print(\"latent vector:\", latent.shape)\n    img = vae.decode(latent).sample\n    img = img.to(device).cpu().permute(0, 2, 3, 1).numpy()\n    print(\"image:\", img.shape)\n\nlatent vector: torch.Size([1, 4, 32, 32])\nimage: (1, 256, 256, 3)\n\n\n\nimg = (img - img.min()) / (img.max() - img.min()) * 255\nimg = img.astype(np.uint8)\nplt.imshow(img[0])\nplt.show()\n\n\n\n\n\n\n\n\nWhat a cute gathering of cartoon dinos.\nDuring training, we only need the latents. So don’t need to directly use these images.",
    "crumbs": [
      "Home",
      "Scaling up: generate bigger and more diverse images",
      "Training a Diffusion Model on Latent Representations"
    ]
  },
  {
    "objectID": "5_2_mj_latents.html#an-improved-diffusion-algorithm-vdm",
    "href": "5_2_mj_latents.html#an-improved-diffusion-algorithm-vdm",
    "title": "Training a Diffusion Model on Latent Representations",
    "section": "",
    "text": "We are going to create a variant of the standard diffusion algorithm based on Variational Diffusion Models (VDM).\n\n\nIn forward diffusion, a simpler formula is used (notice the absence square root):\n\\[\\mathbf{x}_t = (1-\\sigma_t) \\cdot \\mathbf{x}_0 + \\sigma_t \\mathbf{\\epsilon}\n\\]\nwhere \\(\\sigma_t\\) is noise level, and \\(\\mathbf{\\epsilon}\\) is a noise sample from a standard normal distribution.\n\n\n\nIn reverse diffusion, we first predict the clean image \\(\\mathbf{\\hat{x}}_0\\) from the noisy image \\(\\mathbf{x}_t\\), and then apply forward diffusion to get the less noisy image \\(\\mathbf{x}_{t-1}\\) in the previous noise level:\n\\[\\mathbf{x}_{t-1} = (1 - \\frac{\\sigma_{t-1}}{\\sigma_{t}}) \\cdot \\mathbf{\\hat{x}}_0 + \\frac{\\sigma_{t-1}}{\\sigma_{t}} \\cdot \\mathbf{x}_{t}\n\\]\nwhere \\(\\mathbf{\\hat{x}}_0\\) is the predicted clean image from the denoising model \\(f_\\theta(x_t, t)\\). Here the denoising model no longer predicts the noise, but the clean image. This simplifies the parameterization.\n\n\n\nAn improved sampling method, DDPM+, uses a combination of the current and previous predictions to get better estimates:\n\\[\nx_{t-1} = \\frac{(\\sigma_t - \\sigma_{t-1})}{\\sigma_t} D_t + \\frac{\\sigma_{t-1}}{\\sigma_t} x_t\n\\]\nHere \\(D_t\\) is a combination of the current and previous predictions:\n\\[\nD_t =  f_\\theta(x_t, t) + \\frac{1}{2r_t} \\left( f_\\theta(x_t, t) - f_\\theta(x_{t+1}, t+1) \\right)\n\\]\nwhere \\(r_t\\) is the ratio of consecutive step sizes in log-SNR space:\n\\[\nr_t = \\frac{h_t}{h_{t+1}} = \\frac{\\log(\\sigma_t/\\sigma_{t-1})}{\\log(\\sigma_{t+1}/\\sigma_t)}\n\\]\nThis is essentially a form of multistep method that uses information from multiple timesteps to get better estimates, similar to how higher-order numerical methods work.\nWith above improvements, much fewer steps (e.g. 40 instead of 1000) are needed to generate high-quality images.\n\n\n\nFor conditional generation, the classifier-free guidance is applied on the predicted clean image \\(\\mathbf{\\hat{x}}_0\\):\n\\[\n\\mathbf{\\hat{x}}_0 = s \\cdot \\mathbf{\\hat{x}}_0^{\\textrm{cond}} + (1-s) \\cdot \\mathbf{\\hat{x}}_0^{\\textrm{uncond}}\n\\]\nwhere \\(\\mathbf{\\hat{x}}_0^{\\textrm{cond}}\\) is the predicted clean image from the denoising model with the text prompt, and \\(\\mathbf{\\hat{x}}_0^{\\textrm{uncond}}\\) is the predicted clean image from the denoising model without the text prompt, and \\(s\\) is the guidance scale.",
    "crumbs": [
      "Home",
      "Scaling up: generate bigger and more diverse images",
      "Training a Diffusion Model on Latent Representations"
    ]
  },
  {
    "objectID": "5_2_mj_latents.html#transformer-based-denoising-model",
    "href": "5_2_mj_latents.html#transformer-based-denoising-model",
    "title": "Training a Diffusion Model on Latent Representations",
    "section": "",
    "text": "Transformer architecture is an increasingly popular architecture for denoising models. Its benefits include:\n\nFlexibility Across Modalities: The inherent design of transformers allows them to be adapted across various data types with minimal modifications. This flexibility is advantageous in denoising applications that may involve different modalities, such as images, text, or audio. U-Net architectures, being convolution-based, are primarily tailored for spatial data and may require significant adjustments to handle other modalities effectively.\nGlobal Context Modeling: Transformers use self-attention to capture long-range dependencies across the entire input, providing better understanding of global image context for denoising. U-Nets, being convolutional, primarily focus on local features and may struggle with distant relationships.\nScalability: Transformers process all elements simultaneously, enabling efficient parallel computation and better scaling for high-resolution images. Although the quadratic complexity of attention layers increases memory usage and compute time. U-Nets may face scaling limitations due to their hierarchical convolutional structure.\n\nWe use a model architecture from the tld project. It is inspired by DiT and Pixart-Alpha. To address the “patchy” outputs common with Transformers processing spatial data, the model incorporates a depth-wise convolution in the FFN layer (borrowed from the Local ViT paper).\nFor encoding, the model:\n\nProcesses 4×32×32 image latent inputs using a patch size of 2, creating 256 flattened 16-dimensional “pixels”\nUses simple conditioning by concatenating a 768-dimensional pooled CLIP text embedding (ViT/L14) with sinusoidal noise embedding\nFeeds this combined conditioning through cross-attention layers in each transformer block",
    "crumbs": [
      "Home",
      "Scaling up: generate bigger and more diverse images",
      "Training a Diffusion Model on Latent Representations"
    ]
  },
  {
    "objectID": "5_2_mj_latents.html#codebase-improvements",
    "href": "5_2_mj_latents.html#codebase-improvements",
    "title": "Training a Diffusion Model on Latent Representations",
    "section": "",
    "text": "We will add some readability improvements to the codebase. In notebooks/lib_5, you can find the improved codebase. Here are some notable changes:\n\n\nTo consolidate the different diffusion algorithms, we introduce a BaseDiffusionAlgorithm class that contains the common logic for all diffusion algorithms. We will then extend this class to implement DDPM (notebooks/lib_5/ddpm.py), VDM (notebooks/lib_5/vdm.py). Even flow matching can be implemented as a subclass of BaseDiffusionAlgorithm. This way, you can easily switch between different diffusion algorithms, without changing the model architecture, the VAE, or the training loop.\nThis abstraction exposes 2 methods:\n\nprepare_training_examples: Prepare a training example for the denoising model by adding noise to the input data. This is the forward diffusion process, i.e. the teaching.\nsample: Sample from the denoising model. For diffusion, this is the reverse diffusion process. This is the generation process using a learned denoising model.\n\nclass BaseDiffusionAlgorithm:\n    \"\"\"\n    A common interface for diffusion algorithms.\n    \"\"\"\n    def prepare_training_examples(self, batch: MiniBatch, **kwargs) -&gt; Tuple[Dict[str, torch.Tensor], torch.Tensor]:\n        \"\"\"\n        Prepare a training example for the denoising model by adding noise to the input data.\n\n        This is used in the training step.\n\n        For diffusion, this is the forward diffusion process.\n        \"\"\"\n        ...\n    \n    def sample(self, x_T, y = None, guidance_scale: float = None, seed: int = None, **kwargs):\n        \"\"\"\n        Sample from the denoising model. For diffusion, this is the reverse diffusion process.\n\n        Args:\n            x_T: the initial random noise.\n            y: the conditional input.\n            guidance_scale: the guidance scale for classifier-free guidance.\n            seed: the random seed for reproducibility.\n        \"\"\"\n        ...\nDifferent diffusion algorithms implements variants of noise scheduling, forward diffusion as well as sampling. These are encapsulated as helper classes. For example, in the improved diffusion algorithm in vdm.py, we have VDMForwardDiffusion and VDMTrainingExampleGenerator to handle generating training examples. VDMSampler handles the sampling process.\nWhen a model is trained and you are ready to distribute it for inference, you may only need to distribute the Sampler class to go with your model.\n\n\n\nTo reduce the memory usage, we use fp16 training with the help of accelerator:\naccelerator = Accelerator(\n    mixed_precision=\"fp16\" if config.fp16 else \"no\",\n)\ndenoising_model, train_dataloader, optimizer = accelerator.prepare(denoising_model, train_dataloader, optimizer)\n\n\n\nA utility class MiniBatch is added to translate raw mini batch from the dataloader into a stronger typed object.\n@dataclass\nclass MiniBatch:\n    x: torch.Tensor  # latent or pixel space input\n    text_emb: Optional[torch.Tensor] = None\n    cond_emb_dict: Optional[Dict[str, torch.Tensor]] = None\nWith this definition, we can use .x to access the latent or pixel space input, .text_emb to access the text embedding, and .cond_emb_dict to access the conditional embeddings.",
    "crumbs": [
      "Home",
      "Scaling up: generate bigger and more diverse images",
      "Training a Diffusion Model on Latent Representations"
    ]
  },
  {
    "objectID": "5_2_mj_latents.html#training-in-docker",
    "href": "5_2_mj_latents.html#training-in-docker",
    "title": "Training a Diffusion Model on Latent Representations",
    "section": "",
    "text": "On the GPU instance, run the following commands to start training:\ncd notebooks/lib_5\nbash build.sh\nbash train.sh --fp16\nTo see the command line arguments, run bash train.sh --help. By default, we train a small variant (3 layers) of the model with the batch size of 128 for about 1000000 steps. We generate 8 sampled images every 1500 steps. You should begin to see generated images every few minutes. For better quality, we will need to use a larger model. You can edit the model.py file to increase the number of layers to 12, which will create a model with roughly 100M parameters, still quite small. We will use this 100M to do some experiments.",
    "crumbs": [
      "Home",
      "Scaling up: generate bigger and more diverse images",
      "Training a Diffusion Model on Latent Representations"
    ]
  },
  {
    "objectID": "5_2_mj_latents.html#learning-progress",
    "href": "5_2_mj_latents.html#learning-progress",
    "title": "Training a Diffusion Model on Latent Representations",
    "section": "",
    "text": "Let’s examine how the model’s image generation capabilities evolve during training. In the beginning, the model generates random image latents which decode to random “blocky” images with no discernible patterns:\n\nAs training progresses to around 5000 steps, the images start showing distinct global patterns and textures, though the content remains unrecognizable:\n\nBy 30000 steps, rough object shapes and forms begin emerging in the generated images:\n\nAt 265000 steps (approximately 14 hours of training on an H100 GPU), the model produces clearly recognizable objects like purple dinosaurs, mermaids, and cute cartoon dogs. However, noticeable distortions remain, particularly in facial features where human perception is especially sensitive:\n\nAfter extensive training of 1,155,000 steps (roughly 3 days on an H100), the image quality continues improving gradually. While some distortions persist and quality varies between samples, the results are quite impressive. The full training log is track in this wandb run.\n\n\n\n\nExperiment\nLink\nSample Images\n\n\n\n\nAnother VDM example\nwandb\n\n\n\nAdding EMA\nwandb\n\n\n\nTrained using FP32\nwandb\n\n\n\nChanged model architecture to DiT-B2\nwandb\n\n\n\nUsing flow matching and DiT-B2\nwandb\n\n\n\nBaseline DDPM training\nwandb\n\n\n\nBaseline DDPM training with DiT-B2\nwandb\n\n\n\nBaseline DDPM training with UNet-Big\nwandb\n\n\n\n\n\nRemarkably, with just one day of training and approximately $50 in compute costs, you can build a model that demonstrates the “vibe”, the capabilities reminiscent of state-of-the-art image generation systems.",
    "crumbs": [
      "Home",
      "Scaling up: generate bigger and more diverse images",
      "Training a Diffusion Model on Latent Representations"
    ]
  },
  {
    "objectID": "Image Captioning.html",
    "href": "Image Captioning.html",
    "title": "README",
    "section": "",
    "text": "This notebook can be used to generate captions for an image dataset using the following models: - BLIP2 - PaliGemma-3b - CogVLM"
  },
  {
    "objectID": "Image Captioning.html#generate-captions-for-datset-with-blip2",
    "href": "Image Captioning.html#generate-captions-for-datset-with-blip2",
    "title": "README",
    "section": "Generate captions for Datset with BLIP2",
    "text": "Generate captions for Datset with BLIP2\n\nfrom tqdm import tqdm\n\nblip_captions = {}\nfor i, data in enumerate(tqdm(dataset)):\n    raw_image = dataset[i][0]\n    inputs = processor(raw_image, return_tensors=\"pt\").to(\"cuda\")\n\n    out = model.generate(**inputs)\n    caption = processor.decode(out[0], skip_special_tokens=True).strip()\n    blip_captions[i] = caption\n\n\n# Save the captions\nimport json\n\nwith open(\"blip_captions.json\", \"w\") as f:\n    json.dump(blip_captions, f)"
  },
  {
    "objectID": "compare_vlms.html",
    "href": "compare_vlms.html",
    "title": "Compare VLMs",
    "section": "",
    "text": "The VLM dashboard has a list of VLMs and their performance scores.\nWe want to pick a small-ish (under 8B) model that is sufficient to generate captions for animal face images in the AFHQ dataset."
  },
  {
    "objectID": "compare_vlms.html#install-dependencies",
    "href": "compare_vlms.html#install-dependencies",
    "title": "Compare VLMs",
    "section": "Install Dependencies",
    "text": "Install Dependencies\n\n%load_ext autoreload\n%autoreload 2\n\n\n!pip install peft==0.13.2 flash_attn==2.7.* qwen_vl_utils==0.0.8 transformers==4.47.* autoawq==0.2.* jinja2==3.1.4 xformers==0.0.28 bitsandbytes==0.45.0\n\n\nDefaulting to user installation because normal site-packages is not writeable\nWARNING: Ignoring invalid distribution ~vidia-cusparse-cu12 (/home/ubuntu/.local/lib/python3.11/site-packages)\nWARNING: Ignoring invalid distribution ~vidia-cusparse-cu12 (/home/ubuntu/.local/lib/python3.11/site-packages)\nRequirement already satisfied: peft==0.13.2 in /home/ubuntu/.local/lib/python3.11/site-packages (0.13.2)\nRequirement already satisfied: flash_attn==2.7.* in /home/ubuntu/.local/lib/python3.11/site-packages (2.7.0.post2)\nRequirement already satisfied: qwen_vl_utils==0.0.8 in /home/ubuntu/.local/lib/python3.11/site-packages (0.0.8)\nRequirement already satisfied: transformers==4.47.* in /home/ubuntu/.local/lib/python3.11/site-packages (4.47.0)\nRequirement already satisfied: autoawq==0.2.* in /home/ubuntu/.local/lib/python3.11/site-packages (0.2.7.post2)\nRequirement already satisfied: jinja2==3.1.4 in /home/ubuntu/.local/lib/python3.11/site-packages (3.1.4)\nCollecting xformers==0.0.28\n  Downloading xformers-0.0.28-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (1.0 kB)\nRequirement already satisfied: bitsandbytes==0.45.0 in /home/ubuntu/.local/lib/python3.11/site-packages (0.45.0)\nRequirement already satisfied: numpy&gt;=1.17 in /home/ubuntu/.local/lib/python3.11/site-packages (from peft==0.13.2) (1.26.4)\nRequirement already satisfied: packaging&gt;=20.0 in /opt/miniconda/lib/python3.11/site-packages (from peft==0.13.2) (23.1)\nRequirement already satisfied: psutil in /home/ubuntu/.local/lib/python3.11/site-packages (from peft==0.13.2) (5.9.8)\nRequirement already satisfied: pyyaml in /home/ubuntu/.local/lib/python3.11/site-packages (from peft==0.13.2) (6.0.1)\nRequirement already satisfied: torch&gt;=1.13.0 in /home/ubuntu/.local/lib/python3.11/site-packages (from peft==0.13.2) (2.5.1)\nRequirement already satisfied: tqdm in /home/ubuntu/.local/lib/python3.11/site-packages (from peft==0.13.2) (4.66.5)\nRequirement already satisfied: accelerate&gt;=0.21.0 in /home/ubuntu/.local/lib/python3.11/site-packages (from peft==0.13.2) (0.33.0)\nRequirement already satisfied: safetensors in /home/ubuntu/.local/lib/python3.11/site-packages (from peft==0.13.2) (0.4.2)\nRequirement already satisfied: huggingface-hub&gt;=0.17.0 in /home/ubuntu/.local/lib/python3.11/site-packages (from peft==0.13.2) (0.24.5)\nRequirement already satisfied: einops in /home/ubuntu/.local/lib/python3.11/site-packages (from flash_attn==2.7.*) (0.7.0)\nRequirement already satisfied: av in /home/ubuntu/.local/lib/python3.11/site-packages (from qwen_vl_utils==0.0.8) (14.0.0)\nRequirement already satisfied: pillow in /home/ubuntu/.local/lib/python3.11/site-packages (from qwen_vl_utils==0.0.8) (10.2.0)\nRequirement already satisfied: requests in /home/ubuntu/.local/lib/python3.11/site-packages (from qwen_vl_utils==0.0.8) (2.32.3)\nRequirement already satisfied: filelock in /home/ubuntu/.local/lib/python3.11/site-packages (from transformers==4.47.*) (3.13.3)\nRequirement already satisfied: regex!=2019.12.17 in /home/ubuntu/.local/lib/python3.11/site-packages (from transformers==4.47.*) (2023.12.25)\nRequirement already satisfied: tokenizers&lt;0.22,&gt;=0.21 in /home/ubuntu/.local/lib/python3.11/site-packages (from transformers==4.47.*) (0.21.0)\nRequirement already satisfied: triton in /home/ubuntu/.local/lib/python3.11/site-packages (from autoawq==0.2.*) (3.1.0)\nRequirement already satisfied: typing-extensions&gt;=4.8.0 in /home/ubuntu/.local/lib/python3.11/site-packages (from autoawq==0.2.*) (4.10.0)\nRequirement already satisfied: datasets&gt;=2.20 in /home/ubuntu/.local/lib/python3.11/site-packages (from autoawq==0.2.*) (2.21.0)\nRequirement already satisfied: zstandard in /opt/miniconda/lib/python3.11/site-packages (from autoawq==0.2.*) (0.19.0)\nRequirement already satisfied: MarkupSafe&gt;=2.0 in /home/ubuntu/.local/lib/python3.11/site-packages (from jinja2==3.1.4) (2.1.5)\nCollecting torch&gt;=1.13.0 (from peft==0.13.2)\n  Using cached torch-2.4.1-cp311-cp311-manylinux1_x86_64.whl.metadata (26 kB)\nRequirement already satisfied: sympy in /home/ubuntu/.local/lib/python3.11/site-packages (from torch&gt;=1.13.0-&gt;peft==0.13.2) (1.13.1)\nRequirement already satisfied: networkx in /home/ubuntu/.local/lib/python3.11/site-packages (from torch&gt;=1.13.0-&gt;peft==0.13.2) (3.2.1)\nRequirement already satisfied: fsspec in /home/ubuntu/.local/lib/python3.11/site-packages (from torch&gt;=1.13.0-&gt;peft==0.13.2) (2024.3.1)\nCollecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch&gt;=1.13.0-&gt;peft==0.13.2)\n  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch&gt;=1.13.0-&gt;peft==0.13.2)\n  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch&gt;=1.13.0-&gt;peft==0.13.2)\n  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/ubuntu/.local/lib/python3.11/site-packages (from torch&gt;=1.13.0-&gt;peft==0.13.2) (9.1.0.70)\nCollecting nvidia-cublas-cu12==12.1.3.1 (from torch&gt;=1.13.0-&gt;peft==0.13.2)\n  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.0.2.54 (from torch&gt;=1.13.0-&gt;peft==0.13.2)\n  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.2.106 (from torch&gt;=1.13.0-&gt;peft==0.13.2)\n  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch&gt;=1.13.0-&gt;peft==0.13.2)\n  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch&gt;=1.13.0-&gt;peft==0.13.2)\n  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-nccl-cu12==2.20.5 (from torch&gt;=1.13.0-&gt;peft==0.13.2)\n  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\nCollecting nvidia-nvtx-cu12==12.1.105 (from torch&gt;=1.13.0-&gt;peft==0.13.2)\n  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\nCollecting triton (from autoawq==0.2.*)\n  Using cached triton-3.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\nRequirement already satisfied: nvidia-nvjitlink-cu12 in /home/ubuntu/.local/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107-&gt;torch&gt;=1.13.0-&gt;peft==0.13.2) (12.4.127)\nRequirement already satisfied: pyarrow&gt;=15.0.0 in /home/ubuntu/.local/lib/python3.11/site-packages (from datasets&gt;=2.20-&gt;autoawq==0.2.*) (17.0.0)\nRequirement already satisfied: dill&lt;0.3.9,&gt;=0.3.0 in /home/ubuntu/.local/lib/python3.11/site-packages (from datasets&gt;=2.20-&gt;autoawq==0.2.*) (0.3.8)\nRequirement already satisfied: pandas in /home/ubuntu/.local/lib/python3.11/site-packages (from datasets&gt;=2.20-&gt;autoawq==0.2.*) (2.2.2)\nRequirement already satisfied: xxhash in /home/ubuntu/.local/lib/python3.11/site-packages (from datasets&gt;=2.20-&gt;autoawq==0.2.*) (3.5.0)\nRequirement already satisfied: multiprocess in /home/ubuntu/.local/lib/python3.11/site-packages (from datasets&gt;=2.20-&gt;autoawq==0.2.*) (0.70.16)\nRequirement already satisfied: aiohttp in /home/ubuntu/.local/lib/python3.11/site-packages (from datasets&gt;=2.20-&gt;autoawq==0.2.*) (3.10.5)\nRequirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /opt/miniconda/lib/python3.11/site-packages (from requests-&gt;qwen_vl_utils==0.0.8) (2.0.4)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in /opt/miniconda/lib/python3.11/site-packages (from requests-&gt;qwen_vl_utils==0.0.8) (3.4)\nRequirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in /opt/miniconda/lib/python3.11/site-packages (from requests-&gt;qwen_vl_utils==0.0.8) (1.26.18)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /opt/miniconda/lib/python3.11/site-packages (from requests-&gt;qwen_vl_utils==0.0.8) (2023.7.22)\nRequirement already satisfied: aiohappyeyeballs&gt;=2.3.0 in /home/ubuntu/.local/lib/python3.11/site-packages (from aiohttp-&gt;datasets&gt;=2.20-&gt;autoawq==0.2.*) (2.4.0)\nRequirement already satisfied: aiosignal&gt;=1.1.2 in /home/ubuntu/.local/lib/python3.11/site-packages (from aiohttp-&gt;datasets&gt;=2.20-&gt;autoawq==0.2.*) (1.3.1)\nRequirement already satisfied: attrs&gt;=17.3.0 in /home/ubuntu/.local/lib/python3.11/site-packages (from aiohttp-&gt;datasets&gt;=2.20-&gt;autoawq==0.2.*) (23.2.0)\nRequirement already satisfied: frozenlist&gt;=1.1.1 in /home/ubuntu/.local/lib/python3.11/site-packages (from aiohttp-&gt;datasets&gt;=2.20-&gt;autoawq==0.2.*) (1.4.1)\nRequirement already satisfied: multidict&lt;7.0,&gt;=4.5 in /home/ubuntu/.local/lib/python3.11/site-packages (from aiohttp-&gt;datasets&gt;=2.20-&gt;autoawq==0.2.*) (6.0.5)\nRequirement already satisfied: yarl&lt;2.0,&gt;=1.0 in /home/ubuntu/.local/lib/python3.11/site-packages (from aiohttp-&gt;datasets&gt;=2.20-&gt;autoawq==0.2.*) (1.9.4)\nRequirement already satisfied: python-dateutil&gt;=2.8.2 in /home/ubuntu/.local/lib/python3.11/site-packages (from pandas-&gt;datasets&gt;=2.20-&gt;autoawq==0.2.*) (2.9.0.post0)\nRequirement already satisfied: pytz&gt;=2020.1 in /home/ubuntu/.local/lib/python3.11/site-packages (from pandas-&gt;datasets&gt;=2.20-&gt;autoawq==0.2.*) (2024.1)\nRequirement already satisfied: tzdata&gt;=2022.7 in /home/ubuntu/.local/lib/python3.11/site-packages (from pandas-&gt;datasets&gt;=2.20-&gt;autoawq==0.2.*) (2024.1)\nRequirement already satisfied: mpmath&lt;1.4,&gt;=1.1.0 in /home/ubuntu/.local/lib/python3.11/site-packages (from sympy-&gt;torch&gt;=1.13.0-&gt;peft==0.13.2) (1.3.0)\nRequirement already satisfied: six&gt;=1.5 in /home/ubuntu/.local/lib/python3.11/site-packages (from python-dateutil&gt;=2.8.2-&gt;pandas-&gt;datasets&gt;=2.20-&gt;autoawq==0.2.*) (1.16.0)\nDownloading xformers-0.0.28-cp311-cp311-manylinux_2_28_x86_64.whl (16.7 MB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 16.7/16.7 MB 14.8 MB/s eta 0:00:00a 0:00:01\nUsing cached torch-2.4.1-cp311-cp311-manylinux1_x86_64.whl (797.1 MB)\nUsing cached triton-3.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.4 MB)\nUsing cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\nUsing cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\nUsing cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\nUsing cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\nUsing cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\nUsing cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\nUsing cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\nUsing cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\nUsing cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\nUsing cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\nWARNING: Ignoring invalid distribution ~vidia-cusparse-cu12 (/home/ubuntu/.local/lib/python3.11/site-packages)\nInstalling collected packages: triton, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusolver-cu12, torch, xformers\n  Attempting uninstall: triton\n    WARNING: Ignoring invalid distribution ~vidia-cusparse-cu12 (/home/ubuntu/.local/lib/python3.11/site-packages)\n    Found existing installation: triton 3.1.0\n    Uninstalling triton-3.1.0:\n      Successfully uninstalled triton-3.1.0\n  Attempting uninstall: nvidia-nvtx-cu12\n    WARNING: Ignoring invalid distribution ~vidia-cusparse-cu12 (/home/ubuntu/.local/lib/python3.11/site-packages)\n    Found existing installation: nvidia-nvtx-cu12 12.4.127\n    Uninstalling nvidia-nvtx-cu12-12.4.127:\n      Successfully uninstalled nvidia-nvtx-cu12-12.4.127\n  Attempting uninstall: nvidia-nccl-cu12\n    WARNING: Ignoring invalid distribution ~vidia-cusparse-cu12 (/home/ubuntu/.local/lib/python3.11/site-packages)\n    Found existing installation: nvidia-nccl-cu12 2.21.5\n    Uninstalling nvidia-nccl-cu12-2.21.5:\n      Successfully uninstalled nvidia-nccl-cu12-2.21.5\n  Attempting uninstall: nvidia-cusparse-cu12\n    WARNING: Ignoring invalid distribution ~vidia-cusparse-cu12 (/home/ubuntu/.local/lib/python3.11/site-packages)\n    Found existing installation: nvidia-cusparse-cu12 12.3.1.170\n    Uninstalling nvidia-cusparse-cu12-12.3.1.170:\n      Successfully uninstalled nvidia-cusparse-cu12-12.3.1.170\n  Attempting uninstall: nvidia-curand-cu12\n    WARNING: Ignoring invalid distribution ~vidia-cusparse-cu12 (/home/ubuntu/.local/lib/python3.11/site-packages)\n    Found existing installation: nvidia-curand-cu12 10.3.5.147\n    Uninstalling nvidia-curand-cu12-10.3.5.147:\n      Successfully uninstalled nvidia-curand-cu12-10.3.5.147\n  Attempting uninstall: nvidia-cufft-cu12\n    WARNING: Ignoring invalid distribution ~vidia-cusparse-cu12 (/home/ubuntu/.local/lib/python3.11/site-packages)\n    Found existing installation: nvidia-cufft-cu12 11.2.1.3\n    Uninstalling nvidia-cufft-cu12-11.2.1.3:\n      Successfully uninstalled nvidia-cufft-cu12-11.2.1.3\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    WARNING: Ignoring invalid distribution ~vidia-cusparse-cu12 (/home/ubuntu/.local/lib/python3.11/site-packages)\n    Found existing installation: nvidia-cuda-runtime-cu12 12.4.127\n    Uninstalling nvidia-cuda-runtime-cu12-12.4.127:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.4.127\n  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n    WARNING: Ignoring invalid distribution ~vidia-cusparse-cu12 (/home/ubuntu/.local/lib/python3.11/site-packages)\n    Found existing installation: nvidia-cuda-nvrtc-cu12 12.4.127\n    Uninstalling nvidia-cuda-nvrtc-cu12-12.4.127:\n      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.4.127\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    WARNING: Ignoring invalid distribution ~vidia-cusparse-cu12 (/home/ubuntu/.local/lib/python3.11/site-packages)\n    Found existing installation: nvidia-cuda-cupti-cu12 12.4.127\n    Uninstalling nvidia-cuda-cupti-cu12-12.4.127:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.4.127\n  Attempting uninstall: nvidia-cublas-cu12\n    WARNING: Ignoring invalid distribution ~vidia-cusparse-cu12 (/home/ubuntu/.local/lib/python3.11/site-packages)\n    Found existing installation: nvidia-cublas-cu12 12.4.5.8\n    Uninstalling nvidia-cublas-cu12-12.4.5.8:\n      Successfully uninstalled nvidia-cublas-cu12-12.4.5.8\n  Attempting uninstall: nvidia-cusolver-cu12\n    WARNING: Ignoring invalid distribution ~vidia-cusparse-cu12 (/home/ubuntu/.local/lib/python3.11/site-packages)\n    Found existing installation: nvidia-cusolver-cu12 11.6.1.9\n    Uninstalling nvidia-cusolver-cu12-11.6.1.9:\n      Successfully uninstalled nvidia-cusolver-cu12-11.6.1.9\n  Attempting uninstall: torch\n    WARNING: Ignoring invalid distribution ~vidia-cusparse-cu12 (/home/ubuntu/.local/lib/python3.11/site-packages)\n    Found existing installation: torch 2.5.1\n    Uninstalling torch-2.5.1:\n      Successfully uninstalled torch-2.5.1\n  Attempting uninstall: xformers\n    WARNING: Ignoring invalid distribution ~vidia-cusparse-cu12 (/home/ubuntu/.local/lib/python3.11/site-packages)\n    Found existing installation: xformers 0.0.28.post3\n    Uninstalling xformers-0.0.28.post3:\n      Successfully uninstalled xformers-0.0.28.post3\nWARNING: Ignoring invalid distribution ~vidia-cusparse-cu12 (/home/ubuntu/.local/lib/python3.11/site-packages)\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntorchvision 0.20.1 requires torch==2.5.1, but you have torch 2.4.1 which is incompatible.\nSuccessfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvtx-cu12-12.1.105 torch-2.4.1 triton-3.0.0 xformers-0.0.28\nWARNING: Ignoring invalid distribution ~vidia-cusparse-cu12 (/home/ubuntu/.local/lib/python3.11/site-packages)\nWARNING: Ignoring invalid distribution ~vidia-cusparse-cu12 (/home/ubuntu/.local/lib/python3.11/site-packages)\n\n\n\nimport transformers\nimport jinja2\nimport bitsandbytes\n\nprint(transformers.__version__)\nprint(jinja2.__version__)\nprint(bitsandbytes.__version__)\njinja2.parser\n\n4.47.0\n3.1.4\n0.45.0\n\n\n&lt;module 'jinja2.parser' from '/home/ubuntu/.local/lib/python3.11/site-packages/jinja2/parser.py'&gt;"
  },
  {
    "objectID": "compare_vlms.html#load-images",
    "href": "compare_vlms.html#load-images",
    "title": "Compare VLMs",
    "section": "Load Images",
    "text": "Load Images\n\nfrom torch.utils.data import Dataset\nfrom datasets import load_dataset\nfrom torchvision import transforms\nimport torch\nfrom transformers import AutoModel, AutoTokenizer\n\n\ndataset_name = \"zzsi/afhq64_16k\"\n\n\nclass HuggingFaceDataset(Dataset):\n    def __init__(self, dataset_path: str, transform=None):\n        self.dataset = load_dataset(dataset_path, split=\"train\")\n        self.transform = transform\n        self.image_key = self.find_image_key()\n\n    def find_image_key(self) -&gt; str:\n        # Check if the dataset has the \"image\" key\n        # NOTE: Can exapnd this to other common keys if needed\n        if \"image\" in self.dataset[0].keys():\n            return \"image\"\n        raise KeyError(\"Dataset does not have an 'image' key\")\n\n    def __len__(self):\n        return len(self.dataset)\n\n    def __getitem__(self, idx):\n        image = self.dataset[idx][self.image_key]\n        image = image.convert(\"RGB\")  # Convert to RGB to ensure 3 channels\n        # By default, set label to 0 to conform to current expected batch format\n        label = 0\n        if self.transform:\n            image = self.transform(image)\n        return image, label\n\n\n\ntransforms_list = [\n    transforms.ToTensor(),\n    # transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n]\n    \ntransform = transforms.Compose(transforms_list)\nfull_dataset = HuggingFaceDataset(dataset_name, transform=transform)\n\n\nprint(f\"dataset has {len(full_dataset)} images\")\nprint(f\"first image shape: {full_dataset[0][0].shape}\")\n\ndataset has 14630 images\nfirst image shape: torch.Size([3, 64, 64])\n\n\n\nfrom matplotlib import pyplot as plt\nplt.figure(figsize=(3, 3))\nfirst_image = full_dataset[0][0].permute(1, 2, 0).numpy()\nplt.imshow(first_image)\nplt.show()\n\n\n\n\n\n\n\n\n\n# normalize the image to [0, 1]\nimport numpy as np\nfrom PIL import Image\n\nfirst_image = (first_image - first_image.min()) / max(first_image.max() - first_image.min(), 1e-6)\npil_image = Image.fromarray((first_image * 255).astype(np.uint8))\npil_image.save(\"first_image.jpg\")"
  },
  {
    "objectID": "compare_vlms.html#qwen-vl",
    "href": "compare_vlms.html#qwen-vl",
    "title": "Compare VLMs",
    "section": "QWEN-VL",
    "text": "QWEN-VL\n\nfrom transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\nfrom qwen_vl_utils import process_vision_info\n\n# default: Load the model on the available device(s)\nmodel = Qwen2VLForConditionalGeneration.from_pretrained(\n    \"Qwen/Qwen2-VL-2B-Instruct-AWQ\", torch_dtype=\"auto\", device_map=\"auto\"\n)\n\n# We recommend enabling flash_attention_2 for better acceleration and memory saving, especially in multi-image and video scenarios.\n# model = Qwen2VLForConditionalGeneration.from_pretrained(\n#     \"Qwen/Qwen2-VL-2B-Instruct-AWQ\",\n#     torch_dtype=torch.bfloat16,\n#     attn_implementation=\"flash_attention_2\",\n#     device_map=\"auto\",\n# )\n\n# default processer\nprocessor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct-AWQ\")\n\n# The default range for the number of visual tokens per image in the model is 4-16384. You can set min_pixels and max_pixels according to your needs, such as a token count range of 256-1280, to balance speed and memory usage.\n# min_pixels = 256*28*28\n# max_pixels = 1280*28*28\n# processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct-AWQ\", min_pixels=min_pixels, max_pixels=max_pixels)\n\n\n2024-12-06 14:56:55.329856: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n2024-12-06 14:56:55.342432: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-12-06 14:56:55.357560: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-12-06 14:56:55.362094: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2024-12-06 14:56:55.373408: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2024-12-06 14:56:56.085600: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\nWe suggest you to set `torch_dtype=torch.float16` for better efficiency with AWQ.\n`Qwen2VLRotaryEmbedding` can now be fully parameterized by passing the model config through the `config` argument. All other arguments will be removed in v4.46\n\n\n\n# base64 encoded image using jpeg\nimport base64\nimport io\nfrom PIL import Image\nimport numpy as np\n\n# first write the image to a bytes object\nimage_bytes = io.BytesIO()\n\npil_image.save(image_bytes, format=\"JPEG\")\nimage_bytes = image_bytes.getvalue()\nprefix = \"data:image/jpeg;base64,\"\nimage_base64 = prefix + base64.b64encode(image_bytes).decode('utf-8')\n\n\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\n                \"type\": \"image\",\n                \"image\": image_base64,\n                # \"image\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg\",\n            },\n            {\"type\": \"text\", \"text\": \"Describe this image.\"},\n        ],\n    }\n]\n\n# Preprocess the inputs\n\n\n# Preparation for inference\ntext = processor.apply_chat_template(\n    messages, tokenize=False, add_generation_prompt=True\n)\nimage_inputs, video_inputs = process_vision_info(messages)\n\ninputs = processor(\n    text=[text],\n    images=image_inputs,\n    videos=video_inputs,\n    padding=True,\n    return_tensors=\"pt\",\n)\ninputs = inputs.to(\"cuda\")\n\n# Inference: Generation of the output\ngenerated_ids = model.generate(**inputs, max_new_tokens=128)\ngenerated_ids_trimmed = [\n    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n]\noutput_text = processor.batch_decode(\n    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n)\nprint(output_text)\n\n[\"The image depicts a cat with a long, flowing coat that appears to be a Maine Coon breed. The cat has a fluffy, dense fur that is predominantly white with some darker markings, particularly around the eyes and around the neck. The cat's eyes are large and expressive, and it has a long, curved tail that is slightly curled. The cat's ears are pointed and have a distinct, fluffy texture. The overall appearance of the cat suggests it is well-groomed and healthy.\"]"
  },
  {
    "objectID": "compare_vlms.html#cogvlm",
    "href": "compare_vlms.html#cogvlm",
    "title": "Compare VLMs",
    "section": "CogVLM",
    "text": "CogVLM\n\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nMODEL_PATH = \"THUDM/cogvlm2-llama3-chat-19B-int4\"\nDEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\nTORCH_TYPE = torch.bfloat16 if torch.cuda.is_available() and torch.cuda.get_device_capability()[\n    0] &gt;= 8 else torch.float16\n\ntokenizer = AutoTokenizer.from_pretrained(\n    MODEL_PATH,\n    trust_remote_code=True\n)\nmodel = AutoModelForCausalLM.from_pretrained(\n    MODEL_PATH,\n    torch_dtype=TORCH_TYPE,\n    trust_remote_code=True,\n    low_cpu_mem_usage=True,\n).eval()\n\ntext_only_template = \"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: {} ASSISTANT:\"\n\nquery = \"Describe this image.\"\nhistory = []\n\ninput_by_model = model.build_conversation_input_ids(\n                tokenizer,\n                query=query,\n                history=history,\n                images=[pil_image],\n                template_version='chat'\n            )\n\ninputs = {\n    'input_ids': input_by_model['input_ids'].unsqueeze(0).to(DEVICE),\n    'token_type_ids': input_by_model['token_type_ids'].unsqueeze(0).to(DEVICE),\n    'attention_mask': input_by_model['attention_mask'].unsqueeze(0).to(DEVICE),\n    'images': [[input_by_model['images'][0].to(DEVICE).to(TORCH_TYPE)]] if pil_image is not None else None,\n}\ngen_kwargs = {\n    \"max_new_tokens\": 128,\n    \"pad_token_id\": 128002,\n}\n\nwith torch.no_grad():\n    outputs = model.generate(**inputs, **gen_kwargs)\n    outputs = outputs[:, inputs['input_ids'].shape[1]:]\n    response = tokenizer.decode(outputs[0])\n    response = response.split(\"&lt;|end_of_text|&gt;\")[0]\n    print(\"\\nCogVLM2:\", response)\n\nUnused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in &lt;class 'transformers.utils.quantization_config.BitsAndBytesConfig'&gt;.\n\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[9], line 44\n     38 gen_kwargs = {\n     39     \"max_new_tokens\": 128,\n     40     \"pad_token_id\": 128002,\n     41 }\n     43 with torch.no_grad():\n---&gt; 44     outputs = model.generate(**inputs, **gen_kwargs)\n     45     outputs = outputs[:, inputs['input_ids'].shape[1]:]\n     46     response = tokenizer.decode(outputs[0])\n\nFile ~/.local/lib/python3.11/site-packages/torch/utils/_contextlib.py:116, in context_decorator.&lt;locals&gt;.decorate_context(*args, **kwargs)\n    113 @functools.wraps(func)\n    114 def decorate_context(*args, **kwargs):\n    115     with ctx_factory():\n--&gt; 116         return func(*args, **kwargs)\n\nFile ~/.local/lib/python3.11/site-packages/transformers/generation/utils.py:2252, in GenerationMixin.generate(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\n   2244     input_ids, model_kwargs = self._expand_inputs_for_generation(\n   2245         input_ids=input_ids,\n   2246         expand_size=generation_config.num_return_sequences,\n   2247         is_encoder_decoder=self.config.is_encoder_decoder,\n   2248         **model_kwargs,\n   2249     )\n   2251     # 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\n-&gt; 2252     result = self._sample(\n   2253         input_ids,\n   2254         logits_processor=prepared_logits_processor,\n   2255         stopping_criteria=prepared_stopping_criteria,\n   2256         generation_config=generation_config,\n   2257         synced_gpus=synced_gpus,\n   2258         streamer=streamer,\n   2259         **model_kwargs,\n   2260     )\n   2262 elif generation_mode in (GenerationMode.BEAM_SAMPLE, GenerationMode.BEAM_SEARCH):\n   2263     # 11. prepare beam search scorer\n   2264     beam_scorer = BeamSearchScorer(\n   2265         batch_size=batch_size,\n   2266         num_beams=generation_config.num_beams,\n   (...)\n   2271         max_length=generation_config.max_length,\n   2272     )\n\nFile ~/.local/lib/python3.11/site-packages/transformers/generation/utils.py:3257, in GenerationMixin._sample(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\n   3254     outputs = model_forward(**model_inputs, return_dict=True)\n   3256 # synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\n-&gt; 3257 model_kwargs = self._update_model_kwargs_for_generation(\n   3258     outputs,\n   3259     model_kwargs,\n   3260     is_encoder_decoder=self.config.is_encoder_decoder,\n   3261 )\n   3262 if synced_gpus and this_peer_finished:\n   3263     continue\n\nFile ~/.cache/huggingface/modules/transformers_modules/THUDM/cogvlm2-llama3-chat-19B-int4/119df232ab9fca4a1be87f95c239d7b9a765032e/modeling_cogvlm.py:710, in CogVLMForCausalLM._update_model_kwargs_for_generation(self, outputs, model_kwargs, is_encoder_decoder, standardize_cache_format)\n    702 def _update_model_kwargs_for_generation(\n    703         self,\n    704         outputs: \"ModelOutput\",\n   (...)\n    708 ) -&gt; Dict[str, Any]:\n    709     # update past_key_values\n--&gt; 710     model_kwargs[\"past_key_values\"] = self._extract_past_from_model_output(\n    711         outputs, standardize_cache_format=standardize_cache_format\n    712     )\n    713     if getattr(outputs, \"state\", None) is not None:\n    714         model_kwargs[\"state\"] = outputs.state\n\nTypeError: GenerationMixin._extract_past_from_model_output() got an unexpected keyword argument 'standardize_cache_format'"
  },
  {
    "objectID": "compare_vlms.html#try-blip-2",
    "href": "compare_vlms.html#try-blip-2",
    "title": "Compare VLMs",
    "section": "Try Blip-2",
    "text": "Try Blip-2"
  },
  {
    "objectID": "compare_vlms.html#try-minicpm-v-2",
    "href": "compare_vlms.html#try-minicpm-v-2",
    "title": "Compare VLMs",
    "section": "Try MiniCPM-V-2",
    "text": "Try MiniCPM-V-2\n\nmodel_name = 'openbmb/MiniCPM-V-2'\nmodel = AutoModel.from_pretrained(model_name, trust_remote_code=True,\n    attn_implementation='sdpa', torch_dtype=torch.bfloat16) # sdpa or flash_attention_2, no eager\n# For Nvidia GPUs support BF16 (like A100, H100, RTX3090)\nmodel = model.to(device='cuda', dtype=torch.bfloat16)\n# For Nvidia GPUs do NOT support BF16 (like V100, T4, RTX2080)\n#model = model.to(device='cuda', dtype=torch.float16)\n# For Mac with MPS (Apple silicon or AMD GPUs).\n# Run with `PYTORCH_ENABLE_MPS_FALLBACK=1 python test.py`\n#model = model.to(device='mps', dtype=torch.float16)\n\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n_ = model.eval()\n\n\n\n\n\nimport numpy as np\nfrom PIL import Image\n\nprint(first_image.shape, first_image.max() * 255)\n\npil_image = Image.fromarray((first_image * 255).astype(np.uint8))\n\nquestion = 'What is in the image?'\nmsgs = [{'role': 'user', 'content': question}]\n\nres, context, _ = model.chat(\n    image=pil_image,\n    msgs=msgs,\n    tokenizer=tokenizer,\n    context=None,\n    # sampling=True,\n    # temperature=0.7\n)\nprint(res)\n\n(64, 64, 3) 166.00000530481339\n\n\n\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\nCell In[19], line 11\n      8 question = 'What is in the image?'\n      9 msgs = [{'role': 'user', 'content': question}]\n---&gt; 11 res, context, _ = model.chat(\n     12     image=pil_image,\n     13     msgs=msgs,\n     14     tokenizer=tokenizer,\n     15     context=None,\n     16     # sampling=True,\n     17     # temperature=0.7\n     18 )\n     19 print(res)\n\nFile ~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1729, in Module.__getattr__(self, name)\n   1727     if name in modules:\n   1728         return modules[name]\n-&gt; 1729 raise AttributeError(f\"'{type(self).__name__}' object has no attribute '{name}'\")\n\nAttributeError: 'Qwen2VLForConditionalGeneration' object has no attribute 'chat'"
  },
  {
    "objectID": "test_trained_t2i_model.html",
    "href": "test_trained_t2i_model.html",
    "title": "Nano Diffusion",
    "section": "",
    "text": "import sys\nsys.path.append(\"..\")\n\nfrom src.bookkeeping.wandb_utils import load_model_from_wandb\nfrom src.models.factory import create_model\n\nrun_path = \"zzsi_kungfu/nano-diffusion/cil23lq7\"\nfile_name = \"logs/train/2025-01-15_04-40-09/model_checkpoint_step_90000.pth\"\n\nmodel = create_model(net=\"unet\", resolution=64, cond_embed_dim=768)\nload_model_from_wandb(model, run_path, file_name)\n\n\nCreating model unet with resolution 64 and in_channels 3 and cond_embed_dim 768\nmodel params: 33.11 M\nRestoring model from zzsi_kungfu/nano-diffusion/cil23lq7 and logs/train/2025-01-15_04-40-09/model_checkpoint_step_90000.pth\nModel restored from /home/ubuntu/zz/nano-diffusion/notebooks/logs/train/2025-01-15_04-40-09/model_checkpoint_step_90000.pth\n\n\n\nimport torch\nfrom diffusers import AutoencoderKL\nfrom src.diffusion.diffusion_model_components import DDPM, create_noise_schedule\nfrom src.config.diffusion_training_config import DiffusionTrainingConfig\n\nconfig = DiffusionTrainingConfig(\n    dataset=\"\",\n    num_denoising_steps=1000,\n    device=\"cuda:0\",\n    vae_model_name=\"madebyollin/sdxl-vae-fp16-fix\",\n    diffusion_algo=\"ddpm\",\n    net=\"unet\",\n    resolution=64,\n    cond_embed_dim=768,\n    data_is_latent=False,\n    conditional=True,\n)\n\n\ndevice = torch.device(\"cuda:0\")\nmodel = model.to(device)\n# vae = AutoencoderKL.from_pretrained(\"madebyollin/sdxl-vae-fp16-fix\").to(device)\nvae = None\nnoise_schedule = create_noise_schedule(1000, device)\ndiffusion = DDPM(denoising_model=model, noise_schedule=noise_schedule, config=config, vae=vae)\n\n\nx_T = torch.randn(1, 3, 64, 64).to(device)\nsampled_images = diffusion.sample(x_T, y=None, guidance_scale=3.0, seed=0)\n\n100%|██████████| 1000/1000 [00:17&lt;00:00, 56.00it/s, std=0.326]\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nprocessed_images = (sampled_images * 255).permute(0, 2, 3, 1).cpu().numpy().astype(np.uint8)\n\n\nplt.imshow(processed_images[0])\nplt.show()\n\n\n\n\n\n\n\n\n\nimport clip\n\nclip_model, _ = clip.load(\"ViT-L/14\")\nclip_model = clip_model.to(device)\n\n\ntexts = [\"a white cat looking at the side\", \"a bulldog\"]\ntext_tokens = clip.tokenize(texts).to(device)\ntext_embeds = clip_model.encode_text(text_tokens)\n\n\n\n\n# text conditioning\nprint(f\"text embeds shape: {text_embeds.shape}\")\nx_T = torch.randn(len(text_embeds), 3, 64, 64).to(device)\nsampled_images = diffusion.sample(x_T, y=text_embeds, guidance_scale=3.0, seed=0)\n\nprocessed_images = (sampled_images * 255).permute(0, 2, 3, 1).cpu().numpy().astype(np.uint8)\nfig, axes = plt.subplots(1, len(processed_images), figsize=(8*len(processed_images), 8))\nfor i, ax in enumerate(axes):\n    ax.imshow(processed_images[i])\n    ax.set_title(texts[i])\n    ax.axis('off')\nplt.show()\n\ntext embeds shape: torch.Size([2, 768])\n\n\n100%|██████████| 1000/1000 [00:19&lt;00:00, 50.44it/s, std=0.736]"
  },
  {
    "objectID": "2_1_a_move_off_notebook.html",
    "href": "2_1_a_move_off_notebook.html",
    "title": "Time to move off of the notebook",
    "section": "",
    "text": "As our model training becomes more complex, we need to transition from notebooks to production scripts for several reasons:\nTo make training jobs robust, some extra work is needed: dependency management, monitoring, logging, and checkpointing.\nThe lib_2_1 directory contains the complete setup for a training job inside a Docker container. To run it, you need to have the following:",
    "crumbs": [
      "Home",
      "Generating Animal Face Images",
      "Time to move off of the notebook"
    ]
  },
  {
    "objectID": "2_1_a_move_off_notebook.html#running-the-training-job",
    "href": "2_1_a_move_off_notebook.html#running-the-training-job",
    "title": "Time to move off of the notebook",
    "section": "Running the training job",
    "text": "Running the training job\ncd notebooks/lib_2_1\nbash build.sh\nbash train.sh\nBy default, the results will be saved in the checkpoints directory. If you like to use a hosted experiment tracking service like Weights & Biases, you can set the WANB_API_KEY and WANDB_PROJECT environment variables, and the results will be logged to Weights & Biases.\nexport WANB_API_KEY=...\nexport WANDB_PROJECT=...\nbash train.sh --logger wandb",
    "crumbs": [
      "Home",
      "Generating Animal Face Images",
      "Time to move off of the notebook"
    ]
  },
  {
    "objectID": "2_1_a_move_off_notebook.html#code-in-the-notebook",
    "href": "2_1_a_move_off_notebook.html#code-in-the-notebook",
    "title": "Time to move off of the notebook",
    "section": "Code in the notebook",
    "text": "Code in the notebook\nSimilar to the refactoring for the 2D point clouds, the notebook is a mix of stable and experimental code.\n\nStable (library) code\n\nMain components of the diffusion model training pipeline:\n\nData and model setup:\n\nDataLoader setup (load_data)\nModel architectures (UnetModel)\nThe training loop (train)\n\nDiffusion specific components:\n\nForward diffusion (forward_diffusion)\nDenoising step (denoising_step)\nNoise schedule creation (create_noise_schedule)\nSample generation (generate_samples_by_denoising)\n\n\nEvaluation and visualization tools:\n\nVisualization utilities (visualize_sampled_images)\n\n\nThe bolded parts are different from the 2D point clouds notebook.\n\n\nExperimental code\n\nCustom and flexible visualization of intermediate results\nHyperparameter choices and training configurations\n\nWe will also be adding code for bookkeeping: setting up logging, checkpointing, monitoring, and evaluation. Bookkeeping code is not part of the training recipe, and it does not impact how the model is trained. However, it is essential for producing the experiments data that helps us understand the behavior of the training process, and identify the winning ingredients of a good training recipe. For evaluation, we need to add FID score calculation (more about this in the next section).",
    "crumbs": [
      "Home",
      "Generating Animal Face Images",
      "Time to move off of the notebook"
    ]
  },
  {
    "objectID": "2_1_a_move_off_notebook.html#the-refactored-code",
    "href": "2_1_a_move_off_notebook.html#the-refactored-code",
    "title": "Time to move off of the notebook",
    "section": "The refactored code",
    "text": "The refactored code\nFirst, let’s make a copy of lib_1_1. The diffusion part of the logic remains almost the same. The function denoising_step is updated to allow for clipping the denoised sample.\nA new file unets.py is added to hold the UNet models. And model.py is updated to use the new models.\ndata.py is updated with a new class HuggingFaceDataset as a convenience class to load image datasets from HuggingFace.\ntraining_loop.py remains very similar.\nconfig.py is updated to add some new hyperparameters.\nbookkeeping.py is added to handle the bookkeeping logic. A Bookkeeping class is implemented with the following interface:\nclass Bookkeeping:\n    def __init__(self, config: TrainingConfig, denoising_model: nn.Module, noise_schedule: Dict):\n        ...\n\n    def set_up_logger(self):\n        ...\n        \n    def run_callbacks(self, config: TrainingConfig, step: int, loss: float, optimizer: torch.optim.Optimizer, val_dataloader: DataLoader):\n        ...",
    "crumbs": [
      "Home",
      "Generating Animal Face Images",
      "Time to move off of the notebook"
    ]
  },
  {
    "objectID": "2_1_a_move_off_notebook.html#adding-scripts-to-manage-the-training-job",
    "href": "2_1_a_move_off_notebook.html#adding-scripts-to-manage-the-training-job",
    "title": "Time to move off of the notebook",
    "section": "Adding scripts to manage the training job",
    "text": "Adding scripts to manage the training job\nSeveral files are added to manage the environment and run the training job in a Docker container:\n\nbuild.sh: Builds the Docker image.\ntrain.sh: Runs the training job.\nDockerfile: Defines the Docker image.\ntrain.py: The training script.\n\n\nThe Dockerfile\nWe use the Dockerfile to define the environment for the training job. When we run the build.sh script, it will build the Docker image that has all the dependencies installed.\nIt is minimal but complete:\nFROM pytorch/pytorch:2.3.1-cuda12.1-cudnn8-runtime\n\n# Non-root user\nRUN useradd -m -s /bin/bash -G sudo -u 1000 nanodiffusion\nUSER nanodiffusion\n\nRUN pip install torchvision==0.19.1 numpy==2.1.2 scipy==1.14.1 \\\n    clean-fid==0.1.35 wandb==0.18.3 datasets==3.0.2\nLet’s break down the Dockerfile:\n\nBase image\n\nFROM pytorch/pytorch:2.3.1-cuda12.1-cudnn8-runtime: This is the base image. It is a PyTorch image with CUDA 12.1 and cuDNN 8.0 installed.\n\n\n\nUser management\n\nRUN useradd -m -s /bin/bash -G sudo -u 1000 nanodiffusion: This creates a non-root user named nanodiffusion. It is a good practice to run the training job as a non-root user.\nUSER nanodiffusion: This switches to the nanodiffusion user.\n\n\n\nDependencies\n\nRUN pip install torchvision==0.19.1 numpy==2.1.2 scipy==1.14.1 \\     clean-fid==0.1.35 wandb==0.18.3 datasets==3.0.2: This installs the dependencies.\n\n\n\n\nThe train.py script\nThe train.py script is the main script for the training job. It is a simple script that loads the model, sets the hyperparameters, and starts the training.\nWe use a TrainingPipeline class to represent the logical flow of model training.\nclass TrainingPipeline:\n    def __init__(self, config: TrainingConfig):\n        self.config = config\n\n    def fit(self, dataset_builder: DatasetBuilder, train_steps: int=10000):\n        train_dataloader, val_dataloader = self._create_dataloaders(dataset_builder)\n        self.noise_schedule = create_noise_schedule(n_T=self.config.num_denoising_steps, device=self.config.device)\n        self.denoising_model = self._create_model(device=self.config.device)\n        self.optimizer = self._create_optimizer(self.denoising_model)\n        bookkeeping = Bookkeeping(config=self.config, denoising_model=self.denoising_model, noise_schedule=self.noise_schedule)\n        bookkeeping.set_up_logger()\n        train(\n            config=self.config,\n            model=self.denoising_model,\n            train_dataloader=train_dataloader,\n            val_dataloader=val_dataloader,\n            noise_schedule=self.noise_schedule,\n            optimizer=self.optimizer,\n            steps=train_steps,\n            silent=False,\n            bookkeeping=bookkeeping,\n        )\n        return self.denoising_model\n\n    def generate_samples(self, num_samples: int):\n        ...\n\n    def _create_model(self, device: str):\n        ...\n\n    def _create_optimizer(self, denoising_model: nn.Module):\n        ...\n    \n    def _create_dataloaders(self, dataset_builder: DatasetBuilder):\n        ...",
    "crumbs": [
      "Home",
      "Generating Animal Face Images",
      "Time to move off of the notebook"
    ]
  },
  {
    "objectID": "3_1_dit.html",
    "href": "3_1_dit.html",
    "title": "DiT: Diffusion Transformers",
    "section": "",
    "text": "Under construction."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Nano Diffusion: Diffusion and Flow Matching from Scratch",
    "section": "",
    "text": "Welcome to Nano Diffusion, a hands-on tutorial series that teaches you how to build diffusion models and flow matching from scratch. These models are what powers the latest generative AI applications in creating high quality images, videos, audio, and more modalities.\nStarting with small datasets and toy examples, we will evolve a minimal code base, train small models as proof of concept, and then add MLOps tools, go through refactoring, explore variants of the training recipes, and scale up to larger datasets and models."
  },
  {
    "objectID": "index.html#what-youll-learn",
    "href": "index.html#what-youll-learn",
    "title": "Nano Diffusion: Diffusion and Flow Matching from Scratch",
    "section": "What You’ll Learn",
    "text": "What You’ll Learn\n\nHow to train diffusion and flow matching models from scratch.\nWorking with popular model architectures like UNet and DiT.\nTraining on various datasets including 2D point clouds, animal face images and larger datasets.\nHow to condition the models to follow text prompts.\nHow to make model training more efficient, and scale up to higher resolution and larger datasets.\nExperiment tracking and model evaluation."
  },
  {
    "objectID": "index.html#prerequisites",
    "href": "index.html#prerequisites",
    "title": "Nano Diffusion: Diffusion and Flow Matching from Scratch",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nAccess to a GPU (12GB VRAM or above)\nFamiliarity with PyTorch and deep learning basics\nDocker and NVIDIA container toolkit installed"
  },
  {
    "objectID": "index.html#tutorials",
    "href": "index.html#tutorials",
    "title": "Nano Diffusion: Diffusion and Flow Matching from Scratch",
    "section": "Tutorials",
    "text": "Tutorials\nPlease use the sidebar to navigate through the tutorials. Checkout a visual story of diffusion and flow matching, and start with Training a Diffusion Model on 2D Points."
  },
  {
    "objectID": "index.html#hardware-for-learning-start-small",
    "href": "index.html#hardware-for-learning-start-small",
    "title": "Nano Diffusion: Diffusion and Flow Matching from Scratch",
    "section": "Hardware for learning: start small",
    "text": "Hardware for learning: start small\nThe 2D examples do not require GPU. You can use your laptop. The cost is almost zero.\n\n\n\n\n\nImage generation for 64x64 aminal faces trained on 16k images requires a GPU but not much VRAM (12GB is enough). We ran the notebooks on an A10 ($0.75/hr on Lambda Labs) or H100 ($2.5/hr on Lambda Labs, about 6x faster than A10). It typically takes 1~3 H100 hours ($3 ~ $10) to train a decent quality model.\n\n\n\n\n\nA larger dataset like with 600k 256x256 images can be run on A10 but a faster GPU is recommended. About 10 H100 hours ($30) is needed to train a decent Text2Image model for this task."
  },
  {
    "objectID": "index.html#references",
    "href": "index.html#references",
    "title": "Nano Diffusion: Diffusion and Flow Matching from Scratch",
    "section": "References",
    "text": "References\nThere are many great resources for the theoretical foundation, large scale training and applications of diffusion and flow matching models:\n\nDDPM paper\nLillian Weng’s blog\nCVPR’22 tutorial\nFlow matching paper\nDiT paper\nDiffusion course from KAIST, Fall 2024\nLatent diffusion with transformers\nFAIR’s flow matching guide\nSony’s MicroDiffusion\nDistributed training guide from Lambda Labs"
  },
  {
    "objectID": "slurm.html",
    "href": "slurm.html",
    "title": "Managing training jobs with SLURM",
    "section": "",
    "text": "We want to run many experiments as fast as possible, with minimal operational overhead and with the GPU resources available to us.\nTo do this, we can use a job scheduler like SLURM. SLURM is a popular open-source job scheduler that allows us to manage and run many jobs on a single machine.",
    "crumbs": [
      "Home",
      "Additional resources",
      "Managing training jobs with SLURM"
    ]
  },
  {
    "objectID": "slurm.html#set-up-slurm",
    "href": "slurm.html#set-up-slurm",
    "title": "Managing training jobs with SLURM",
    "section": "Set up SLURM",
    "text": "Set up SLURM\nInstall dependencies:\nsudo apt install -y build-essential munge libmunge-dev libmunge2 \\\n    mariadb-server mariadb-client libmariadb-dev \\\n    nfs-common ntp libcurl4-openssl-dev\nInstall slurm:\nsudo apt install -y slurm-wlm\n\nCreate directories:\nsudo mkdir -p /etc/slurm /var/spool/slurm /var/log/slurm\nsudo chown slurm:slurm /var/spool/slurm /var/log/slurm\nsudo chmod -R 755 /var/spool/slurm /var/log/slurm\nSet Up the Configuration File: Create /etc/slurm/slurm.conf and populate it with the basic configuration.\nThe\nClusterName=mycluster\nControlMachine=localhost  # please use `hostname` to get the hostname of the machine!\nSlurmUser=slurm\nAuthType=auth/munge\nStateSaveLocation=/var/spool/slurm\nSlurmdSpoolDir=/var/spool/slurm\nSchedulerType=sched/backfill\nSelectType=select/cons_res\nSelectTypeParameters=CR_Core_Memory\nNodeName=localhost Gres=gpu:1 CPUs=16 RealMemory=32000 State=UNKNOWN  # Change this according to the actual resources available\nPartitionName=gpu Nodes=ALL Default=YES MaxTime=INFINITE State=UP\nSlurmdLogFile=/var/log/slurm/slurmd.log\nSlurmctldLogFile=/var/log/slurm/slurmctld.log\nProctrackType=proctrack/cgroup\nTaskPlugin=task/cgroup\nCreate /etc/slurm/cgroup.conf:\nCgroupAutomount=yes\nConstrainCores=yes\nConstrainRAMSpace=yes\nConstrainDevices=yes\nCheck the cgroup version:\nstat -fc %T /sys/fs/cgroup/\nIf the output is cgroup2fs, you are using cgroup v2. If the output is tmpfs, you are using cgroup v1. SLURM currently does not support the freezer subsystem in cgroup v2. To enable cgroup v1:\nEdit /etc/default/grub to have:\nGRUB_CMDLINE_LINUX=\"systemd.unified_cgroup_hierarchy=0\"\nEither reboot:\nsudo update-grub\nsudo reboot\nOr manually remount the cgroup:\nsudo umount /sys/fs/cgroup\nsudo mount -t tmpfs cgroup_root /sys/fs/cgroup\nsudo mkdir -p /sys/fs/cgroup/{cpu,cpuacct,cpuset,memory,devices,freezer,blkio,pids}\nMount individual cgroup subsystems:\nfor subsystem in cpu cpuacct cpuset memory devices freezer blkio pids; do\n    sudo mount -t cgroup -o $subsystem cgroup /sys/fs/cgroup/$subsystem\ndone\nVerify Cgroup v1 Check if cgroup v1 is now active:\nstat -fc %T /sys/fs/cgroup/\nYou should see tmpfs as the output, which indicates cgroup v1 is in use.\nRestart Systemd Services Restart systemd services to ensure they pick up the changes:\nsudo systemctl daemon-reexec\nConfigure GPU Resources: Create a gres.conf file at /etc/slurm/gres.conf. The content of this file depends on the number of GPUs you have.\nNodeName=localhost Name=gpu File=/dev/nvidia0  # please use `hostname` to get the hostname of the machine!\nNodeName=localhost Name=gpu File=/dev/nvidia1\nStart munge:\nsudo systemctl enable --now munge\nTo verify that munge is running, you can run:\nmunge -n | unmunge\nMount freezer:\nsudo mkdir -p /sys/fs/cgroup/freezer\nsudo mount -t cgroup -o freezer freezer /sys/fs/cgroup/freezer\nTo verify, run:\nmount | grep freezer\nStart SLURM Services:\nsudo systemctl start slurmctld\nsudo systemctl start slurmd\nEnable Services on Boot:\nsudo systemctl enable slurmctld\nsudo systemctl enable slurmd\nIf there is an error, you can check the logs with:\nsudo cat /var/log/slurm/slurmd.log",
    "crumbs": [
      "Home",
      "Additional resources",
      "Managing training jobs with SLURM"
    ]
  },
  {
    "objectID": "slurm.html#modifying-the-training-script",
    "href": "slurm.html#modifying-the-training-script",
    "title": "Managing training jobs with SLURM",
    "section": "Modifying the training script",
    "text": "Modifying the training script\nThe training script in lib_2_1 looks like this:\ndocker run --runtime nvidia -it --rm \\\n    --shm-size 16G \\\n    --gpus 'device=0' \\\n    -v $(pwd):/workspace/lib_2_1 \\\n    -v $(pwd)/checkpoints:/workspace/checkpoints \\\n    -v $(pwd)/data/container_cache:/home/$USER/.cache \\\n    -e WANDB_API_KEY=$WANDB_API_KEY \\\n    -e WANDB_PROJECT=$WANDB_PROJECT \\\n    docker_lib_2_1 \\\n    python -m lib_2_1.train $@\nWith SLURM, we can pass in the GPUs assigned to the SLURM job:\n+ # Retrieve the GPUs assigned by SLURM\n+ ASSIGNED_GPUS=$(echo $SLURM_JOB_GPUS | tr ',' ',')\n\n# Create unique output directories using SLURM job ID\n+ JOB_ID=${SLURM_JOB_ID:-default}  # Fallback to 'default' if not run in SLURM\n+ CHECKPOINT_DIR=$(pwd)/checkpoints/$JOB_ID\n+ mkdir -p $CHECKPOINT_DIR && chmod -R 777 $CHECKPOINT_DIR\n\n# Run the Docker container with the assigned GPUs\ndocker run --runtime nvidia -it --rm \\\n    --shm-size 16G \\\n-   --gpus 'device=0' \\\n+   --gpus \"device=$ASSIGNED_GPUS\" \\\n    -v $(pwd):/workspace/lib_2_1 \\\n-   -v $(pwd)/checkpoints:/workspace/checkpoints \\\n+   -v $CHECKPOINT_DIR:/workspace/checkpoints \\\n    -v $(pwd)/data/container_cache:/home/$USER/.cache \\\n    -e WANDB_API_KEY=$WANDB_API_KEY \\\n    -e WANDB_PROJECT=$WANDB_PROJECT \\\n    docker_lib_2_1 \\\n    python -m lib_2_1.train $@",
    "crumbs": [
      "Home",
      "Additional resources",
      "Managing training jobs with SLURM"
    ]
  },
  {
    "objectID": "slurm.html#defining-a-slurm-job",
    "href": "slurm.html#defining-a-slurm-job",
    "title": "Managing training jobs with SLURM",
    "section": "Defining a slurm job",
    "text": "Defining a slurm job\nWe can define a slurm job by creating a script in the bin/sweep directory.\nFor example, we can define a job for the AFHQ dataset by creating a file called afhq.slurm in the bin/sweep directory.\ntouch bin/sweep/afhq.slurm\nWe can then add the following to the file:\n#!/bin/bash\n#SBATCH --gres=gpu:1                         # Request 1 GPU\n#SBATCH --job-name=afhq_job\n#SBATCH --cpus-per-task=8                    # Number of CPU cores\n#SBATCH --mem=32G                            # Memory allocation\n#SBATCH --time=10:00:00                      # Maximum runtime\n#SBATCH --output=logs/slurm/afhq_job_%j.out\n#SBATCH --error=logs/slurm/afhq_job_%j.err\n\n\n# Run the training script\nbin/train.sh --logger wandb -d zzsi/afhq64_16k --resolution 64 --fid_every 20000 --sample_every 3000 --total_steps 100000\nThis slurm script will request 1 GPU, 8 CPU cores, 32GB of memory, and a maximum runtime of 10 hours.",
    "crumbs": [
      "Home",
      "Additional resources",
      "Managing training jobs with SLURM"
    ]
  },
  {
    "objectID": "slurm.html#submitting-a-job",
    "href": "slurm.html#submitting-a-job",
    "title": "Managing training jobs with SLURM",
    "section": "Submitting a job",
    "text": "Submitting a job\nWe can submit a job to SLURM by running the sbatch command.\nsbatch bin/sweep/afhq.slurm\nIf you want to submit another training job, you can edit this slurm script and run the sbatch command again.\n\nEach job will request 1 GPU (--gres=gpu:1) and run independently.\nIf sufficient resources (GPUs) are available, SLURM will allocate them, and both jobs will run simultaneously.\nIf all GPUs are occupied, the new job will wait in the queue until a GPU becomes available.\nThe #SBATCH --output=logs/slurm/afhq_job_%j.out directive creates a unique output file for each job using %j, which represents the SLURM job ID. The output files will not overwrite each other, as they will have different names.\nWarning: However, if both jobs write to the same output directories or files (e.g. checkpoints), there may be conflicts or overwriting. So we need to make sure that the output directories are unique for each job.",
    "crumbs": [
      "Home",
      "Additional resources",
      "Managing training jobs with SLURM"
    ]
  },
  {
    "objectID": "slurm.html#monitoring-jobs",
    "href": "slurm.html#monitoring-jobs",
    "title": "Managing training jobs with SLURM",
    "section": "Monitoring jobs",
    "text": "Monitoring jobs\nYou can monitor the status of your jobs by running the squeue command.\nsqueue\nYou can also view the output of your jobs by running the tail command.\ntail -f logs/slurm/afhq_job_%j.out",
    "crumbs": [
      "Home",
      "Additional resources",
      "Managing training jobs with SLURM"
    ]
  }
]