<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>mj_latents – Nano Diffusion</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./5_1_vae_and_latent_space.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="./index.html">
    <span class="navbar-title">Nano Diffusion</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="./index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
    <a href="https://github.com/kungfuai/nano-diffusion.git" title="GitHub" class="quarto-navigation-tool px-1" aria-label="GitHub"><i class="bi bi-github"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./5_0_scale_up.html">Scaling up: generate bigger and more diverse images</a></li><li class="breadcrumb-item"><a href="./5_2_mj_latents.html">Training a Diffusion Model on Latent Representations</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./visual_story.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">A Visual Story of Diffusion and Flow matching</span></a>
  </div>
</li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Generating a 2D Point Cloud</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="false">
 <span class="menu-text">Diffusion</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./1_1_Diffusion 2D Toy.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Training a Diffusion Model on 2D Points</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./1_1_a_refactor.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">A refactoring exercise</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./1_1_b_Diffusion_2D_hyperparams.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Our First Pareto Frontier Plot</span></a>
  </div>
</li>
      </ul>
  </li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="false">
 <span class="menu-text">Flow Matching</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./1_2_Flow Matching 2D Toy.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Training a Flow Matching Model for 2D Point Generation</span></a>
  </div>
</li>
      </ul>
  </li>
      </ul>
  </li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true">
 <span class="menu-text">Generating Animal Face Images</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./2_1_diffusion_afhq.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Training a Diffusion Model for Animal Face Images</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./2_1_a_move_off_notebook.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Time to move off of the notebook</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./2_2_fid.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Quality evaluation of an image generation model</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true">
 <span class="menu-text">Text Conditioning</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./4_1_text_conditioning_ddpm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Text Conditioning (text2image)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./4_1a_generate_t2i_ddpm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Generate images with text conditioning</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Image Captioning Lesson.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Generating an Image Captions Dataset</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./4_2_text_conditioning_cfm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Text conditioning for Flow Matching</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./4_2a_generate_t2i_cfm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Generate images with text prompts using flow matching</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true">
 <span class="menu-text">Scaling up: generate bigger and more diverse images</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./5_0_scale_up.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Scaling up</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./5_1_vae_and_latent_space.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Generating in the Latent Space</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./5_2_mj_latents.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">Training a Diffusion Model on Latent Representations</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="true">
 <span class="menu-text">Additional resources</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./slurm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Managing training jobs with SLURM</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#training-a-diffusion-model-on-latent-representations" id="toc-training-a-diffusion-model-on-latent-representations" class="nav-link active" data-scroll-target="#training-a-diffusion-model-on-latent-representations">Training a Diffusion Model on Latent Representations</a>
  <ul class="collapse">
  <li><a href="#hardware-requirements-and-considerations" id="toc-hardware-requirements-and-considerations" class="nav-link" data-scroll-target="#hardware-requirements-and-considerations">Hardware Requirements and Considerations</a></li>
  <li><a href="#inpsecting-the-dataset-the-latents-from-midjourney-images" id="toc-inpsecting-the-dataset-the-latents-from-midjourney-images" class="nav-link" data-scroll-target="#inpsecting-the-dataset-the-latents-from-midjourney-images">Inpsecting the Dataset: the Latents from MidJourney Images</a></li>
  <li><a href="#an-improved-diffusion-algorithm-vdm" id="toc-an-improved-diffusion-algorithm-vdm" class="nav-link" data-scroll-target="#an-improved-diffusion-algorithm-vdm">An Improved Diffusion Algorithm: VDM</a>
  <ul class="collapse">
  <li><a href="#forward-diffusion-in-vdm" id="toc-forward-diffusion-in-vdm" class="nav-link" data-scroll-target="#forward-diffusion-in-vdm">Forward Diffusion in VDM</a></li>
  <li><a href="#reverse-diffusion-in-vdm" id="toc-reverse-diffusion-in-vdm" class="nav-link" data-scroll-target="#reverse-diffusion-in-vdm">Reverse Diffusion in VDM</a></li>
  <li><a href="#a-faster-sampling-method-ddpm" id="toc-a-faster-sampling-method-ddpm" class="nav-link" data-scroll-target="#a-faster-sampling-method-ddpm">A Faster Sampling Method: DDPM+</a></li>
  <li><a href="#conditional-generation" id="toc-conditional-generation" class="nav-link" data-scroll-target="#conditional-generation">Conditional Generation</a></li>
  </ul></li>
  <li><a href="#transformer-based-denoising-model" id="toc-transformer-based-denoising-model" class="nav-link" data-scroll-target="#transformer-based-denoising-model">Transformer based denoising model</a></li>
  <li><a href="#codebase-improvements" id="toc-codebase-improvements" class="nav-link" data-scroll-target="#codebase-improvements">Codebase Improvements</a>
  <ul class="collapse">
  <li><a href="#basediffusionalgorithm-class" id="toc-basediffusionalgorithm-class" class="nav-link" data-scroll-target="#basediffusionalgorithm-class">1. <code>BaseDiffusionAlgorithm</code> class</a></li>
  <li><a href="#fp16-training" id="toc-fp16-training" class="nav-link" data-scroll-target="#fp16-training">2. <code>fp16</code> training</a></li>
  <li><a href="#minibatch-class" id="toc-minibatch-class" class="nav-link" data-scroll-target="#minibatch-class">3. <code>MiniBatch</code> class</a></li>
  </ul></li>
  <li><a href="#training-in-docker" id="toc-training-in-docker" class="nav-link" data-scroll-target="#training-in-docker">Training in Docker</a></li>
  <li><a href="#learning-progress" id="toc-learning-progress" class="nav-link" data-scroll-target="#learning-progress">Learning Progress</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">




<section id="training-a-diffusion-model-on-latent-representations" class="level1">
<h1>Training a Diffusion Model on Latent Representations</h1>
<p>In previous tutorials, we trained diffusion models to generate small, domain-specific images like animal faces. While this was a good starting point, real-world applications often require generating larger, more diverse images.</p>
<p>This tutorial explores a more scalable approach by training on latent representations rather than raw pixels. We’ll follow the <a href="https://github.com/CompVis/stable-diffusion">Stable Diffusion</a> approach and build upon the <a href="https://github.com/apapiu/transformer_latent_diffusion">Transformer Latent Diffusion</a> project by Alexandru Papiu, which efficiently handles high-resolution, diverse image generation by operating in a compressed latent space.</p>
<section id="hardware-requirements-and-considerations" class="level2">
<h2 class="anchored" data-anchor-id="hardware-requirements-and-considerations">Hardware Requirements and Considerations</h2>
<p>Training large diffusion models requires significant computational resources. Let’s examine some common GPU options available in cloud computing platforms.</p>
<p>The NVIDIA A10, A100, and H100 GPUs represent different performance tiers:</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>GPU Model</th>
<th>VRAM Options</th>
<th>Performance Comparison</th>
<th>Cost (per hour)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>A10</td>
<td>24GB</td>
<td>Baseline</td>
<td>$0.75</td>
</tr>
<tr class="even">
<td>A100</td>
<td>40GB, 80GB</td>
<td>~3x faster than A10</td>
<td>$1.79</td>
</tr>
<tr class="odd">
<td>H100</td>
<td>40GB, 80GB</td>
<td>~2x faster than A100</td>
<td>$2.99</td>
</tr>
</tbody>
</table>
<p>The A10 offers a cost-effective entry point with 24GB VRAM. The A100 and H100 both come in 40GB and 80GB variants, providing substantially more memory and computing power. The A100 performs about 3 times faster than the A10, while the H100 delivers roughly twice the performance of an A100.</p>
<p>These prices reflect <a href="https://lambda.ai/service/gpu-cloud#pricing">Lambda Cloud</a>’s on-demand pricing for instances (as of Feb 2025). While the H100 has the highest hourly cost, its superior performance makes it the most efficient choice when considering total compute cost needed for training.</p>
</section>
<section id="inpsecting-the-dataset-the-latents-from-midjourney-images" class="level2">
<h2 class="anchored" data-anchor-id="inpsecting-the-dataset-the-latents-from-midjourney-images">Inpsecting the Dataset: the Latents from MidJourney Images</h2>
<div id="cell-1" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the Latents</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Run this only once.</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>cd ..</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> sys</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> nanodiffusion.datasets <span class="im">import</span> MJLatentsDataset</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>ds <span class="op">=</span> MJLatentsDataset()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/home/ubuntu/.local/lib/python3.11/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.
  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>/home/ubuntu/zz/nano-diffusion
File data/raw/mj_latents.npy already exists. Skipping download.
File data/raw/mj_text_emb.npy already exists. Skipping download.</code></pre>
</div>
</div>
<div id="cell-2" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Print out the first example</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> k, v <span class="kw">in</span> ds[<span class="dv">0</span>].items():</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(k, v.shape)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>image_emb (4, 32, 32)
text_emb (768,)</code></pre>
</div>
</div>
<p>Each training example has one latent image <code>image_emb</code> that has 4 channels and 32x32 resolution, and a text embedding <code>text_emb</code> which is a 768 dimensional vector.</p>
<p>What does the actual image look like? Let’s decode <code>image_emb</code> back to the image space.</p>
<div id="cell-4" class="cell" data-execution_count="28">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Load VAE</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> diffusers <span class="im">import</span> AutoencoderKL</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> torch.device(<span class="st">"cuda"</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">"cpu"</span>)</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>vae <span class="op">=</span> AutoencoderKL.from_pretrained(<span class="st">"madebyollin/sdxl-vae-fp16-fix"</span>, torch_dtype<span class="op">=</span>torch.float32).to(device)</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> vae.<span class="bu">eval</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-5" class="cell" data-execution_count="29">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>    latent <span class="op">=</span> torch.asarray(ds[<span class="dv">2</span>][<span class="st">"image_emb"</span>]).unsqueeze(<span class="dv">0</span>).<span class="bu">float</span>().to(device)</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"latent vector:"</span>, latent.shape)</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>    img <span class="op">=</span> vae.decode(latent).sample</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>    img <span class="op">=</span> img.to(device).cpu().permute(<span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">1</span>).numpy()</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"image:"</span>, img.shape)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>latent vector: torch.Size([1, 4, 32, 32])
image: (1, 256, 256, 3)</code></pre>
</div>
</div>
<div id="cell-6" class="cell" data-execution_count="30">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>img <span class="op">=</span> (img <span class="op">-</span> img.<span class="bu">min</span>()) <span class="op">/</span> (img.<span class="bu">max</span>() <span class="op">-</span> img.<span class="bu">min</span>()) <span class="op">*</span> <span class="dv">255</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>img <span class="op">=</span> img.astype(np.uint8)</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>plt.imshow(img[<span class="dv">0</span>])</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="5_2_mj_latents_files/figure-html/cell-6-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>What a cute gathering of cartoon dinos.</p>
<p>During training, we only need the latents. So don’t need to directly use these images.</p>
</section>
<section id="an-improved-diffusion-algorithm-vdm" class="level2">
<h2 class="anchored" data-anchor-id="an-improved-diffusion-algorithm-vdm">An Improved Diffusion Algorithm: VDM</h2>
<p>We are going to create a <a href="https://github.com/apapiu/transformer_latent_diffusion">variant</a> of the standard diffusion algorithm based on <a href="https://arxiv.org/pdf/2107.00630">Variational Diffusion Models (VDM)</a>.</p>
<section id="forward-diffusion-in-vdm" class="level3">
<h3 class="anchored" data-anchor-id="forward-diffusion-in-vdm">Forward Diffusion in VDM</h3>
<p>In forward diffusion, a simpler formula is used (notice the absence square root):</p>
<p><span class="math display">\[\mathbf{x}_t = (1-\sigma_t) \cdot \mathbf{x}_0 + \sigma_t \mathbf{\epsilon}
\]</span></p>
<p>where <span class="math inline">\(\sigma_t\)</span> is noise level, and <span class="math inline">\(\mathbf{\epsilon}\)</span> is a noise sample from a standard normal distribution.</p>
</section>
<section id="reverse-diffusion-in-vdm" class="level3">
<h3 class="anchored" data-anchor-id="reverse-diffusion-in-vdm">Reverse Diffusion in VDM</h3>
<p>In reverse diffusion, we first predict the clean image <span class="math inline">\(\mathbf{\hat{x}}_0\)</span> from the noisy image <span class="math inline">\(\mathbf{x}_t\)</span>, and then apply forward diffusion to get the less noisy image <span class="math inline">\(\mathbf{x}_{t-1}\)</span> in the previous noise level:</p>
<p><span class="math display">\[\mathbf{x}_{t-1} = (1 - \frac{\sigma_{t-1}}{\sigma_{t}}) \cdot \mathbf{\hat{x}}_0 + \frac{\sigma_{t-1}}{\sigma_{t}} \cdot \mathbf{x}_{t}
\]</span></p>
<p>where <span class="math inline">\(\mathbf{\hat{x}}_0\)</span> is the predicted clean image from the denoising model <span class="math inline">\(f_\theta(x_t, t)\)</span>. Here the denoising model no longer predicts the noise, but the clean image. This simplifies the parameterization.</p>
</section>
<section id="a-faster-sampling-method-ddpm" class="level3">
<h3 class="anchored" data-anchor-id="a-faster-sampling-method-ddpm">A Faster Sampling Method: DDPM+</h3>
<p>An improved sampling method, DDPM+, uses a combination of the current and previous predictions to get better estimates:</p>
<p><span class="math display">\[
x_{t-1} = \frac{(\sigma_t - \sigma_{t-1})}{\sigma_t} D_t + \frac{\sigma_{t-1}}{\sigma_t} x_t
\]</span></p>
<p>Here <span class="math inline">\(D_t\)</span> is a combination of the current and previous predictions:</p>
<p><span class="math display">\[
D_t =  f_\theta(x_t, t) + \frac{1}{2r_t} \left( f_\theta(x_t, t) - f_\theta(x_{t+1}, t+1) \right)
\]</span></p>
<p>where <span class="math inline">\(r_t\)</span> is the ratio of consecutive step sizes in log-SNR space:</p>
<p><span class="math display">\[
r_t = \frac{h_t}{h_{t+1}} = \frac{\log(\sigma_t/\sigma_{t-1})}{\log(\sigma_{t+1}/\sigma_t)}
\]</span></p>
<p>This is essentially a form of multistep method that uses information from multiple timesteps to get better estimates, similar to how higher-order numerical methods work.</p>
<p><strong>With above improvements, much fewer steps (e.g.&nbsp;40 instead of 1000) are needed to generate high-quality images.</strong></p>
</section>
<section id="conditional-generation" class="level3">
<h3 class="anchored" data-anchor-id="conditional-generation">Conditional Generation</h3>
<p>For conditional generation, the classifier-free guidance is applied on the predicted clean image <span class="math inline">\(\mathbf{\hat{x}}_0\)</span>:</p>
<p><span class="math display">\[
\mathbf{\hat{x}}_0 = s \cdot \mathbf{\hat{x}}_0^{\textrm{cond}} + (1-s) \cdot \mathbf{\hat{x}}_0^{\textrm{uncond}}
\]</span></p>
<p>where <span class="math inline">\(\mathbf{\hat{x}}_0^{\textrm{cond}}\)</span> is the predicted clean image from the denoising model with the text prompt, and <span class="math inline">\(\mathbf{\hat{x}}_0^{\textrm{uncond}}\)</span> is the predicted clean image from the denoising model without the text prompt, and <span class="math inline">\(s\)</span> is the guidance scale.</p>
</section>
</section>
<section id="transformer-based-denoising-model" class="level2">
<h2 class="anchored" data-anchor-id="transformer-based-denoising-model">Transformer based denoising model</h2>
<p>Transformer architecture is an increasingly popular architecture for denoising models. Its benefits include:</p>
<ul>
<li>Flexibility Across Modalities: The inherent design of transformers allows them to be adapted across various data types with minimal modifications. This flexibility is advantageous in denoising applications that may involve different modalities, such as images, text, or audio. U-Net architectures, being convolution-based, are primarily tailored for spatial data and may require significant adjustments to handle other modalities effectively.</li>
<li>Global Context Modeling: Transformers use self-attention to capture long-range dependencies across the entire input, providing better understanding of global image context for denoising. U-Nets, being convolutional, primarily focus on local features and may struggle with distant relationships.</li>
<li>Scalability: Transformers process all elements simultaneously, enabling efficient parallel computation and better scaling for high-resolution images. Although the quadratic complexity of attention layers increases memory usage and compute time. U-Nets may face scaling limitations due to their hierarchical convolutional structure.</li>
</ul>
<p>We use a model architecture from the <a href="https://github.com/apapiu/transformer_latent_diffusion">tld</a> project. It is inspired by DiT and Pixart-Alpha. To address the “patchy” outputs common with Transformers processing spatial data, the model incorporates a depth-wise convolution in the FFN layer (borrowed from the Local ViT paper).</p>
<p>For encoding, the model:</p>
<ul>
<li>Processes 4×32×32 image latent inputs using a patch size of 2, creating 256 flattened 16-dimensional “pixels”</li>
<li>Uses simple conditioning by concatenating a 768-dimensional pooled CLIP text embedding (ViT/L14) with sinusoidal noise embedding</li>
<li>Feeds this combined conditioning through cross-attention layers in each transformer block</li>
</ul>
</section>
<section id="codebase-improvements" class="level2">
<h2 class="anchored" data-anchor-id="codebase-improvements">Codebase Improvements</h2>
<p>We will add some readability improvements to the codebase. In <code>notebooks/lib_5</code>, you can find the improved codebase. Here are some notable changes:</p>
<section id="basediffusionalgorithm-class" class="level3">
<h3 class="anchored" data-anchor-id="basediffusionalgorithm-class">1. <code>BaseDiffusionAlgorithm</code> class</h3>
<p>To consolidate the different diffusion algorithms, we introduce a <code>BaseDiffusionAlgorithm</code> class that contains the common logic for all diffusion algorithms. We will then extend this class to implement <code>DDPM</code> (<code>notebooks/lib_5/ddpm.py</code>), <code>VDM</code> (<code>notebooks/lib_5/vdm.py</code>). Even flow matching can be implemented as a subclass of <code>BaseDiffusionAlgorithm</code>. This way, you can easily switch between different diffusion algorithms, without changing the model architecture, the VAE, or the training loop.</p>
<p>This abstraction exposes 2 methods:</p>
<ul>
<li><code>prepare_training_examples</code>: Prepare a training example for the denoising model by adding noise to the input data. This is the forward diffusion process, i.e.&nbsp;the teaching.</li>
<li><code>sample</code>: Sample from the denoising model. For diffusion, this is the reverse diffusion process. This is the generation process using a learned denoising model.</li>
</ul>
<div class="sourceCode" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> BaseDiffusionAlgorithm:</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="co">    A common interface for diffusion algorithms.</span></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> prepare_training_examples(<span class="va">self</span>, batch: MiniBatch, <span class="op">**</span>kwargs) <span class="op">-&gt;</span> Tuple[Dict[<span class="bu">str</span>, torch.Tensor], torch.Tensor]:</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a><span class="co">        Prepare a training example for the denoising model by adding noise to the input data.</span></span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a><span class="co">        This is used in the training step.</span></span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a><span class="co">        For diffusion, this is the forward diffusion process.</span></span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a>        ...</span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> sample(<span class="va">self</span>, x_T, y <span class="op">=</span> <span class="va">None</span>, guidance_scale: <span class="bu">float</span> <span class="op">=</span> <span class="va">None</span>, seed: <span class="bu">int</span> <span class="op">=</span> <span class="va">None</span>, <span class="op">**</span>kwargs):</span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a><span class="co">        Sample from the denoising model. For diffusion, this is the reverse diffusion process.</span></span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="cb10-20"><a href="#cb10-20" aria-hidden="true" tabindex="-1"></a><span class="co">            x_T: the initial random noise.</span></span>
<span id="cb10-21"><a href="#cb10-21" aria-hidden="true" tabindex="-1"></a><span class="co">            y: the conditional input.</span></span>
<span id="cb10-22"><a href="#cb10-22" aria-hidden="true" tabindex="-1"></a><span class="co">            guidance_scale: the guidance scale for classifier-free guidance.</span></span>
<span id="cb10-23"><a href="#cb10-23" aria-hidden="true" tabindex="-1"></a><span class="co">            seed: the random seed for reproducibility.</span></span>
<span id="cb10-24"><a href="#cb10-24" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb10-25"><a href="#cb10-25" aria-hidden="true" tabindex="-1"></a>        ...</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Different diffusion algorithms implements variants of noise scheduling, forward diffusion as well as sampling. These are encapsulated as helper classes. For example, in the improved diffusion algorithm in <code>vdm.py</code>, we have <code>VDMForwardDiffusion</code> and <code>VDMTrainingExampleGenerator</code> to handle generating training examples. <code>VDMSampler</code> handles the sampling process.</p>
<p>When a model is trained and you are ready to distribute it for inference, you may only need to distribute the <code>Sampler</code> class to go with your model.</p>
</section>
<section id="fp16-training" class="level3">
<h3 class="anchored" data-anchor-id="fp16-training">2. <code>fp16</code> training</h3>
<p>To reduce the memory usage, we use <code>fp16</code> training with the help of <code>accelerator</code>:</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>accelerator <span class="op">=</span> Accelerator(</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>    mixed_precision<span class="op">=</span><span class="st">"fp16"</span> <span class="cf">if</span> config.fp16 <span class="cf">else</span> <span class="st">"no"</span>,</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>denoising_model, train_dataloader, optimizer <span class="op">=</span> accelerator.prepare(denoising_model, train_dataloader, optimizer)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="minibatch-class" class="level3">
<h3 class="anchored" data-anchor-id="minibatch-class">3. <code>MiniBatch</code> class</h3>
<p>A utility class <code>MiniBatch</code> is added to translate raw mini batch from the dataloader into a stronger typed object.</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="at">@dataclass</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MiniBatch:</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>    x: torch.Tensor  <span class="co"># latent or pixel space input</span></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>    text_emb: Optional[torch.Tensor] <span class="op">=</span> <span class="va">None</span></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>    cond_emb_dict: Optional[Dict[<span class="bu">str</span>, torch.Tensor]] <span class="op">=</span> <span class="va">None</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>With this definition, we can use <code>.x</code> to access the latent or pixel space input, <code>.text_emb</code> to access the text embedding, and <code>.cond_emb_dict</code> to access the conditional embeddings.</p>
</section>
</section>
<section id="training-in-docker" class="level2">
<h2 class="anchored" data-anchor-id="training-in-docker">Training in Docker</h2>
<p>On the GPU instance, run the following commands to start training:</p>
<pre><code>cd notebooks/lib_5
bash build.sh
bash train.sh --fp16</code></pre>
<p>To see the command line arguments, run <code>bash train.sh --help</code>. By default, we train a small variant (3 layers) of the model with the batch size of 128 for about 1000000 steps. We generate 8 sampled images every 1500 steps. You should begin to see generated images every few minutes. For better quality, we will need to use a larger model. You can edit the <code>model.py</code> file to increase the number of layers to 12, which will create a model with roughly 100M parameters, still quite small. We will use this 100M to do some experiments.</p>
</section>
<section id="learning-progress" class="level2">
<h2 class="anchored" data-anchor-id="learning-progress">Learning Progress</h2>
<p>Let’s examine how the model’s image generation capabilities evolve during training. In the beginning, the model generates random image latents which decode to random “blocky” images with no discernible patterns:</p>
<p><img src="assets/lib_5_results/test_samples_0_6cfb603d120140dfa927.jpg" class="img-fluid"></p>
<p>As training progresses to around 5000 steps, the images start showing distinct global patterns and textures, though the content remains unrecognizable:</p>
<p><img src="assets/lib_5_results/media_images_test_samples_4999_bfc5c6deb6cb8827f4c6.jpg" class="img-fluid"></p>
<p>By 30000 steps, rough object shapes and forms begin emerging in the generated images:</p>
<p><img src="assets/lib_5_results/media_images_test_samples_29999_3672d880f3e96a586695.jpg" class="img-fluid"></p>
<p>At 265000 steps (approximately 14 hours of training on an H100 GPU), the model produces clearly recognizable objects like purple dinosaurs, mermaids, and cute cartoon dogs. However, noticeable distortions remain, particularly in facial features where human perception is especially sensitive:</p>
<p><img src="assets/lib_5_results/media_images_test_samples_264999_edd8b42c896131b3ed86.jpg" class="img-fluid"></p>
<p>After extensive training of 1,155,000 steps (roughly 3 days on an H100), the image quality continues improving gradually. While some distortions persist and quality varies between samples, the results are quite impressive. The full training log is track in this <a href="https://wandb.ai/zzsi_kungfu/nano-diffusion/runs/7hmcm6lg?nw=nwuserzzsi_kungfu">wandb run</a>.</p>
<p><img src="assets/lib_5_results/media_images_test_samples_1154999_776bb35cde85f5309d86.jpg" class="img-fluid"></p>
<table class="caption-top table">
<thead>
<tr class="header">
<th style="text-align: left;">Experiment</th>
<th style="text-align: left;">Link</th>
<th style="text-align: right;">Sample Images</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Another VDM example</td>
<td style="text-align: left;"><a href="https://wandb.ai/zzsi_kungfu/nano-diffusion/runs/uu1beq47/overview">wandb</a></td>
<td style="text-align: right;"></td>
</tr>
<tr class="even">
<td style="text-align: left;">Adding EMA</td>
<td style="text-align: left;"><a href="https://wandb.ai/zzsi_kungfu/nano-diffusion/runs/38mf1qp2/workspace?nw=nwuserzzsi_kungfu">wandb</a></td>
<td style="text-align: right;"></td>
</tr>
<tr class="odd">
<td style="text-align: left;">Trained using FP32</td>
<td style="text-align: left;"><a href="https://wandb.ai/zzsi_kungfu/nano-diffusion/runs/pgrb4csb">wandb</a></td>
<td style="text-align: right;"></td>
</tr>
<tr class="even">
<td style="text-align: left;">Changed model architecture to DiT-B2</td>
<td style="text-align: left;"><a href="https://wandb.ai/zzsi_kungfu/nano-diffusion/runs/qcizsk4n/overview">wandb</a></td>
<td style="text-align: right;"></td>
</tr>
<tr class="odd">
<td style="text-align: left;">Using flow matching and DiT-B2</td>
<td style="text-align: left;"><a href="https://wandb.ai/zzsi_kungfu/nano-diffusion/runs/u30xta7j/workspace?nw=nwuserzzsi_kungfu">wandb</a></td>
<td style="text-align: right;"></td>
</tr>
<tr class="even">
<td style="text-align: left;">Baseline DDPM training</td>
<td style="text-align: left;"><a href="https://wandb.ai/zzsi_kungfu/nano-diffusion/runs/co2p9vec?nw=nwuserzzsi_kungfu">wandb</a></td>
<td style="text-align: right;"></td>
</tr>
<tr class="odd">
<td style="text-align: left;">Baseline DDPM training with DiT-B2</td>
<td style="text-align: left;"><a href="https://wandb.ai/zzsi_kungfu/nano-diffusion/runs/j4duh9j5/overview">wandb</a></td>
<td style="text-align: right;"></td>
</tr>
<tr class="even">
<td style="text-align: left;">Baseline DDPM training with UNet-Big</td>
<td style="text-align: left;"><a href="https://wandb.ai/zzsi_kungfu/nano-diffusion/runs/f11d5vd5/overview">wandb</a></td>
<td style="text-align: right;"></td>
</tr>
</tbody>
</table>
<!-- cfm dit_b2: https://wandb.ai/zzsi_kungfu/nano-diffusion/runs/u30xta7j/workspace?nw=nwuserzzsi_kungfu

ddpm with tld_b2 (bad): https://wandb.ai/zzsi_kungfu/nano-diffusion/runs/co2p9vec?nw=nwuserzzsi_kungfu

ddpm, dit_b2, continued (ok): https://wandb.ai/zzsi_kungfu/nano-diffusion/runs/j4duh9j5/overview

ddpm, unet_big continued (ok): https://wandb.ai/zzsi_kungfu/nano-diffusion/runs/f11d5vd5/overview

cfm, unet_big: https://wandb.ai/zzsi_kungfu/nano-diffusion/runs/23gvpltp?nw=nwuserzzsi_kungfu -->
<p>Remarkably, with just one day of training and approximately $50 in compute costs, you can build a model that demonstrates the “vibe”, the capabilities reminiscent of state-of-the-art image generation systems.</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/kungfuai\.github\.io\/nano-diffusion\/");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./5_1_vae_and_latent_space.html" class="pagination-link" aria-label="Generating in the Latent Space">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">Generating in the Latent Space</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
  </div>
</nav>
</div> <!-- /content -->




</body></html>