{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To use the files in the parent directory run this cell\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.vqvae import VQVAE\n",
    "# from src.models.full_vqvae import VQVAE\n",
    "from src.models.var import VAR\n",
    "from src.datasets.hugging_face_dataset import HuggingFaceDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import make_grid\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "model_params = {\n",
    "    \"mnist\": {\n",
    "        \"VQVAE_DIM\": 64,\n",
    "        \"VOCAB_SIZE\": 32,\n",
    "        \"PATCH_SIZES\": [1, 2, 3, 4, 8],\n",
    "        \"VAR_DIM\": 64,\n",
    "        \"N_HEADS\": 4,\n",
    "        \"N_LAYERS\": 6,\n",
    "        \"channels\": 1,\n",
    "    },\n",
    "    \"cifar\": {\n",
    "        \"VQVAE_DIM\": 512,\n",
    "        \"VOCAB_SIZE\": 2048,\n",
    "        \"PATCH_SIZES\": [1, 2, 3, 4, 6, 8],\n",
    "        \"VAR_DIM\": 512,\n",
    "        \"N_HEADS\": 16,\n",
    "        \"N_LAYERS\": 12,\n",
    "        \"channels\": 3,\n",
    "    },\n",
    "    \"small\": {\n",
    "        \"VQVAE_DIM\": 128,\n",
    "        \"VOCAB_SIZE\": 4096,\n",
    "        \"PATCH_SIZES\": [1, 2, 3, 4, 6, 8, 16],\n",
    "        \"VAR_DIM\": 512,\n",
    "        \"N_HEADS\": 16,\n",
    "        \"N_LAYERS\": 16,\n",
    "        \"channels\": 3,\n",
    "    },\n",
    "    # \"medium\": {\n",
    "    #     \"VQVAE_DIM\": 512,\n",
    "    #     \"VOCAB_SIZE\": 2048,\n",
    "    #     \"PATCH_SIZES\": [1, 2, 3, 4, 6, 8],\n",
    "    #     \"VAR_DIM\": 512,\n",
    "    #     \"N_HEADS\": 32,\n",
    "    #     \"N_LAYERS\": 20,\n",
    "    #     \"channels\": 3,\n",
    "    # },\n",
    "    # \"large\": {\n",
    "    #     \"VQVAE_DIM\": 512,\n",
    "    #     \"VOCAB_SIZE\": 4096,\n",
    "    #     \"PATCH_SIZES\": [1, 2, 3, 4, 6, 8],\n",
    "    #     \"VAR_DIM\": 512,\n",
    "    #     \"N_HEADS\": 64,\n",
    "    #     \"N_LAYERS\": 24,\n",
    "    #     \"channels\": 3,\n",
    "    # },\n",
    "}\n",
    "\n",
    "training_params = {\n",
    "    \"mnist\": {\n",
    "        \"VQVAE\": {\n",
    "            \"batch_size\": 2048,\n",
    "            \"lr\": 3e-4,\n",
    "            \"epochs\": 40,\n",
    "        },\n",
    "        \"VAR\": {\n",
    "            \"batch_size\": 1024,\n",
    "            \"lr\": 1e-3,\n",
    "            \"epochs\": 100,\n",
    "        },\n",
    "    },\n",
    "    \"cifar\": {\n",
    "        \"VQVAE\": {\n",
    "            \"batch_size\": 128,\n",
    "            \"lr\": 1e-4,\n",
    "            \"epochs\": 100,\n",
    "        },\n",
    "        \"VAR\": {\n",
    "            \"batch_size\": 64,\n",
    "            \"lr\": 1e-4,\n",
    "            \"epochs\": 100,\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "def get_data(batch_size=1024, dataset=\"mnist\"):\n",
    "    if dataset == \"cifar\":\n",
    "        transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.ToTensor(),\n",
    "                transforms.RandomCrop(32),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.Normalize((0.5,), (0.5,)),\n",
    "            ]\n",
    "        )\n",
    "        train_ds = datasets.CIFAR10(\n",
    "            root=\"./data\", train=True, download=True, transform=transform\n",
    "        )\n",
    "        test_ds = datasets.CIFAR10(\n",
    "            root=\"./data\", train=False, download=True, transform=transform\n",
    "        )\n",
    "    elif dataset == \"mnist\":\n",
    "        transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Pad(2),\n",
    "                transforms.Normalize((0.5,), (0.5,)),\n",
    "            ]\n",
    "        )\n",
    "        train_ds = datasets.MNIST(\n",
    "            root=\"./data\", train=True, download=True, transform=transform\n",
    "        )\n",
    "        test_ds = datasets.MNIST(\n",
    "            root=\"./data\", train=False, download=True, transform=transform\n",
    "        )\n",
    "    else:\n",
    "        transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Resize((32, 32)),\n",
    "                transforms.Normalize((0.5,), (0.5,)),\n",
    "            ]\n",
    "        )\n",
    "        # Use HuggingFace datasets\n",
    "        train_ds = HuggingFaceDataset(dataset_path=dataset, split=\"train\", transform=transform)\n",
    "        test_ds = HuggingFaceDataset(dataset_path=dataset, split=\"val\", transform=transform)\n",
    "\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_ds, batch_size=batch_size, shuffle=False, drop_last=False\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_ds, batch_size=batch_size, shuffle=False, drop_last=True\n",
    "    )\n",
    "\n",
    "    print(len(train_loader), len(test_loader))\n",
    "\n",
    "    return train_loader, test_loader\n",
    "\n",
    "\n",
    "def plot_images(pred, original=None):\n",
    "    n = pred.size(0)\n",
    "    pred = pred * 0.5 + 0.5\n",
    "    pred = pred.clamp(0, 1)\n",
    "    img = pred.cpu().detach()\n",
    "\n",
    "    if original is not None:\n",
    "        original = original * 0.5 + 0.5\n",
    "        original = original.clamp(0, 1)\n",
    "        original = original.cpu().detach()\n",
    "        img = torch.cat([original, img], dim=0)\n",
    "\n",
    "    img_grid = make_grid(img, nrow=n)\n",
    "    img_grid = img_grid.permute(1, 2, 0).numpy()\n",
    "    img_grid = (img_grid * 255).astype(\"uint8\")\n",
    "    plt.imshow(img_grid)\n",
    "    plt.axis(\"off\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"zzsi/afhq64_16k\"\n",
    "model_params = model_params[\"small\"]\n",
    "training_params = training_params[\"cifar\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training VQVAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========Training VQVAE==========\n",
      "115 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Encoder Output: mean=0.0861, std=0.7615, min=-4.1897, max=4.1117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 512.00 MiB. GPU 0 has a total capacity of 23.67 GiB of which 12.81 MiB is free. Including non-PyTorch memory, this process has 23.47 GiB memory in use. Of the allocated memory 22.45 GiB is allocated by PyTorch, and 725.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 48\u001b[0m\n\u001b[1;32m     45\u001b[0m         latent \u001b[38;5;241m=\u001b[39m vq_model\u001b[38;5;241m.\u001b[39mencoder(x)\n\u001b[1;32m     46\u001b[0m         print_latent_stats(latent, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Encoder Output\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 48\u001b[0m xhat, r_maps, idxs, scales, q_loss \u001b[38;5;241m=\u001b[39m \u001b[43mvq_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m recon_loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmse_loss(xhat, x)\n\u001b[1;32m     50\u001b[0m loss \u001b[38;5;241m=\u001b[39m recon_loss \u001b[38;5;241m+\u001b[39m q_loss\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/code/personal/nano-diffusion/src/models/vqvae.py:284\u001b[0m, in \u001b[0;36mVQVAE.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    283\u001b[0m     f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(x)\n\u001b[0;32m--> 284\u001b[0m     fhat, r_maps, idxs, scales, loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquantizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m     x_hat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(fhat)\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x_hat, r_maps, idxs, scales, loss\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/code/personal/nano-diffusion/src/models/vqvae.py:194\u001b[0m, in \u001b[0;36mVectorQuantizer.forward\u001b[0;34m(self, f_BCHW)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, f_BCHW: torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m--> 194\u001b[0m     r_R_BChw, idx_R_BL, zqs_post_conv_R_BCHW \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf_BCHW\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    195\u001b[0m     f_hat_BCHW, scales_BLC, loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecode(f_BCHW, zqs_post_conv_R_BCHW)\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f_hat_BCHW, r_R_BChw, idx_R_BL, scales_BLC, loss\n",
      "File \u001b[0;32m~/code/personal/nano-diffusion/src/models/vqvae.py:207\u001b[0m, in \u001b[0;36mVectorQuantizer.encode\u001b[0;34m(self, f_BCHW)\u001b[0m\n\u001b[1;32m    204\u001b[0m r_BChw \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39minterpolate(f_BCHW, (resolution_k, resolution_k), mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marea\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    205\u001b[0m r_flattened_NC \u001b[38;5;241m=\u001b[39m r_BChw\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim)\n\u001b[1;32m    206\u001b[0m dist \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 207\u001b[0m     \u001b[43mr_flattened_NC\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcodebook\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m r_flattened_NC \u001b[38;5;241m@\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcodebook\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mT\n\u001b[1;32m    210\u001b[0m )\n\u001b[1;32m    212\u001b[0m idx_Bhw \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmin(dist, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mview(B, resolution_k, resolution_k)\n\u001b[1;32m    213\u001b[0m idx_R_BL\u001b[38;5;241m.\u001b[39mappend(idx_Bhw\u001b[38;5;241m.\u001b[39mreshape(B, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 512.00 MiB. GPU 0 has a total capacity of 23.67 GiB of which 12.81 MiB is free. Including non-PyTorch memory, this process has 23.47 GiB memory in use. Of the allocated memory 22.45 GiB is allocated by PyTorch, and 725.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "print(\"=\" * 10 + \"Training VQVAE\" + \"=\" * 10)\n",
    "\n",
    "def print_latent_stats(latents, name=\"Latents\"):\n",
    "    mean_val = latents.mean().item()\n",
    "    std_val = latents.std().item()\n",
    "    min_val = latents.min().item()\n",
    "    max_val = latents.max().item()\n",
    "    print(f\"{name}: mean={mean_val:.4f}, std={std_val:.4f}, min={min_val:.4f}, max={max_val:.4f}\")\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "start_time = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "log_dir = f\"../logs/var/vqvae/{start_time}\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "vq_model = VQVAE(\n",
    "    dim=model_params[\"VQVAE_DIM\"],\n",
    "    vocab_size=model_params[\"VOCAB_SIZE\"],\n",
    "    patch_sizes=model_params[\"PATCH_SIZES\"],\n",
    "    num_channels=model_params[\"channels\"],\n",
    ")\n",
    "optimizer = torch.optim.AdamW(\n",
    "    vq_model.parameters(), lr=training_params[\"VQVAE\"][\"lr\"]\n",
    ")\n",
    "\n",
    "train_loader, test_loader = get_data(\n",
    "    batch_size=training_params[\"VQVAE\"][\"batch_size\"], dataset=dataset\n",
    ")\n",
    "vq_model = vq_model.to(\"cuda\")\n",
    "\n",
    "# All epochs\n",
    "for epoch in tqdm(range(training_params[\"VQVAE\"][\"epochs\"])):\n",
    "    epoch_loss = 0\n",
    "    epoch_recon_loss = 0\n",
    "    # Single epochs\n",
    "    for i, (x, c) in enumerate(train_loader):\n",
    "        x, c = x.cuda(), c.cuda()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Optionally check and print the latent stats from the encoder on the first batch of each epoch.\n",
    "        if i == 0:\n",
    "            with torch.no_grad():\n",
    "                latent = vq_model.encoder(x)\n",
    "                print_latent_stats(latent, name=f\"Epoch {epoch} Encoder Output\")\n",
    "\n",
    "        xhat, r_maps, idxs, scales, q_loss = vq_model(x)\n",
    "        recon_loss = F.mse_loss(xhat, x)\n",
    "        loss = recon_loss + q_loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_recon_loss += recon_loss.item()\n",
    "\n",
    "    epoch_loss /= len(train_loader)\n",
    "    epoch_recon_loss /= len(train_loader)\n",
    "    print(f\"Epoch: {epoch}, Loss: {epoch_loss}, Recon Loss: {epoch_recon_loss}\")\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        with torch.no_grad():\n",
    "            total_loss = 0\n",
    "            total_recon_loss = 0\n",
    "            for i, (x, c) in enumerate(test_loader):\n",
    "                x, c = x.cuda(), c.cuda()\n",
    "                xhat, r_maps, idxs, scales, q_loss = vq_model(x)\n",
    "                recon_loss = F.mse_loss(xhat, x)\n",
    "                loss = recon_loss + q_loss\n",
    "                total_loss += loss.item()\n",
    "                total_recon_loss += recon_loss.item()\n",
    "\n",
    "            total_loss /= len(test_loader)\n",
    "            total_recon_loss /= len(test_loader)\n",
    "\n",
    "            print(\n",
    "                f\"Epoch: {epoch}, Test Loss: {total_loss}, Test Recon Loss: {total_recon_loss}\"\n",
    "            )\n",
    "\n",
    "            x = x[:10, :].cuda()\n",
    "            x_hat = vq_model(x)[0]\n",
    "\n",
    "            plot_images(pred=x_hat, original=x)\n",
    "            plt.savefig(f\"{log_dir}/vqvae_{epoch}.png\")\n",
    "            plt.close()\n",
    "\n",
    "torch.save(vq_model.state_dict(), f\"{log_dir}/vqvae_animal_faces.pth\")\n",
    "del vq_model, optimizer, x, x_hat, train_loader, test_loader\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training VAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========Training VAR==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_43419/3453378021.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  vqvae.load_state_dict(torch.load(\"../logs/var/vqvae/2025-02-23_11-26-09/vqvae_animal_faces.pth\"))  # LOADS the trained VQVAE model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VQVAE Parameters: 16.68M\n",
      "VAR Parameters: 111.84M\n",
      "229 23\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 23.67 GiB of which 832.00 KiB is free. Including non-PyTorch memory, this process has 23.48 GiB memory in use. Of the allocated memory 22.95 GiB is allocated by PyTorch, and 228.58 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 33\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVAR Parameters: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28msum\u001b[39m(p\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m var_model\u001b[38;5;241m.\u001b[39mparameters())\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m1e6\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mM\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     30\u001b[0m train_loader, test_loader \u001b[38;5;241m=\u001b[39m get_data(\n\u001b[1;32m     31\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mtraining_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVAR\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m], dataset\u001b[38;5;241m=\u001b[39mdataset\n\u001b[1;32m     32\u001b[0m )\n\u001b[0;32m---> 33\u001b[0m var_model \u001b[38;5;241m=\u001b[39m \u001b[43mvar_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(training_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVAR\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m\"\u001b[39m])):\n\u001b[1;32m     35\u001b[0m     epoch_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1174\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1171\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1172\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1174\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:780\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 780\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    782\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    785\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    791\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:780\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 780\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    782\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    785\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    791\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 780 (1 times)]\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:780\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 780\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    782\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    785\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    791\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:805\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    801\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    803\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 805\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    806\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    808\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1160\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1153\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1154\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1155\u001b[0m             device,\n\u001b[1;32m   1156\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1157\u001b[0m             non_blocking,\n\u001b[1;32m   1158\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1159\u001b[0m         )\n\u001b[0;32m-> 1160\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1163\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1164\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1165\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 23.67 GiB of which 832.00 KiB is free. Including non-PyTorch memory, this process has 23.48 GiB memory in use. Of the allocated memory 22.95 GiB is allocated by PyTorch, and 228.58 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 10 + \"Training VAR\" + \"=\" * 10)\n",
    "vqvae = VQVAE(\n",
    "    model_params[\"VQVAE_DIM\"],\n",
    "    model_params[\"VOCAB_SIZE\"],\n",
    "    model_params[\"PATCH_SIZES\"],\n",
    "    num_channels=model_params[\"channels\"],\n",
    ")\n",
    "vqvae.load_state_dict(torch.load(\"../logs/var/vqvae/2025-02-23_11-26-09/vqvae_animal_faces.pth\"))  # LOADS the trained VQVAE model\n",
    "vqvae = vqvae.to(\"cuda\")\n",
    "vqvae.eval()\n",
    "\n",
    "for param in vqvae.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "var_model = VAR(\n",
    "    vqvae=vqvae,\n",
    "    dim=model_params[\"VAR_DIM\"],\n",
    "    n_heads=model_params[\"N_HEADS\"],\n",
    "    n_layers=model_params[\"N_LAYERS\"],\n",
    "    patch_sizes=model_params[\"PATCH_SIZES\"],\n",
    "    n_classes=10,\n",
    ")\n",
    "optimizer = torch.optim.AdamW(\n",
    "    var_model.parameters(), lr=training_params[\"VAR\"][\"lr\"]\n",
    ")\n",
    "\n",
    "print(f\"VQVAE Parameters: {sum(p.numel() for p in vqvae.parameters())/1e6:.2f}M\")\n",
    "print(f\"VAR Parameters: {sum(p.numel() for p in var_model.parameters())/1e6:.2f}M\")\n",
    "\n",
    "train_loader, test_loader = get_data(\n",
    "    batch_size=training_params[\"VAR\"][\"batch_size\"], dataset=dataset\n",
    ")\n",
    "var_model = var_model.to(\"cuda\")\n",
    "for epoch in tqdm(range(training_params[\"VAR\"][\"epochs\"])):\n",
    "    epoch_loss = 0\n",
    "    for i, (x, c) in enumerate(train_loader):\n",
    "        x, c = x.cuda(), c.cuda()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        _, _, idxs_R_BL, scales_BlC, _ = vqvae(x)\n",
    "        idx_BL = torch.cat(idxs_R_BL, dim=1)\n",
    "        scales_BlC = scales_BlC.cuda()\n",
    "        logits_BLV = var_model(scales_BlC, cond=c)\n",
    "        loss = F.cross_entropy(\n",
    "            logits_BLV.view(-1, logits_BLV.size(-1)), idx_BL.view(-1)\n",
    "        )\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    epoch_loss /= len(train_loader)\n",
    "    print(f\"Epoch: {epoch}, Loss: {epoch_loss}\")\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        with torch.no_grad():\n",
    "\n",
    "            cond = torch.arange(10).cuda()\n",
    "            out_B3HW = var_model.generate(cond, 0)\n",
    "            plot_images(pred=out_B3HW)\n",
    "\n",
    "            plt.savefig(f\"var_{epoch}.png\")\n",
    "            plt.close()\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        torch.save(var_model.state_dict(), \"var.pth\")\n",
    "\n",
    "torch.save(var_model.state_dict(), \"var.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Quality of VQVAE\n",
    "\n",
    "To test the quality of the vqvae we take a ground truth image pass it through the vqvae, change a few of the tokens, and then pass it through the decoder.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_42854/2028737188.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  vq_model.load_state_dict(torch.load(f\"../logs/var/vqvae/2025-02-23_11-26-09/vqvae_animal_faces.pth\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14630 1500\n"
     ]
    }
   ],
   "source": [
    "vq_model = VQVAE(\n",
    "    dim=model_params[\"VQVAE_DIM\"],\n",
    "    vocab_size=model_params[\"VOCAB_SIZE\"],\n",
    "    patch_sizes=model_params[\"PATCH_SIZES\"],\n",
    "    num_channels=model_params[\"channels\"],\n",
    ").cuda()\n",
    "# vq_model.load_state_dict(torch.load(\"vqvae.pth\"))\n",
    "vq_model.load_state_dict(torch.load(f\"../logs/var/vqvae/2025-02-23_11-26-09/vqvae_animal_faces.pth\"))\n",
    "\n",
    "train_loader, test_loader = get_data(\n",
    "    batch_size=1, dataset=dataset\n",
    ")\n",
    "x = next(iter(train_loader))[0].cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    latent = vq_model.encoder(x)\n",
    "    fhat, r_maps, idxs, scales, loss = vq_model.quantizer(latent)\n",
    "    x_hat = vq_model.decoder(fhat)\n",
    "\n",
    "    plot_images(x_hat, original=x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 256, 8, 8])\n",
      "2 1\n",
      "4 6\n",
      "7 6\n",
      "6 4\n",
      "7 6\n",
      "7 5\n",
      "2 7\n",
      "7 5\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import random\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    latent = vq_model.encoder(x)\n",
    "    fhat, r_maps, idxs, scales, loss = vq_model.quantizer(latent)\n",
    "\n",
    "    print(fhat.shape)\n",
    "\n",
    "    # For 5 random patches, change the vector to a random vector from the codebook\n",
    "    for i in range(8):\n",
    "        random_x = random.randint(0, fhat.shape[2] - 1)\n",
    "        random_y = random.randint(0, fhat.shape[3] - 1)\n",
    "        print(random_x, random_y)\n",
    "        new_vector = vq_model.quantizer.codebook(torch.randint(0, model_params[\"VOCAB_SIZE\"], (1,)).cuda())\n",
    "        fhat[0, :, random_x, random_y] = new_vector\n",
    "    # new_vector = vq_model.quantizer.codebook(torch.tensor(500).cuda())\n",
    "    # fhat[0, :, 0, 0] = new_vector\n",
    "\n",
    "    # Decode the altered latent\n",
    "    x_hat = vq_model.decoder(fhat)\n",
    "\n",
    "    plot_images(x_hat, original=x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
