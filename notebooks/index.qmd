---
title: "Nano Diffusion: Diffusion and Flow Matching from Scratch"
---

Welcome to Nano Diffusion, a hands-on tutorial series that teaches you how to build diffusion models and flow matching from scratch. These models are what powers the latest generative AI applications in creating high quality images, videos, audio, and more modalities.

Starting with small datasets and toy examples, we will evolve a minimal code base, train small models as proof of concept, and then add MLOps tools, go through refactoring, explore variants of the training recipes, and scale up to larger datasets and models.

## What You'll Learn

- How to train diffusion and flow matching models from scratch.
- Working with popular model architectures like [UNet](https://arxiv.org/abs/1505.04597) and [DiT](https://arxiv.org/abs/2212.09748).
- Training on various datasets including 2D point clouds, animal face images and larger datasets.
- How to condition the models to follow text prompts.
- How to make model training more efficient, and scale up to higher resolution and larger datasets.
- Experiment tracking and model evaluation.

## Prerequisites

- Access to a GPU (12GB VRAM or above)
- Familiarity with PyTorch and deep learning basics
- Docker and NVIDIA container toolkit installed

## Tutorials

Please use the sidebar to navigate through the tutorials. Checkout [a visual story of diffusion and flow matching](https://kungfuai.github.io/nano-diffusion/visual_story.html), and start with [Training a Diffusion Model on 2D Points](https://kungfuai.github.io/nano-diffusion/1_1_Diffusion%202D%20Toy.html). You can go through the tutorials in order or skip around to the sections that interest you.

## Hardware for learning: start small

::: {.callout-note}
## Compute Resources Sponsored by Lambda Labs

This project's compute resources are generously sponsored by [Lambda Labs](https://lambdalabs.com/), providing access to their powerful and cost effective GPU infrastructure including A10 and H100 instances.
:::

The 2D examples do not require GPU. You can use your laptop. The cost is almost zero.

![](assets/learned_denoising_vector_field.gif){fig-align="center"}

Image generation for 64x64 aminal faces trained on 16k images requires a GPU but not much VRAM (12GB is enough).
We ran the notebooks on an A10 ($0.75/hr on Lambda Labs) or H100 ($2.5/hr on Lambda Labs, about 6x faster than A10).
It typically takes 1~3 H100 hours ($3 ~ $10) to train a decent quality model.

![](assets/afhq64_test_samples.png){fig-align="center"}

A larger dataset like with 600k 256x256 images can be run on A10 but a faster GPU is recommended.
With 15 H100 hours ($50) you can train a decent Text2Image model from scratch that begins to show the vibe of frontier image generation models.

![](assets/mj_latents_0.png){fig-align="center"}



## References

There are many great resources for the theoretical foundation, large scale training and applications of diffusion and flow matching models:


- [DDPM paper](https://arxiv.org/abs/2006.11239)
- [Lillian Weng's blog](https://lilianweng.github.io/posts/2021-07-11-diffusion-models/)
- [CVPR'22 tutorial](https://cvpr2022-tutorial-diffusion-models.github.io/)
- [Flow matching paper](https://arxiv.org/abs/2210.02747)
- [DiT paper](https://arxiv.org/abs/2212.09748)
- [Diffusion course from KAIST, Fall 2024](https://mhsung.github.io/kaist-cs492d-fall-2024/)
- [Latent diffusion with transformers](https://github.com/apapiu/transformer_latent_diffusion)
- [FAIR's flow matching guide](https://arxiv.org/abs/2412.06264)
- [Sony's MicroDiffusion](https://github.com/SonyResearch/micro_diffusion)
- [Distributed training guide from Lambda Labs](https://github.com/LambdaLabsML/distributed-training-guide)

